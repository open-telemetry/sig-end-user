# OTel Me...with Jerome Johnson  &amp; Cal Loomis

Published on 2025-03-21T05:53:37Z

## Description

Join us as we catch up with Relativity from our first convo with them back in 2022 -- How has the development of OpenTelemetry ...

URL: https://www.youtube.com/watch?v=DrD35XxTDsY

## Summary

In this episode of "Hotel Me," hosts Ree and Adriana, broadcasting from Vancouver and Toronto respectively, engage in a discussion about observability and OpenTelemetry with guests Cal and Jerome from Relativity. The conversation dives into Relativity's journey as one of the early adopters of OpenTelemetry, exploring their tech stack, architecture, and the complexities of managing a vast number of services and telemetry data. They share insights on how they transitioned from a bespoke telemetry system to OpenTelemetry, the challenges faced, and the cultural shifts within their organization towards a more observability-driven approach. The guests discuss their experience with deploying custom OpenTelemetry collectors, their strategies for data management, and the importance of standardizing telemetry data for downstream consumers like AI divisions and management reporting. They also provide feedback for the OpenTelemetry project, emphasizing the need for improved documentation and better integration of semantic conventions. The episode concludes with audience questions regarding data consumption and auto instrumentation, highlighting the ongoing evolution in telemetry practices.

## Chapters

00:00:00 Welcome and intro
00:01:50 Guest introductions
00:04:10 Relativity overview
00:06:00 Tech stack insights
00:09:50 OpenTelemetry adoption discussion
00:12:30 Collector setup explanation
00:15:00 Managing collector fleet
00:20:00 Observability team structure
00:21:50 Implementation evolution
00:30:00 Data consumption and management

**Ree:** Hello. Welcome everyone to a new episode of Hotel Me, formerly known as the end user Q&A. I'm very excited to be here. If you have been here before, I am Ree and I have Adriana here with me. I am coming to you live from Vancouver, Washington, not BC. Adriana, where are you coming from?

**Adriana:** I am coming to you live from Toronto, Canada.

**Ree:** So, we're at opposite ends of the coast, which is pretty—

**Adriana:** We are. Yeah.

[00:01:50] **Ree:** And so, for those of you who are familiar with the flow, hang tight. We are going to bring our guests on pretty soon. For those of you who are new, welcome. So, how this is going to go is we are going to bring on our two special guests from an end user organization shortly. We're going to have a quick introduction. They'll introduce us to their tech stack and how long they've been using open telemetry. This one's gonna be a really interesting one because they were actually one of the first guests we talked to when we started this segment back in 2022. So, I'm pretty excited to follow up with them and see how far they've come in their journey, what kind of struggles they've had along the way, and what they've figured out. And then we'll get into some time for them to give us some feedback about the project. We should also have some time at the end for any audience members to ask questions. If you do have questions that come up as we go, feel free to pop them in the chat, whether you're on LinkedIn or YouTube, and we will get to them as appropriate during the conversation. If not, we will wait until the end. And with that, I believe we can bring our guests on. Oh, and Adriana, do you want to talk about our resources just a little bit?

**Adriana:** Oh, yeah. So, our resources, I believe this will—I'm going to scan the QR code myself because I don't remember what this does. This will take us to—this takes us to open telemetry.io. We have a page in the hotel site all about our end user SIG. If you want to learn more about the end user SIG, we welcome you to join. I believe there's a link on there also for joining our Slack channel on CNCF Slack. So, if you're already on CNCF Slack, please come find us. We welcome everyone. People come along to ask questions. We might not have all the answers, but we can definitely direct you to where you need to be. Oh, and also, we would love to know where you are listening from. So feel free to put that in the chat as well so we can—we just like to know where people are listening from. 

**Ree:** And so the guests we have for you today are Jerome Johnson and Cal—columnist from Relativity. 

**Jerome:** Hello. 

**Cal:** Hi everyone. Howdy howdy.

**Ree:** We would love to learn about you. Can you tell us a little bit about your role in the company and kind of what Relativity is focused on?

**Cal:** You can go ahead, Jerome. Alphabetical order and whatnot.

[00:04:10] **Jerome:** Oh, okay. All right. Excellent. Very democratic. Yeah. So, a little bit about Relativity. Relativity is a company that has predominantly a SaaS product that provides services to the legal community. So, we automate essentially all the processes around legal e-discovery, so the exchange of documents and all of that. It is a SaaS platform. We have a lot of companies around the world that actually use it, and we as observability are really tuned toward how our SaaS project is actually working at any particular point in time. As far as me, I started with Relativity about four—almost five years ago now. I was brought in basically to upgrade the way we do telemetry, and as Ree mentioned, we were one of the probably the first ones to have a chat with you folks but also one of the first adopters of open telemetry in production. So we were very, very early adopters of open telemetry. I'm an architect here, and then I'll let Jerome introduce himself.

**Jerome:** Hello. As Cal said, we're on the observability team. My name is Jerome. I've been at the company for about 10 years or so now, but I've only been on the observability team, specifically the one that's dealing with open telemetry, for a little over a year now. So hopefully I can bring some perspective on how easy it is to use from an internal side and how nice it is—so looking forward to chatting it up with you guys.

[00:06:00] **Ree:** Cool. That's so exciting. Well, I think this is a great opportunity for us to start digging in. Can either of you or both of you give us some insights into your tech stack and architecture?

**Cal:** Yeah, so I could talk about that a bit. Like I said, we were an early adopter of open telemetry. What we had before that was an entirely bespoke, I would say, system for collecting telemetry from a lot of different sources. In addition to that, we did use New Relic at the time, but we also had a lot of other vendors in there too. So, it was kind of a nightmare from an operational standpoint to actually do any troubleshooting and debugging because you had to go to lots of different sources to sort of put all that telemetry together in your head. At the time when I was hired, the company made sort of a decision to try to unify everything. We looked around to see what would be the best platform for doing that. The one that sort of checked all the boxes, even though it was really immature, was open telemetry. So that was the one which we pointed to to do that. Now internally we have a very complicated platform. Maybe this is time to bring up the diagram on what we actually do now with open telemetry. We have 1500 plus different services running on our platform, and that is a huge mix of things. We are predominantly a Windows and .NET shop, so most of that is actually Windows and .NET. But we have a mix of basically every other language in the world. So we have a very, very complicated tech stack and we need to pull in the telemetry from all those different things. Most of the complexity in our observability platform—and I'll probably mention at various points—if I say RELI, that is our observability platform that's built over open telemetry. Most of the complexity is just being able to pull those signals in from lots of different sources, and you see most of them here. We really prefer that people use the open telemetry SDKs now, and we push that as hard as we can. But we still have people using our old platform, which is relatively APM, which we translate into open telemetry now. We have things like SNMP. We pull logging from lots and lots of different places, and that's probably the part which is most difficult and most heterogeneous in terms of the platform itself. We have a whole fleet of open telemetry collectors that we run in 20 different regions around the world. Each one of those is load balanced. So we're running anywhere between, depending on the time of day and all that, between 200 and probably 800 individual collectors around the world to pull all this telemetry in. Then we send that to various data stores for operations. This is really New Relic at this point. So that's where most of our engineers go to actually see what the telemetry is doing. We have a subset of people who use LaunchDarkly and use that for future releases. So we send some set of telemetry there. We actually archive all of our telemetry for compliance reasons for a year and for forensic analysis and stuff like that. We also have a reporting platform that uses the Azure blob storage basically as its backend to make that reporting for managers and things like that. This makes it look simpler than it is. It's a lot more gnarly than this diagram lets on. But we do have a large scale—I mean, 20 plus regions around the world, 1500 services—all feeding into this is a fairly complicated platform.

[00:09:50] **Ree:** Dang, that was really cool actually. Questions. So many questions. I do actually have a lot of follow-up questions, but I also want to get into why open telemetry. How, like when exactly—I know you mentioned, you know, kind of back in 2022, your team was early adopters. How did you come upon open telemetry and like why did you decide, like, "Oh, this makes sense for us?"

[00:12:30] **Cal:** Yeah. So, you know, I think the thing which sort of ticked all the boxes for us—we came from a place where we were having to maintain our own observability libraries, and given the fact that we were .NET and then trying to diversify into lots of different languages meant that we were having to translate that thing again and again and again. On top of that, we also had—well, we were using New Relic at the time and we had a lot of the New Relic agents in the mix as well. So we had this weird mix of our own libraries for some stuff. We had a mix of things from New Relic with the agents and all of that kind of stuff. We had a lot of people who were writing to New Relic directly from their code as well as to other sources. One of the things which was really complicated on our side was every time we wanted to move something or we wanted to change something, it meant code changes. That's something we wanted to avoid going forward. So we wanted to standardize on something which was ideally open source that we could include in our services one time and maintain that and then change where we put things but didn't have to change the code every time. So we were really looking for, you know, something that would give us stability in terms of the codebase but allow us the flexibility to send things to other places as we needed to do that. Open telemetry was the only one which sort of ticked those boxes for us at the time. The only worry we really had at that point was that it was a new project. There was a lot of risk in going that direction. We decided in the end—it wasn't an easy decision. We actually discussed this a long time internally, but we decided that it was worth the risk to go ahead and adopt that because it looked like it was going to tick the boxes for what we wanted to do there. Basically, it's to isolate basically our codebase from what we do with the telemetry afterwards, which was sort of the selling point for what we wanted to do there.

**Jerome:** Oh yeah, just to add on to that, I can give somewhat of a perspective to what it was like before we had open telemetry. I was serving on the performance team for a good number of years, and it was very challenging to do analysis of specific applications just because, like Cal had mentioned, it was kind of the wild west on how we handled metrics. Performance metrics and monitoring are somewhat look similar, a little different, but enough to where every time we had to analyze a specific application, we would have to do a deep dive on that application. Like, "What telemetry are you guys putting out?" "Oh, you're not putting out any at all." Then we'd have to dig into the code, and that would really muck up how much time we could actually drill down into what the core problem of said application from a performance perspective was. Usually, it would take half the time we would do an analysis. It would be like trying to understand what the product was and then actually coming up with a solution would be the latter 50 or 40%. So since joining observability, even though Cal showed a simplified version of our RELI stack—and like you said, it's a bit more complicated once you drill down into it—I was amazed at just how much cleaner it is to understand what our Relativity is doing as a whole. Take that with what you want.

**Ree:** That's very cool. Now as a follow-up question, one of the things that we are always curious about in the SIG is collector setup. What kind of—you mentioned, I believe, Cal mentioned that you guys manage a fleet of collectors. How do you manage that fleet of collectors? Can you give us an idea of how many collectors? Is there a particular setup that you use? We would love to know.

[00:15:00] **Cal:** Yeah. So, you know, I think in terms of how things are usually deployed, I think you call it a gateway setup. All of our collectors are basically in a gateway setup. People send their telemetry to us, we process it, and send it on. Over the years, we've had more or less complicated pipelines, but in general, we have essentially a single set of collectors with a common configuration that handles all of that telemetry. In terms of the numbers of things, we are running over 20 plus regions. Each region has somewhere between, depending on the load, between 10 and probably 50 to 100 instances themselves. So we're running at any point in time somewhere between 200 and 500 individual collectors. These are all running in Kubernetes. We are very focused on compute and Kubernetes. So that's where almost all these things are running. In terms of the deployment itself, we actually handle all of our deployments via Helm charts. We use Helm charts basically to deploy these all out to the Kubernetes clusters. We do take pains to make sure that essentially all of our configurations are isolated. Our collectors, except in a very few cases, do not reach out to external services. We manage the configurations all locally to make sure that they are as available and robust as possible. So that's kind of how we do all of that. In terms of the pipelines themselves, one of the other reasons why we went with open telemetry is we have some internal attribute enrichments that we do specifically to our product and the way we use things. We actually implement those inside of the collector itself. So we have some custom processors which enhance the data in various ways. We actually validate it. We actually drop data which is not—contains things which shouldn't be there, for instance. So we do a lot of validation on the fly as things go through the collectors themselves before we write it out to our various data stores. I think I think that hit most of the points you're asking for. Did I miss any of the things you're asking?

**Ree:** I think you got most of them. And yeah, actually, one follow-up question I had was, since you mentioned you deploy your collectors in Kubernetes via Helm charts, have you used the open telemetry operator to manage any of your collectors? Is that something you played with?

**Cal:** We have not. The open telemetry operator sort of came out after we were already doing stuff. In fact, we don't—but not for any particularly good reason. No.

**Ree:** Okay. Fair enough.

**Cal:** So, it's that we had—you can consider it sort of tech debt. You know, we had the Helm chart and all of that beforehand. So we've just continued doing that, and we're kind of used to managing things that way. We actually will have an internal shift in our deployment tooling coming up for the organization. So that might be an opportunity to switch over how we're actually managing things. But at the moment, yeah, we don't have any experience with the open telemetry operator at the moment.

**Ree:** Got it. Got it. And I have another similar question. Since you do manage a fleet of collectors, have you played around with the opamp protocol for doing that?

**Cal:** We've not. Again, we’ve tried very hard to make our fleet essentially standalone. So even for configurations and things like that, we don't actually reach out to anything else. We really deployed sort of static config maps for that. That was basically a choice that we made just to—because we have such a large deployment and because it's distributed so widely, we wanted to make sure we were as resilient against networking problems as possible. So even for things like configuration changes, we push out new configs for those things rather than doing something that—

**Ree:** Got it. Got it. Do you have a—it's not to—not to say that's a bad idea. It's just it's a choice. It's a choice that we made.

**Cal:** Of course. Of course. That makes sense. What about maintaining the collector pipeline? Is there a dedicated team? Is that just whoever is on the observability team, or how do you maintain the local configurations and the pipelines?

[00:20:00] **Jerome:** Yeah, so we do have an observability team. So we are split between Poland and here. Our other big engineering office is actually in Krakow, Poland. So we have two observability teams split between the two regions. Each one has I think around five people at the moment. But collectively we are responsible for maintaining those pipelines and that configuration. Almost, I would say almost entirely, we are responsible for that. There are a few areas where we share responsibility with certain places where we pull in configurations. One of them is our Kubernetes team. They actually define part of our configuration for which metrics we actually pull into the global system rather than just keeping it local to their Prometheus instances. But by and large, we are responsible for that entire configuration. We try to avoid shared responsibility there just because we want to make sure that, you know, shared responsibility always means no one's responsible. So we try to really avoid that.

**Ree:** So, as you know, the project has matured, and it sounds like you've got a pretty good setup that works well for you guys right now. How has the implementation evolved over the years, you know, as things have become GA? Have you added additional instrumentation signals? How has that looked as you've kind of grown alongside the project?

[00:21:50] **Cal:** Yeah. So, yeah, it's actually an interesting question in the sense that the thing that surprised me overall is that the deployment that we have hasn't really changed in terms of its architecture since the beginning. It's been the same architecture the entire time. Now, we've made tweaks here and there, and we've simplified things, you know, as new features have come out and that kind of stuff, but by and large, it's stayed the same. I think that is amazing given the fact that it was really sort of pre-alpha when we started. So we've gone through, you know, we sort of gone through the alpha, beta, you know, and things are starting to become stable now, and we've not really noticed any really heavy changes that we've needed to make in the platform. Now, that being said, I mean, we have added new signals. So at the beginning, of course, it was metrics and traces that were there at the beginning. Logs came in afterwards. We had logs coming into our system even earlier than that, so we had some of our own receivers to do some of that before it was an official signal. We've sort of folded those changes into the platform as they come along, and it hasn't really been terribly painful in doing that. As far as the community goes, the alpha, beta, stable kind of thing, we've not seen really any negative consequences from that. We've had to update on occasion, but by and large, those updates are pretty easy. Jerome has been doing a lot of the following for the collectors. We have our own build of the collector, which of course we try to keep in sync with the external one, and Jerome has been doing quite a lot of that, so you can probably give some feedback on how difficult that process is and where we've run into issues.

**Jerome:** Oh yeah, I'm sure. I mean, for the most part, I don't know if this speaks to open telemetry or the automation we put around it, but as far as just updating the collectors, it's been a relatively painless process. I mean, sure, there have been a few changes that we've had to team up and just file down on as a team and like, "Hey, what path do we want to go down?" But as far as just like pushing out those changes, they've been pretty simple, especially with how elaborate and detailed the white papers or the change logs have been out of open telemetry, just explicitly telling, "Hey, these libraries or modules you've been using—here's an introduction to breaking changes or an API that's going to be deprecated." But overall, it's been quite an easy experience, and getting more people on our team to actually utilize that process has been fairly easy as well.

**Ree:** I have a follow-up actually on building your collectors because I think out of all the people we've talked to, I think you guys might be the ones that have built your own collector, which makes me very excited. In terms of building your own—did you have—what was your experience around that, like with as far as the documentation provided on the open telemetry site? Because that's another thing that we want to hear from practitioners of OTEL is how usable is the documentation, and have you noticed an improvement in the documentation? Even building your own collector, how useful was that documentation? Did you have to do a little extra Sherlock Holmesing to figure stuff out? Also, were you able to build a nice streamlined process for building the collector that—did you have to go outside of the confines of what was already documented to do that?

**Cal:** Yeah. So, I'm not sure I'm going to be able to provide necessarily documentation feedback on the documentation, but the process as a whole—I mean, one of the things we started with was we had to start with our own build of the open telemetry collector mainly because we do have custom receivers, we have custom processors, we have custom exporters as well, and we kind of needed that at the beginning to cover some of our use cases. So we were forced basically at the beginning to build our own collector, right? At the beginning, it was very painful. It was basically us cloning the entire repository and then making the changes we needed to build it. Once the collector builder came along, that made that whole process so much easier, and it was a great addition to that process. Right now, basically we have moved to a situation where we have our own repository, and basically we have the single config file for the collector builder that just tells us what we pull in from contrib and from the main open telemetry collector, and the only thing we have in our repo at this point is our own custom stuff. Once the builder was there, everything since then has been really, really easy and smooth to build our own stuff. We did that right at the beginning, so before there was documentation, so it's worked since then. I can't say whether the current documentation is good there or not, but we use the builder and it works seamlessly for us.

**Jerome:** And just as a follow-up because you mentioned you built your own components. How was that experience of building your own components? Was that tricky? What was the—because that’s definitely something that I would say personally feels a little bit lacking in the open telemetry documentation for building your own receivers, processors, exporters, that kind of thing. How was that for you?

**Cal:** Yeah. So I think in terms of the team itself, I think the biggest hurdle there was learning Go. We weren’t—so I think from the team standpoint that was sort of the biggest initial obstacle for that. All of us, I think, have some good experience with Go now, so I don't think that's an obstacle anymore. In terms of how to do it, we again were doing this very early. So it came at a time when it was basically, "Well, let's take one which we know works, copy it over, and then make the changes you need to make our own stuff." Since then, the documentation has gotten much better, and it's much easier to sort of start with those things and get them done. But you know, I must say the process of just copying something that works and moving it over and making the changes you need also worked pretty easily as well. For an experienced programmer, I don't think it's a huge barrier to do those things. Now, there are some things which I would love to have better documentation on, like in terms of internal telemetry—how to hook in better to the internal telemetry and that kind of stuff. We sort of worked it out, you know, by reverse engineering what was there, but some of the things like that would be much more helpful if they were better documented, for instance.

**Ree:** Thanks. Oh, I was just going to say, yes, I remember that when we were catching up a bit yesterday, and we definitely want to get into more of the feedback that you have for the project. I know you talked a little bit about the issue with conventions for internal data schemas. So if you want to go into a little bit more about that and anything else that you would like to see improvements in or feature requests, things like that.

[00:30:00] **Cal:** Yeah. So, first let me preface this by saying I think probably from what I've said already, I am 100% in with open telemetry, and I think it's a great platform. What the maintainers are doing is fantastic, both on the documentation level and the code level, right? So that's all great. This isn't feedback necessarily in terms of the actual collector implementations or open telemetry project as a whole, but I think it's sort of feedback on I think where we're going in our own journey, and probably that has some bearing on other people as well. As we're generating more and more telemetry and it's becoming more and more useful for people, we have a lot of downstream consumers now. So we have an entire AI division who's now looking at our telemetry. We have people managers who are trying to do reporting off of this, and getting standard data is becoming more and more important. One of the reasons we had to have a custom processor is we do have standards for the attributes which come into the system. So we enforce a sort of a global schema on everyone, even though that's extensible. Now how that fits in with the semantic conventions, how that fits in with the Elastic Common Schema, in general, how this fits in with sort of data contracts and how you have contracts between the producers of the data and the consumers of it. All those things are things we're thinking very hard about right now, and we don't have good ideas on how to apply that across the board and good ways and the level at which we need to do that. I think discussions about the semantic conventions and the ECS and how to do things like data contracts with open telemetry data I think is going to become more and more important. I see that in our own organization, and it would be good to come up with sort of common themes there and common tooling and common ideas. A little more convergence, I think, within the community there I think would go a long way. In terms of other feedback, the tech stack, I think in general, is really, really solid, and the way things are managed I think is great. So I don't have a whole lot of feedback there.

**Ree:** Yeah, so I think it's mainly looking forward and seeing how we can sort of use the technological base that we have and come up with better ways of standardizing the data which is produced, making it more useful for downstream consumers, I think is the main thing I would like to see discussed more.

**Adriana:** Right on. Thank you, and I think we have a bit of time to catch up on some audience questions. So let's see, one of them is from Doug. Doug on YouTube wants to know, with regards to collectors, their dream is that cloud providers will offer as a boring text similar to an SMTP or SFTP. Is Doug crazy to want that?

**Cal:** We—the first question we ask any vendor now is do they support OTLP? So, you know, we treat that as our standard protocol, and it's a negative check mark against someone if they're not supporting it already. So we're pushing all of our vendors and various tech products and whatever to support that as much as possible. I don't think that that's a pipe dream. I think pushing people in that direction is a good thing. As I said, we are very heavily a Microsoft shop, so we use Azure very much, and the feedback we've consistently given them is we want Azure Monitor and those types of things to produce open telemetry signals because we don't want to have to do that translation ourselves.

**Adriana:** Thank you. And Manas, I see you have a question about OPM tutorials. We will respond directly with some resources for you in the LinkedIn chat. In the meantime, there was another question. Did you ever feel that there were classes of telemetry data that open telemetry didn't collect well and you needed to add another collection method, such as eBPF?

**Cal:** We've had to come up with some custom receivers and things like that that weren't covered beforehand. One of the early things we did was there was no way to sort of provide a web hook into open telemetry at the beginning. There is now. So that's one of our legacy receivers that we've had, and that was something that was not difficult to write but something that was sort of lacking. In terms of other classes of telemetry, I would say that one of the biggest gaps we've had internally as an organization is front-end telemetry. That's something that we're actually closing now. We actually have a project currently going on basically to collect more of our front-end telemetry with the open telemetry SDK—the JavaScript one. So that was something that we internally found difficult to do for many reasons, and we're now sort of closing that gap. In terms of other types of telemetry that's been difficult, I don't think we've had any real issues. Most of the things that we've had to collect, if it's not OTLP or things like Splunk, Hack, or Fluent Bit and things like that, we've been able to get things into our collectors fairly easily.

**Adriana:** Perfect. Thank you. And I think our time is just about up. Were there any last—well, not last parting words? 

**Cal:** No, just thanks. Thanks for inviting us, I think is the last thing from our side. If anyone wants to reach out or whatever, I am in the Slack, so feel free to reach out to me directly if you have questions or whatever. Happy to discuss what we've done in detail if it's helpful.

**Ree:** Thank you so much for offering that, and thank you so much, Cal and Jerome, for being on and sharing your journey with us. We will link to our hotel SIG and user channel as well, so you can find all of us in there. You can also reach out to Cal directly via the CNCF Slack as well. And so we do have a link, Manas, for you that we'll put up here in a second that links to OPE documentation, and—oh, well, we might have time for one more quick question. This is kind of—what do you think about auto instrumentation?

**Cal:** Me personally, I think it's a great thing. We use it where we can, and in fact, for the front-end stuff, we are planning to use some of that auto instrumentation there. The downside we've seen—we have tried to use it, and we do use it on the .NET side in the .NET SDK for HTTP requests and things like that. The problem we've run into there is volume. So it tends to be difficult to sort of scale down to exactly which ones you want. The auto instrumentation tends to be overly broad sometimes, but that's the only sort of downside we've seen with it. So in general, we think it's a great thing, and if it works for you, then yeah, definitely use it.

**Adriana:** Awesome. Thank you. Well, Ree and I would like to thank you again so much for being on here. This has been really great, and I'm sure I'll have myself and other people will have more questions to learn more about open telemetry implementation since you guys were early adopters. We do have a couple things we want to share real quick before we head out. Hotel Community Day is coming up on June 25th, I believe, and we are currently accepting talk proposals. So, if you would like to submit a talk, we definitely encourage you to submit a topic for open telemetry day. It's going to be in Colorado as part of Open Source in Denver. It's a collocated event of Open Source Summit North America in Denver, Colorado.

**Ree:** Yes. So if you want to come out—well, I guess that's too late to ski, but the beautiful mountains, I guess.

**Adriana:** Yes. Also, if you are going to—or if you are already in Europe and you're planning to head out to KubeCon EU in London in two weeks—less than two weeks.

**Ree:** Yes.

**Adriana:** We would love to see you there. Ree and I will be speaking at the event, and we have a blog post about the event that outlines all the open telemetry specific talks. The recordings, if you're not able to make it, will be available on the CNCF YouTube channel, I think about one to two weeks after, I think—depending.

**Ree:** Yeah, they're usually pretty fast. Sometimes they get it within a couple of days, so just keep an eye out.

**Adriana:** Yes, so if you can make it in person, you are definitely welcome to check out the recordings on YouTube. And I believe that's it. Do you want to mention also if you are at KubeCon EU, we are going to be hanging out at the hotel observatory off and on. So if you're there, come say hi. The observatory is always lots of fun because there's usually tons of hotel contributors, maintainers, etc. hanging out. It's great to have ad hoc conversations with folks. There's also going to be—if you're a KubeCon hotel project updates, I strongly encourage folks to join that as well if they want to see what's new and exciting in hotel. That's always a fun session to attend. I think it's usually like half an hour that it's slated for.

**Ree:** And also, if you made this recording but you have a friend who couldn't make it, you can replay on our LinkedIn—just share the same link for the live recording with your friends, or this will also be available on our hotel YouTube channel. So tell your friends—officially tell your friends.

**Adriana:** Tell your friends.

**Ree:** All right. Thank you all so much, and we will see you next time. Bye.

## Raw YouTube Transcript

Hello. Welcome everyone to a new episode of Hotel Me, formerly known as the end user Q&A. I'm very excited to be here. Um, if you have been here before, I am Ree and I have Adriana here with me. I am coming to you live from Vancouver, Washington, not BC. Um, Adriana, where are you coming from? I am coming to you live from Toronto, Canada. So, we're at opposite ends of the coast, which is pretty We are. Yeah. And so, for those of you who are familiar with the flow, um, hang tight. We are going to bring our guests on pretty soon. Um, for those of you who are new, welcome. So, how this is going to go is we are going to bring on our two special guests from a an end user organization on shortly. We're going to have a quick introduction. Um, they'll introduce us to their tech stack and how long they've been using open telemetry. This one's gonna be a really interesting one because they were actually one of the first guests we talked to when we started this segment um back in 2022. So, I'm pretty excited to follow up with them and see, you know, how far they've come in their journey, what, you know, what kind of struggles they've had along the way and um what they've figured out. Um and then we'll get into some time for them to give us some feedback about the project. And we should also have some time at the end for any audience members to ask questions. Um, if you do have questions that come up as we go, feel free to pop them in the chat, whether you're on LinkedIn or YouTube, and we will get to them as appropriate um during the conversation. If not, we will wait until the end. And with that, I believe we can bring our guests on. Oh, and Adrian, do you want to talk about our resources just a little bit? Oh, yeah. So, uh, our resources, um, I believe this will I'm going to scan the QR code myself because I don't remember what this does. Um, this will take us to this takes us to open telemetry.io. Um, and we have a page in the hotel uh, site all about our um, all about our end user SIG. If you want to learn more about the end user SIG, we welcome you to join. I believe there's a a link on there also for joining our uh Slack channel on CNCF Slack. So, if you're already on CNCF Slack, um please come find us. We welcome everyone. Um people come along to ask questions. We might not have all the answers, but we can definitely direct you to where you need to be. Oh, and also we would love to know where you are listening from. So feel free to put that in the chat as well so we can we just like to know where people are listening from. And so the guests we have for you today are Jerome Johnson and columnist from relativity. Hello. Hi everyone. Howdy howdy. Hello. Would you we would love to um learn about you and can you tell us a little bit about your role in the company and kind of what relativity is focused on? You can go ahead, Cal. Alphabetical order and whatnot. Oh, okay. All right. Excellent. Very democratic. Yeah. So, a little bit about relativity. So, Relativity is a company that has a predominantly a SAS product that uh provides services to the legal community. So, we automate essentially all the uh processes around legal eiscocovery. So, exchange of documents and all of that. Um it is a SAS platform. We have a lot of companies around the world that actually use it and uh we as observability are really tuned toward how how the cloud our our SAS project is actually working at any particular point in time. Um as far as me I um I started with relativity about four almost five years ago now. I was brought in basically to upgrade uh the way we do telemetry and as Ree mentioned we were one of the probably the first first ones to have a chat with with you folks but also one of the first adopters of open telemetry in production. So we were very very early adopters of of open telemetry. Um I'm an architect here. Um and then I'll let Jerome introduce himself. Hello. As Cal said, we're on the observability team. My name is Jerome. Um, I've been at the company for about 10 years or so now. Uh, but, uh, I've only been on the observability team, specifically the one that's dealing with open telemetry for a little over a year now. So hopefully I can bring some perspective on like how easy it is to use from an on the internal side and how nice it is to so looking forward to chatting it up with you guys. Cool. That's so exciting. Well, um I think this is a great opportunity for us to uh start digging in and can um can either of you or both of you give us some insights into your tech stack and architecture? Yeah, so I could talk about that a bit and uh you know, so like I said, we were an early adopter of open telemetry. Uh what we had before that was an entirely bespoke I would say uh system for collecting telemetry from a lot of different sources. Um, in addition to that, um, we we did use New Relic at the time, but we also had a lot of other vendors in there, too. So, it was kind of a nightmare from an operational standpoint to actually do any uh troubleshooting and debugging because you had to go to lots of different sources to to sort of put all that telemetry together in your head. Um, at the time when I was hired, we we uh the company made sort of a decision to try to unify everything. We looked around to see what would be the best platform for doing that. Um and the one that sort of checked all the boxes even though it was really immature was open telemetry. So that was the one which we sort of pointed to to do that. Now internally we have a very complicated platform. Um and maybe this is time to bring up the diagram on on what we actually do now with um um with open telemetry. We have 1500 plus different services running on our platform and that is a huge mix of things. We are predominantly a Windows andnet shop. So most of that is actually Windows and .NET. Um but we have a mix of basically every other language in the world. Uh so we have a very very complicated tech stack and we need to pull in the telemetry from all those different things. So most of the complexity in our observability platform and I'll probably mention at various points. So if I say RELI, that is our observability platform that's built over open telemetry. Um most of the complexity is just being able to pull those signals in from lots of different sources and you see most of them here. We really prefer that people use the open telemetry SDKs now and we push that as hard as we can. But we still have people using our old platform which is relatively APM which we translate into open telemetry now. We have things like SNMP. uh we pull logging from lots and lots of different places and that's probably the part which is most most difficult and most uh heterogeneous in terms of the platform itself. We have a whole fleet of uh open telemetry collectors that we run in 20 20 different regions around the world. Each one of those is load balanced. So we're running we're running anywhere between uh depending on the time of day and all that between 200 and probably 800 individual collectors around the world to to pull all this telemetry in. Um and then we we send that to various uh data stores for operations. This is really new relic at this point. Um so that's where most of our uh engineers go to actually see what the what the telemetry is doing. We have a subset of people who use launch darkly and use that for future releases. So we send some set of telemetry there. Um and then we actually archive all of our telemetry for compliance reasons for a year and for forensic analysis and stuff like that. Uh we also have a reporting platform that uses the Azure blob storage basically as as its back end to to make that reporting for managers and things like that. Um so this makes it look simpler than it is. It's a lot more uh gnarly than than this diagram uh uh lets on. But we do have a large scale. I mean 20 plus regions around the world, 1500 services all feeding into this is is a fairly complicated platform. Dang, that was really cool actually. Um questions. So many questions. I do actually have a lot of um followup questions, but I also want to um get into why open telemetry like how like when exactly I know you mentioned, you know, kind of back in 2022 you you know your team was early adopters. Um how did you come upon open telemetry and like why did you decide like oh this makes sense for us? Yeah. So you know I think the thing which which sort of ticked all the boxes for us we we came from a place where we were having to maintain our own observability libraries um and given the fact that we werenet and then trying to diversify into lots of different languages uh meant that we were having to translate that thing again and again and again. Um and on top of that we also had uh well we were using New Relic at the time and we had a lot of the New Relic agents in the mix as well. So we had this this weird mix of our own libraries for some stuff. We had uh a mix of of things from open from New Relic uh with the uh with the agents and all of that kind of stuff. Um and we had a lot of people who were writing to New Relic directly from from their code as well uh as well as to other sources. So one of one of the things which was really complicated on our side was every time we wanted to move something or we wanted to change something it meant code changes. Um and that's something we wanted to avoid going forward. So we wanted to standardize on something which was ideally open source that we could include in our uh in our services one time and maintain that and then change what where we put things but didn't didn't have to change the code every time. So we're really looking for um you know something that would give us stability in terms of the codebase but allow us the flexibility to send things to other places as we needed to do that and open telemetry was the only one which sort of ticked those boxes for us at the time and the only worry we really had at the point was that it was it was a new project. Um there was a lot of risk in going that direction. uh we decided in the end it it wasn't an easy decision. We we actually discussed this a long time internally but we decided that it was worth the risk to to go ahead and adopt that because it looked like it was going to take tick the boxes for what we wanted to do there. Um you know so basically it's it's to isolate basically our code base from from the from what we do with the telemetry afterwards which was sort of the selling point for what we wanted to do there. That is very awesome. Um I don't know I don't know if Jerome has anything. Uh oh yeah I yeah just to add on to that um I can give somewhat of a perspective to what it was like before we had open telemetry like um I was serving on the performance team for a good number of years and um it was very challenging to do uh analysis of specific applications just because like Cal had mentioned things were it was kind of the wild west on how we handled metrics like performance metrics and monitoring are are somewhat look similar, a little different, but enough to where like every time we had to analyze a specific application, we would have to do a deep dive on a set application like what telemetry are you guys putting out? Oh, you're not putting out any at all. And then we'd have to like dig into the code and that would really muck up like how much time we could actually like drill down into what the core problem of said application performance uh perspective was. Usually it would take half the time we would do an analysis it would be like trying to understand what the product was and then actually coming up with a solution would be the the la latter 50 or 40% they have. So um since joining uh observability like and even like even though Cal showed a simplified version of um our reli stack and uh it's a like you said it's a bit more complicated once you drill down into it. I was amazed just how like how much cleaner it is to just understand like what our what relativity is doing as a whole. So, uh take that with what you want. That's very cool. Um now as a as a follow-up question, um one of the things that we are always curious about in the SIG is collector setup. Um what kind of you you mentioned I I believe um Cal mentioned that you guys manage a fleet of collectors. Um how do you manage that fleet of collectors? How many can you give us an idea of how many collectors? Is there a particular setup that you use? Like we we would love to know. Yeah. So, you know, I think um in terms of how things are usually deployed, I think you call it a gateway setup. So, all of our collectors are basically in a gateway setup. Um so, people send their telemetry to us, we process it and send it on. Um over the years, we've had more or less complicated pipelines, but in general, we have essentially a single set of collectors with a common configuration that handles all of that uh all of that uh telemetry. We um so in terms of the numbers of things so we are running over 20 plus regions each uh each region has somewhere between depending on the load between 10 and probably 50 to 100 uh instances themselves. So we're running at any point in time somewhere between 200 and 500 individual collectors. These are all running in Kubernetes. Um we are uh very focused on compute and Kubernetes. So that's where almost all these things are running. In terms of the deployment itself, we actually handle all of our deployments um uh via Helm charts. So we use Helm charts basically to deploy these all out to the um to the um uh Kubernetes clusters. Uh we do take pains to make sure that essentially all of our configurations are isolated. So our collectors except in a very few cases do not reach out to external services. We manage the configurations all locally to make sure that they are as available and uh as as available and robust as possible, right? Um so that's kind of how we do all of that. In terms of the pipelines themselves, um one of the other reasons why we went with open telemetry is we have some internal uh I would say attribute enrichments that we do specifically to our product and the way we use things. We actually implement those inside of the uh collector itself. Um so we have some custom processors which en enhance the data in various ways. We actually validate it. We actually drop data which is not uh which contains things which shouldn't be there for instance. Um so we do a lot of validation on the fly as things go through the the collectors themselves before we write it out to our various data stores. Um I think I I think that hit most of the points you're asking for. Did I miss any of the uh the things you're asking? I think I think you got most of them. And and yeah, actually one one follow-up question I had was uh also since you you mentioned you deploy your your collectors in Kubernetes via Helmcharts, have you used um the hotel operator to manage any of your collectors? Is that something you played with? We have not. The the hotel operator sort of came out after we were already doing stuff. So in fact, we uh we don't uh but not for any particularly good reason. No. Okay. Okay. Fair enough. So, it's it's that we had uh you consider it sort of tech debt. You know, we had the Helm chart and all of that beforehand. Um so, we've just continued doing that and we're kind of used to managing things that way. Um we actually will have an internal shift in in our deployment uh tooling uh coming up for the organization. So, that might be an that might be an opportunity to switch over how we're actually managing things. But at the moment, yeah, we're not we don't have any experience with the uh open telemetry operator at the moment. Got it. Got it. And I I uh another similar question. Um since you do meet manage a fleet of collectors, um have you played around with the opamp protocol for for doing that? We've not again again we've tried very hard to make our our fleet essentially uh standalone. So even for configurations and things like that, we don't actually reach out to anything else. We really deployed uh sort of static config maps for that. Um that that was basically a choice that we made um just to because we have such a large deployment and because it's distributed so widely, we wanted to make sure we were as um resilient against networking problems as possible. So even for things like configuration changes, we we push out new new configs for those things rather than than doing something that Got it. Got it. Do you have a It's not to not to say that's a bad idea. It's just it's a choice. It's a choice that we made. Of course. Of course. That makes sense. That makes sense. What about um maintaining the collector pipeline? Who is there a dedicated team? Is that just whoever is on the observability team or how do you maintain the local configurations and the pipelines? Yeah, so we do have an observability team. Um, so we are uh split between Poland and here um our other big engineering office is actually in Krakow, Poland. Um, so we have two uh two observability teams split between the two regions. Uh, each one has I think around five people at the moment. Um but collectively we are responsible for maintaining those pipelines and that configuration. Um almost I would say almost entirely we are responsible for that. There are a few areas where we share responsibility with certain places where we pull in configurations. One of them is our Kubernetes team. um they actually define part of our configuration for which metrics we actually pull into the global system rather than just keeping it local to their Prometheus instances. But by and large we are responsible for that entire configuration. Um and we try to avoid shared responsibility there just because we want to make sure that uh you know shared responsibility is always means no one's responsible. So we try to really avoid that. So, as you know, the project has matured um and it sounds like you've got a pretty good setup that work is working well for you guys right now. Um how has the implementation evolved over the years, you know, as um things have become GA? Have you added additional instrumentation signals? Um how has that looked as you've kind of grown alongside the project? Yeah. So, um yeah, it's actually an interesting question um in the sense that the thing that surprised me overall is that the deployment that we have hasn't really changed in terms of its architecture since the beginning. It's been the same architecture the entire time. Now, we've made tweaks here and there and we've simplified things, you know, as new features have come out and that kind of stuff, but by and large, it's stayed the same. and and that I think is amazing given the fact that it was really sort of pre-alpha when we started. Um so we've gone through you know we sort of gone through the uh the alpha beta you know and things are starting to become stable now and we've not really noticed uh any really heavy changes that we've needed needed to make in the platform. Now that being said I mean we have added new signals. So at the beginning of course it was metrics and traces that were were there at the beginning. Logs came in afterwards. Um we had logs coming into our system even earlier than that. So we we had some of our own receivers to do some of that before it was an official signal. So we've we've sort of folded those changes into the platform as they as they come along and it hasn't really been terribly painful in doing that. Um, so you know, as far as the community goes, the, you know, the the alpha beta stable kind of thing, you know, we've not seen really any negative consequences from that. We've had to update on occasion. Um, but by and large, those updates are pretty easy. and and Jerome I mean his he's been doing a lot of the following for the collect we have our own uh build of the collector uh which of course we try to keep in sync with the external one and Jerome has been doing quite a lot of that so you can probably give some feedback on how difficult that process is and and where we've run into issues. Oh yeah, I'm sure. I mean, for the most part, um, uh, it's, and I don't know if this speaks to open telemetry or the automation we put around it, but as far as just updating the collectors, it's been a relatively painless process. I mean, sure, um, there have been a few changes that we've had to uh, kind of team uh, just file down on as a team and like, hey, what path do we want to go down? But um as far as just like pushing out those changes, they've been pretty simple, especially with how elaborate and um detailed the white papers or the change logs have been out of open telemetry just explicitly telling hey these libraries or modules you've been using uh they here's an uh we're introducing a breaking change or an API is uh going to be deprecated. Uh but overall it's been uh quite e uh use of ease experience and um uh getting more people in our team to actually utilize that process has been uh fairly easy as well. Um, I I have a followup actually on on um on on building your your collectors because I think uh of I think out of all the people we've talked to, I think you guys might be the ones that have done uh have built your own collector, which makes me very excited. Um did you um in in terms of building like your your own dro um did you have like what was your experience around that like with as far as like the documentation provided on the hotel site because that that's another thing that we we want to hear from from um practitioners of OTEL is how how usable is is the documentation and and have you noticed an improvement in the documentation? So even um building like building your your own collector, how useful was that documentation? Did you have to do a little extra like Sherlock Holmesing to figure stuff out? And and also um were you able to uh build like a nice streamlined process for building the collector that uh uh like did you I guess have to go outside of the confines of of what was already documented to do that? Yeah. So um I'm not sure I'm going to be able to provide necessarily documenta you know feedback on the documentation but the process as a whole I mean we one of the things we started with was we had to start with our own uh our own build of the open open telemetry collector mainly because we do have custom custom receivers we have custom processors we have custom exporters as well and we kind of needed that at the beginning to cover some of our some of our use case so we were forced basically at the beginning to to build our own collector, right? Um, at the beginning it was very painful. It was basically us cloning the entire repository and then making the changes we needed to build it. Um, once once the collector builder came along um that made that whole process so much easier and it was it was a great addition to that uh that process. Um right now basically we have moved to a situation where we have our own repository. Um and basically we have the single config file for the uh for the collector builder that just tells us what we pull in from from contrib and from the uh the main open telemetry collector and the only thing we have in our repo at this point is our own custom stuff. Um so once the once the builder was there everything since then has been really really easy and smooth to to build our own stuff. Um we did that right at the beginning. So before there was documentation so um and it's worked since then. So I I can't say you know whether the current documentation is good there or not but we use the builder and it it works seamlessly for us. And uh just as a followup because you mentioned you you built your own components. Um, how how was that experience of of building your own components? Was that um was that tricky? Like what was the um uh because I that that's definitely something that um I I would say personally feels a little bit lacking in the hotel documentation for from building like your own receivers, processors, exporters kind of thing. How how is that for you? Yeah. So I think in terms of the team itself um I think the biggest hurdle there was was learning go we we weren't uh we weren't um so I think from the team standpoint that was sort of the biggest uh initial obstacle for that um all of us I think have some good experience with go uh now so I don't think that's an obstacle anymore um in terms of how to do it um we again were doing this very early. So it came came at a time when it was basically well let's take one which we know works copy it over and then make the changes make the changes you know that we need to do to make our own stuff right. Um since then since then the documentation has gotten much better and it's much easier to sort of start with with those things and get them done. Um, but you know the I must say the the process of just copying something that works and moving it over and making the changes you need that also worked pretty easily as well. So you know I don't think there's a huge for an experienced programmer I don't think it's a huge barrier to do those things. Um now there are some things which I would love to have better documentation on like in terms of internal telemetry how to you know how to hook in better to the internal telemetry and and that kind of stuff. Um we sort of worked it out uh you know by uh reverse engineering what was there but some some of the things like that would be much more helpful if uh they were better documented for instance. Thanks. Oh, I was just going to say um yes, I remember um that when we were catching up um a bit yesterday and we definitely want to get into more of like the feedback that you have for the project. Um there was a an audience question that I think is interesting from Sumit. uh how much data size do you consume and have you found the need to reduce it for example via tail sampling or other methods and from application logs and security logs perspective. Yeah. So uh in terms of the data that we push through the system we are somewhere around uh we write typically somewhere between 10 and 15 terabytes a day. So that's sort of the the uh the volume of the data that we we consume. Um the in terms of the split between logs and and and traces and metrics, it's not quite an even slip between them, but you know, it's we we we uh take all of those things in, right? Um so it's it's not so different between the between the three of them. Um in terms of the platform itself, for the stuff that flows through the collectors, we've never had an issue with scale. uh we've been able to take whatever people have thrown at us uh because we have autoscaling on all of our collectors and everything else. Um we've not run into any issues at that scale and there are times when that scale goes up by a factor or two because someone's doing something stupid. So you know uh so in terms of of tech and all of that things have just scaled correctly. The place where we've had to reduce data is mainly because we do push this data into New Relic and we pay for what we push into New Relic and we do have budget constraints. So we do manage essentially the volume of data we push into New Relic mainly for budget reasons rather than technical reasons. Um and in those cases yes we do sample down the traces. That's what gets sampled down most heavily in in the things that we we pull in. We obviously manage our logs by log levels. So we really do have dynamic ways of changing the log levels we use for that. Um typically coming from the Kubernetes platform as well. Um and metrics we tend to leave untouched. Um but there are cases where people abuse that and and we and we go after people. It tends to be rather than a proactive thing. It tends to be reactive. uh we see that uh we do look at who's who's pushing out data um and then go after them if we think that they're doing something unreasonable. Um but I will say in terms of tech, we've not run into any issues with scaling which I think is really amazing uh for for this for open telemetry in general. Um and most of the most of the things have been really uh based on budget rather than rather than the actual technical stack. Um, I wanted to uh ask a follow-up question to one of the comments that you made because actually overall um because the the vibe I get from from speaking with both of you guys is that your organization is very much into like very much supportive of of like the observability journey um and and open telemetry adoption. Can you talk a little bit more about like this culture of observability? Uh yeah, so you know the I was brought in basically when I when I started to to change the way we do observability at the at the organization you know and and a major part of that has been open telemetry and it's been really nice to see that the technical aspects of that have been have been actually the easiest parts uh you know the tech has just worked. uh you know not to say that there aren't hiccups and all of that but um you know the the tech stack is actually working really well and the data flows that we have in place I think have been uh have been really good. The harder part of this has been the cultural changes which you mentioned a bit. um and changing the way the organization deals with observability. Um so there's the you know resistant to migration. So you know there there are migrations there and people are resistant to changing code and we have a large code base. So that's one of the reasons why we still have some of our old libraries still producing some telemetry. Um so there there are res there has been resistance there. Um, and the the nice thing I've seen over the last few years is that as people get more and more experienced with the open telemetry and see what it can do for them and how it, you know, sort of simplifies the way they generate the telemetry, there's been less and less resistance going forward, which is which is a nice change to see, right? Um, so people are starting to see the benefits of this and they're starting to really, you know, uh, lean into open telemetry as a platform for that. Um the other thing I've seen on the other side of things is because we can write to various different places very easily. Um being able to do that has uh has sort of enabled people to to do nice new interesting things. And you know one of the things which is you know on the diagram I showed at the beginning was one of the places we write telemetry is launch darkly. Um, and they have a really cool feature where you can say, um, uh, it's called release guardian, and it's a way of actually, uh, changing a flag and having launch darkly ch make those changes automatically and progressively roll it out based on the telemetry it's seeing, right? So, we send spans there and if the spans look like we're getting a lot more errors or that the latency is getting larger or things like that, it will automatically roll back. And the fact that we could actually roll that out by just a configuration change on the open on the observability side is really cool because we've you know incorporated an entire another platform with our telemetry without having to change anything fundamental in our in our code and that uh for me is really really powerful and I think as people see things like that I think they're they're more convinced that this is a good choice. That's great. Um oh sorry go ahead. I don't know just going to give another before story as well. Um like like I mentioned uh I was on the performance team initially and we did have uh like Cal said like uh people are very uh hesitant for change. Uh like to give you some perspective like we did uh we had an initiative uh in the performance sphere to try to get more people to start using more performance telemetry and that dragged out for like almost a year and a half with teams like moaning and complaining all the way through uh just because of how non-industry standard it was and even at the end of it people weren't 100% sure like how much benefit it was bringing but Um right but to bring that to back to today we have another initiative uh along with open telemetry and we've gotten feedback basically just to get more telemetry uh maturity and coverage throughout all of our systems and services and we've gotten feedback from teams like almost immediately like how much they're benefiting from it as they mature their own uh application telemetry. uh and uh we we've seen like just like uh like response times to like issues that have come up uh that uh have brought about this because of open telemetry. So it's at least from someone who's been on both halves of this, it's like night and day. That's really exciting and you actually answered the question I was about to ask you. So that's great. Um Reese, do you have anything else that you wanted to touch upon? Um, I'll say quite a bit, but um, in the interest of time, um, you know, I did want to give some space for you to share additional feedback that we could share with the project maintainers. Um, I know you talked a little bit about, um, the issue with, um, conventions for internal data schemas. Um, so if you want to go into a little bit more about that and anything else that you would like to see improvements in or feature requests, things like that. Yeah. So, um, yeah. So, first let me preface this by, you know, I I think probably from what I've said already, you know, I am 100% in with open telemetry and I think it's a great platform and what the maintainers are doing is fantastic both on the documentation level and the code level, right? So, Um, that's all great. Um, and and this isn't feedback necessarily uh in terms of the actual uh collector implementations or open telemetry project as a whole, but I think it's sort of feedback on I think where we're going in our own journey and probably that has some bearing on other people as well as as we're generating more and more telemetry and it's becoming more and more useful for people. We have a lot of downstream consumers now. So we have an entire AI division who's now looking at our telemetry. We have people managers who are trying to do reporting off of this and getting standard data is becoming more and more important. Um one of the one of the things we built initially and that one of the reasons why we had to have a custom processor is we do have standards for the attributes which come into the system. So we enforce a sort of a global schema on everyone even though that's extensible. Um now how that fits in with the semantic conventions, how that fits in with you know the elastic common uh uh elastic common schema um in general how this fits in with sort of data contracts and how you have contracts between the producers of the data and the consumers of it. Um all those things are things we're thinking very hard about right now and we don't have good ideas on how to you know apply that across the board um and good ways and the level at which we need to do that. So you know I think discussions about uh the semantic conventions and the ECS and you know how to do things like data contracts with with open telemetry data I think is going to become more and more important. uh I see that in our own organization and it'd be good to come up with sort of common common themes there and common tooling and common ideas. Um so a little more convergence I think within the conver within the community there I think would go a long way. Um in terms of other feedback the tech stack I think uh in general is really really solid and you know the way things are managed I think is great. So I don't have a whole lot of feedback there. Um yeah so I think it's mainly looking forward and seeing how we can sort of use the technological base that we have and and come up with better ways of standardizing the data which is produced uh making it more useful for downstream consumers I think is the the main thing I would like to see discussed more right on thank you and I think we have a bit of time to catch up on some audience questions um so let's see one of them is from Doug Doug on YouTube and Doug wants to know with regards to collectors, their dream is that cloud providers will offer as a boring text similar to an SMTP or SFTP. Is Doug crazy to want that? Um we um the first question we ask any vendor now is do they support OTLP? Um, so you know, we treat that as our standard protocol and uh it's a negative uh a negative check mark against someone if they're not supporting it already. So we we're pushing all of our vendors and and various tech products and whatever to support that as much as possible. Um so I don't think that that's um I don't think that's a pipe dream. I think uh pushing people in that direction is is a good thing. Um as I said we are uh very heavily a Microsoft shop so we use Azure uh very much and the feedback we've consistently given them is uh we want you know Azure monitor and those types of things to produce open telemetry signals because we don't want to have to do that translation ourselves. Thank you. And Manas I see you have a question about OPM tutorials. we will respond directly with some resources for you um in the LinkedIn chat. Um in the meantime, there was another question. So, did you ever feel that there were classes of telemetry data that hotel didn't collect well and you needed to add another collection method such as eBPF? we've had to uh we've had to come up with some um some custom receivers and things like that that uh that weren't covered beforehand. One of the early things we did was there was no way to sort of uh provide a web hook into open telemetry at the beginning. There is now. Um so that's one of our one of our legacy receivers that we've had. Um and that was something that was not difficult to write but something that was sort of lacking. Um in terms of other classes of telemetry uh I would say that the one of the biggest gaps we've had internally as an organization is front-end telemetry. Um and that's something that we're actually closing now. We actually have a pro project in uh in currently going on basically to collect more of our front end telemetry uh with the open telemetry SDK the JavaScript one. Um so that was something that we internally was difficult to do for for many reasons and we're now sort of closing that gap. In terms of other types of cle telemetry that's been difficult. I don't think we've had any real issues. most of the things that we've had to collect if it's not OTLP or things like Splunk Hack or Fluent Bit and things like that where we've been able to get things into into our collectors fairly easily. Perfect. Thank you. And I think our time is just about up. Um were there any last well not last parting words? No just uh thanks thanks for inviting us I think is the is the the last thing from our side. Um and you know if anyone wants to reach out or whatever I am in the uh in the slack so feel free to reach out to me directly if you have questions or whatever happy to discuss what uh what we've done in detail if it's if it's helpful. Thank you so much for offering that and thank you so much Kyle and Jerome for being on and sharing your journey with us. Um we will link to our um hotel sig and user channel as well. Um so you can find all of us in there. Um you can also reach out to Cal directly via the CNCF Slack as well. Um, and so we do have a link manus for you um that we'll put up here in a second um that links to OPE documentation and oh well we might have time for one more quick question. This is kind of what do you think about auto instrumentation? Uh me personally I think it's a great thing uh you know and we use it where we can uh and in fact for the front end stuff we are planning to use some of that auto instrumentation there. Um the downside we've seen uh we have tried to use it and we do use it on the net side in the net SDK for uh HTTP requests and things like that. The the problem we've run into there is volume. So it it it tends to be difficult to sort of scale down to exactly which ones you want. So the auto instrumentation tends to be overly broad sometimes, but that's the only sort of downside we've seen with uh with it. Uh so in general, we you know, I think it's a great thing and if it works for you, then yeah, definitely use it. Awesome. Thank you. Well, uh, Adriana and I would like to thank you again so much for being on here. This has been really great and I'm sure I'll have um myself and other people will have more questions to learn more about open s implementation since you guys were early adopters. Um, we do have a couple things we want to share real quick before we head out. Um, hotel community day is coming up on June 25th, I believe, and we are currently accepting uh talk proposals. So, if you would like to submit a talk, we definitely encourage you to submit a topic for open telemetry day. It's going to be in Colorado as part of open source in Denver. It it's a collocca I think it's colllocated event of opensource summit North America in Denver, Colorado. Yes. So if you want to come out get um well I guess that's too late to ski but the beautiful mountains I guess. Yes. Um, also if you are going to or if you are already in Europe and you're planning to um head out to CubeCon EU in London in two weeks, less than two weeks. Yeah. Um, we would love to see you there. Adrienne and I will be speaking um at the event and we have a blog post. Yes. and we have a blog post um about the event that outlines all the open specific talks. Um the recordings if you're not able to make it will be available on the CNCF YouTube channel I think about one to two weeks after I think um depending. So yeah, they're they're usually pretty fast. Sometimes they get it within a couple of days. So just keep an eye out. Yeah. So if you can make it um in person, you are definitely welcome to check out the recordings on YouTube. And I believe that's it. Um, do you want to mention also if you if you are at CubeCon uh EU, um, we are going to be hanging out at the hotel observatory off and on. So if you're there, come say hi. Um, the observatory is always lots of fun. Um, because there's usually tons of hotel contributors, maintainers, etc. hanging out. So it's great to have like ad hoc conversations with folks. There's also going to be um if you're a CubeCon hotel project updates. Um so I strongly encourage folks to join that as well if they want to see like what's new and exciting in hotel. That's always a a fun session to attend. I think it's it's usually like half an hour um that it's slated for. So, and and also if you um if you made this recording but you have a friend who couldn't make it um you can replay on our LinkedIn um just share this the same link for the link the live um recording um with your friends or um this will also be available on our hotel YouTube channel um so tell your friends- official tell your friends. Tell your friends. All right. Thank you all so much and we will see you next time. Bye.

