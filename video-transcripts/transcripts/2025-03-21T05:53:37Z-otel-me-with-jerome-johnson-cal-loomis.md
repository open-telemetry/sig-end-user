# OTel Me...with Jerome Johnson  &amp; Cal Loomis

Published on 2025-03-21T05:53:37Z

## Description

Join us as we catch up with Relativity from our first convo with them back in 2022 -- How has the development of OpenTelemetry ...

URL: https://www.youtube.com/watch?v=DrD35XxTDsY

## Summary

In this episode of "Hotel Me," hosts Ree and Adriana welcome guests Cal and Jerome from Relativity, who discuss their journey with open telemetry and their complex tech stack. The conversation highlights Relativity's early adoption of open telemetry and how it has significantly improved their observability practices. They delve into their architecture, which includes a network of over 200 to 500 collectors managed via Kubernetes and Helm charts, and share insights into the challenges they faced with legacy systems. The guests emphasize the importance of standardizing telemetry data and the cultural shift within their organization towards embracing observability. They also address audience questions regarding data consumption, collector management, and the benefits of auto instrumentation. Overall, the discussion showcases Relativity's commitment to leveraging open telemetry for enhanced operational insights while providing constructive feedback for further improvements in the open telemetry project.

## Chapters

Here are the key moments from the livestream along with their timestamps:

00:00:00 Introductions and welcome to the stream  
00:02:20 Introduction of guests from Relativity  
00:05:30 Overview of Relativity's tech stack and use of OpenTelemetry  
00:12:00 Discussion on the decision to adopt OpenTelemetry  
00:18:00 Insights into the complexity of the observability platform  
00:25:00 Explanation of the collector setup and management  
00:32:00 Discussion on maintaining the collector pipeline and configurations  
00:39:00 Evolution of the OpenTelemetry implementation over the years  
00:45:00 Discussion on data volume and budget management for telemetry  
00:52:00 Sharing feedback on the OpenTelemetry project and its documentation  

Feel free to reach out if you need further assistance!

# Hotel Me Episode Transcript

**Hello and welcome everyone to a new episode of Hotel Me**, formerly known as the End User Q&A. I'm very excited to be here. If you've been here before, I am Ree, and I have Adriana here with me. I am coming to you live from Vancouver, Washington, not BC. Adriana, where are you coming from?

**Adriana:** I am coming to you live from Toronto, Canada. So, we're at opposite ends of the coast, which is pretty cool!

**Ree:** Yes, it is! For those of you who are familiar with the flow, hang tight. We will bring our guests on pretty soon. For those of you who are new, welcome! Here's how this will go: we will bring on our two special guests from an end-user organization shortly. We’ll have a quick introduction, and they’ll introduce us to their tech stack and how long they’ve been using OpenTelemetry. This segment is particularly interesting because they were actually one of the first guests we spoke with when we started this segment back in 2022. I'm excited to follow up with them and see how far they've come in their journey, what struggles they've faced along the way, and what they've figured out.

Then, we'll have some time for them to give us feedback about the project, and we should also have time at the end for any audience members to ask questions. If you have questions that come up as we go, feel free to pop them in the chat, whether you’re on LinkedIn or YouTube, and we will address them as appropriate during the conversation. If not, we will wait until the end.

With that, I believe we can bring our guests on. Oh, Adriana, do you want to talk about our resources just a little bit?

**Adriana:** Sure! Our resources will take us to [OpenTelemetry.io](https://opentelemetry.io). We have a page on the Hotel site dedicated to our End User SIG. If you want to learn more about the End User SIG, we welcome you to join. There’s a link on there for joining our Slack channel on CNCF Slack. If you’re already on CNCF Slack, please come find us. We welcome everyone. People come along to ask questions. We might not have all the answers, but we can definitely direct you to where you need to be. 

We would also love to know where you are listening from, so feel free to put that in the chat as well. Now, the guests we have for you today are Jerome Johnson and Calumnus from Relativity. 

**Jerome:** Hello!

**Calumnus:** Hi everyone!

**Ree:** We would love to learn about you. Can you tell us a little bit about your role in the company and what Relativity is focused on?

**Calumnus:** Sure! A little bit about Relativity: we predominantly have a SaaS product that provides services to the legal community. We automate essentially all the processes around legal e-discovery, such as the exchange of documents. It’s a SaaS platform used by many companies around the world. Our observability is tuned towards how our SaaS product is actually working at any given time. 

As for me, I started with Relativity about four, almost five years ago now. I was brought in to upgrade our telemetry. As Ree mentioned, we were one of the first to adopt OpenTelemetry in production. I’m an architect here. Jerome, would you like to introduce yourself?

**Jerome:** Sure! As Cal mentioned, we’re on the observability team. I’ve been with the company for about 10 years now, but I’ve only been on the observability team, specifically dealing with OpenTelemetry, for a little over a year. I’m looking forward to chatting with you all!

**Ree:** That’s exciting! I think this is a great opportunity for us to start digging in. Can either of you, or both of you, give us some insights into your tech stack and architecture?

**Calumnus:** Certainly! As I mentioned, we were an early adopter of OpenTelemetry. Before that, we had a completely bespoke system for collecting telemetry from various sources. We were using New Relic at the time, but we also had a lot of other vendors, which made troubleshooting and debugging quite complicated. 

When I was hired, the company decided to unify everything. We looked for the best platform for that, and even though OpenTelemetry was immature at the time, it checked all the boxes for us. Internally, we have a very complicated platform with over 1,500 different services running, predominantly on Windows and .NET, but also incorporating many other languages.

We prefer that people use the OpenTelemetry SDKs, but we still have some people using our old platform. We run a fleet of OpenTelemetry collectors in over 20 different regions worldwide, with load balancing. Depending on the time of day, we run between 200 to 800 individual collectors globally to pull all this telemetry in. We send that telemetry to various data stores for operations, primarily New Relic, which is where most of our engineers go to see what the telemetry is doing. 

We also archive all our telemetry for compliance reasons and have a reporting platform that uses Azure Blob Storage as its backend. Overall, we have a large-scale system supporting 1,500 services across the globe.

**Ree:** That sounds really cool! I have so many questions. I’d like to learn more about why you chose OpenTelemetry. When did you first come upon it, and why did you decide it made sense for you?

**Calumnus:** The main reason for us was the need to maintain our own observability libraries while trying to diversify into various languages, which required constant translations. We were also using New Relic, so we had a mixed setup that complicated things further. 

We were looking for something open source that we could include in our services once and maintain without changing the code every time we needed to move something. OpenTelemetry was the only option that ticked those boxes. There was a lot of risk in going in that direction, but we decided it was worth it. 

The primary selling point was to isolate our code base from telemetry processing, allowing us to adapt without constant code changes.

**Jerome:** To add to that, before OpenTelemetry, we had a very challenging time analyzing specific applications. We had to deep dive into each application to understand what telemetry they were emitting, which was often very inconsistent. Since adopting OpenTelemetry, it’s been much cleaner and easier to understand what Relativity is doing as a whole.

**Ree:** That’s fantastic! Now, regarding your collector setup, how do you manage your fleet of collectors?

**Calumnus:** We have a gateway setup for our collectors. They receive telemetry from users, process it, and send it on. We have a common configuration for our collectors, which handles all the telemetry. 

We run over 20 regions, each with 10 to 100 instances, so we usually have between 200 to 500 individual collectors running in Kubernetes. We deploy everything via Helm charts and make sure all configurations are isolated. The collectors typically don’t reach out to external services. 

We also implement custom processors to enhance the data, validate it, and drop any invalid data on the fly before writing it to our data stores.

**Ree:** That’s an impressive setup! Have you used the OpenTelemetry operator to manage any of your collectors?

**Calumnus:** We have not, mainly because the operator came out after we had already set things up. We have our Helm chart and are used to managing things that way. However, we are considering an internal shift in our deployment tooling, which might be an opportunity to switch over how we manage things.

**Jerome:** To add to that, we have focused on keeping our fleet standalone to avoid networking issues, so we push out new configurations rather than relying on external management.

**Ree:** That makes sense. Who maintains the collector pipeline? Is there a dedicated team?

**Calumnus:** Yes, we have an observability team split between our offices in Poland and here. Each team has about five people who are responsible for maintaining our pipelines and configurations, avoiding shared responsibility to ensure accountability.

**Ree:** As the project has matured, how has your implementation evolved? Have you added additional instrumentation signals?

**Calumnus:** Interestingly, the overall architecture of our deployment hasn’t changed much since we started. We’ve made tweaks and simplified things as new features have come out, but the core architecture has remained stable. 

We initially started with metrics and traces, then added logs. We’ve folded in changes as they come along, and updates have been relatively easy to implement. Jerome has been handling a lot of the collector updates, and it’s been a smooth process.

**Jerome:** Yes, updating the collectors has been relatively painless. The detailed change logs from OpenTelemetry have been very helpful in navigating updates and breaking changes.

**Ree:** That’s great to hear! Can you tell us about your experience building your own collector and components? 

**Calumnus:** We had to build our own collector because we needed custom receivers, processors, and exporters. Initially, it was quite painful, but once the collector builder came along, the process became much easier. Now we have our own repository with a single config file that tells us what we need and pulls in from the OpenTelemetry collector and contrib.

**Jerome:** In terms of building our own components, the biggest hurdle was learning Go, but now it’s not an issue. The process of copying existing components and modifying them worked well for us, and the documentation has improved over time.

**Ree:** That’s insightful! Regarding data volume, how much data do you consume, and have you found the need to reduce it via methods like tail sampling?

**Calumnus:** We typically write between 10 to 15 terabytes a day. We haven’t had scaling issues with our collectors, but we do manage the data volume we push to New Relic for budget reasons. We sample down traces and manage log levels dynamically to control what we send.

**Ree:** It sounds like your organization is very supportive of observability and OpenTelemetry adoption. Can you talk about the culture of observability within your organization?

**Calumnus:** When I started, I was tasked with changing how we approached observability. The technical aspects have been relatively easy, but the cultural changes have been more challenging. There was resistance to migration at first, but as people see the benefits of OpenTelemetry, that resistance has lessened. 

The ability to write telemetry to various platforms has enabled teams to do interesting things, which has encouraged adoption.

**Jerome:** To add to that, I’ve seen a significant shift in attitudes toward telemetry. Teams are now more responsive to adopting telemetry practices and seeing the benefits, which is a positive change.

**Ree:** Before we wrap up, do you have any feedback for the project maintainers?

**Calumnus:** Absolutely! I think OpenTelemetry is a fantastic platform, and the maintainers are doing a great job. However, as we generate more telemetry, standardizing data for downstream consumers is becoming increasingly important. Discussions around semantic conventions and data contracts will be vital for organizations moving forward.

Overall, the tech stack is solid, and I’m excited to see how it evolves.

**Ree:** Thank you so much for sharing your insights! We appreciate you both being here today. 

Before we go, I want to remind everyone about the upcoming Hotel Community Day on June 25th, where we are accepting talk proposals. Also, if you’re attending KubeCon EU in London soon, Adriana and I will be speaking there. 

If you missed this recording but have friends who couldn’t make it, you can replay it on our LinkedIn or find it on our YouTube channel. 

Thank you all so much, and we will see you next time! Goodbye!

## Raw YouTube Transcript

Hello. Welcome everyone to a new episode of Hotel Me, formerly known as the end user Q&A. I'm very excited to be here. Um, if you have been here before, I am Ree and I have Adriana here with me. I am coming to you live from Vancouver, Washington, not BC. Um, Adriana, where are you coming from? I am coming to you live from Toronto, Canada. So, we're at opposite ends of the coast, which is pretty We are. Yeah. And so, for those of you who are familiar with the flow, um, hang tight. We are going to bring our guests on pretty soon. Um, for those of you who are new, welcome. So, how this is going to go is we are going to bring on our two special guests from a an end user organization on shortly. We're going to have a quick introduction. Um, they'll introduce us to their tech stack and how long they've been using open telemetry. This one's gonna be a really interesting one because they were actually one of the first guests we talked to when we started this segment um back in 2022. So, I'm pretty excited to follow up with them and see, you know, how far they've come in their journey, what, you know, what kind of struggles they've had along the way and um what they've figured out. Um and then we'll get into some time for them to give us some feedback about the project. And we should also have some time at the end for any audience members to ask questions. Um, if you do have questions that come up as we go, feel free to pop them in the chat, whether you're on LinkedIn or YouTube, and we will get to them as appropriate um during the conversation. If not, we will wait until the end. And with that, I believe we can bring our guests on. Oh, and Adrian, do you want to talk about our resources just a little bit? Oh, yeah. So, uh, our resources, um, I believe this will I'm going to scan the QR code myself because I don't remember what this does. Um, this will take us to this takes us to open telemetry.io. Um, and we have a page in the hotel uh, site all about our um, all about our end user SIG. If you want to learn more about the end user SIG, we welcome you to join. I believe there's a a link on there also for joining our uh Slack channel on CNCF Slack. So, if you're already on CNCF Slack, um please come find us. We welcome everyone. Um people come along to ask questions. We might not have all the answers, but we can definitely direct you to where you need to be. Oh, and also we would love to know where you are listening from. So feel free to put that in the chat as well so we can we just like to know where people are listening from. And so the guests we have for you today are Jerome Johnson and columnist from relativity. Hello. Hi everyone. Howdy howdy. Hello. Would you we would love to um learn about you and can you tell us a little bit about your role in the company and kind of what relativity is focused on? You can go ahead, Cal. Alphabetical order and whatnot. Oh, okay. All right. Excellent. Very democratic. Yeah. So, a little bit about relativity. So, Relativity is a company that has a predominantly a SAS product that uh provides services to the legal community. So, we automate essentially all the uh processes around legal eiscocovery. So, exchange of documents and all of that. Um it is a SAS platform. We have a lot of companies around the world that actually use it and uh we as observability are really tuned toward how how the cloud our our SAS project is actually working at any particular point in time. Um as far as me I um I started with relativity about four almost five years ago now. I was brought in basically to upgrade uh the way we do telemetry and as Ree mentioned we were one of the probably the first first ones to have a chat with with you folks but also one of the first adopters of open telemetry in production. So we were very very early adopters of of open telemetry. Um I'm an architect here. Um and then I'll let Jerome introduce himself. Hello. As Cal said, we're on the observability team. My name is Jerome. Um, I've been at the company for about 10 years or so now. Uh, but, uh, I've only been on the observability team, specifically the one that's dealing with open telemetry for a little over a year now. So hopefully I can bring some perspective on like how easy it is to use from an on the internal side and how nice it is to so looking forward to chatting it up with you guys. Cool. That's so exciting. Well, um I think this is a great opportunity for us to uh start digging in and can um can either of you or both of you give us some insights into your tech stack and architecture? Yeah, so I could talk about that a bit and uh you know, so like I said, we were an early adopter of open telemetry. Uh what we had before that was an entirely bespoke I would say uh system for collecting telemetry from a lot of different sources. Um, in addition to that, um, we we did use New Relic at the time, but we also had a lot of other vendors in there, too. So, it was kind of a nightmare from an operational standpoint to actually do any uh troubleshooting and debugging because you had to go to lots of different sources to to sort of put all that telemetry together in your head. Um, at the time when I was hired, we we uh the company made sort of a decision to try to unify everything. We looked around to see what would be the best platform for doing that. Um and the one that sort of checked all the boxes even though it was really immature was open telemetry. So that was the one which we sort of pointed to to do that. Now internally we have a very complicated platform. Um and maybe this is time to bring up the diagram on on what we actually do now with um um with open telemetry. We have 1500 plus different services running on our platform and that is a huge mix of things. We are predominantly a Windows andnet shop. So most of that is actually Windows and .NET. Um but we have a mix of basically every other language in the world. Uh so we have a very very complicated tech stack and we need to pull in the telemetry from all those different things. So most of the complexity in our observability platform and I'll probably mention at various points. So if I say RELI, that is our observability platform that's built over open telemetry. Um most of the complexity is just being able to pull those signals in from lots of different sources and you see most of them here. We really prefer that people use the open telemetry SDKs now and we push that as hard as we can. But we still have people using our old platform which is relatively APM which we translate into open telemetry now. We have things like SNMP. uh we pull logging from lots and lots of different places and that's probably the part which is most most difficult and most uh heterogeneous in terms of the platform itself. We have a whole fleet of uh open telemetry collectors that we run in 20 20 different regions around the world. Each one of those is load balanced. So we're running we're running anywhere between uh depending on the time of day and all that between 200 and probably 800 individual collectors around the world to to pull all this telemetry in. Um and then we we send that to various uh data stores for operations. This is really new relic at this point. Um so that's where most of our uh engineers go to actually see what the what the telemetry is doing. We have a subset of people who use launch darkly and use that for future releases. So we send some set of telemetry there. Um and then we actually archive all of our telemetry for compliance reasons for a year and for forensic analysis and stuff like that. Uh we also have a reporting platform that uses the Azure blob storage basically as as its back end to to make that reporting for managers and things like that. Um so this makes it look simpler than it is. It's a lot more uh gnarly than than this diagram uh uh lets on. But we do have a large scale. I mean 20 plus regions around the world, 1500 services all feeding into this is is a fairly complicated platform. Dang, that was really cool actually. Um questions. So many questions. I do actually have a lot of um followup questions, but I also want to um get into why open telemetry like how like when exactly I know you mentioned, you know, kind of back in 2022 you you know your team was early adopters. Um how did you come upon open telemetry and like why did you decide like oh this makes sense for us? Yeah. So you know I think the thing which which sort of ticked all the boxes for us we we came from a place where we were having to maintain our own observability libraries um and given the fact that we werenet and then trying to diversify into lots of different languages uh meant that we were having to translate that thing again and again and again. Um and on top of that we also had uh well we were using New Relic at the time and we had a lot of the New Relic agents in the mix as well. So we had this this weird mix of our own libraries for some stuff. We had uh a mix of of things from open from New Relic uh with the uh with the agents and all of that kind of stuff. Um and we had a lot of people who were writing to New Relic directly from from their code as well uh as well as to other sources. So one of one of the things which was really complicated on our side was every time we wanted to move something or we wanted to change something it meant code changes. Um and that's something we wanted to avoid going forward. So we wanted to standardize on something which was ideally open source that we could include in our uh in our services one time and maintain that and then change what where we put things but didn't didn't have to change the code every time. So we're really looking for um you know something that would give us stability in terms of the codebase but allow us the flexibility to send things to other places as we needed to do that and open telemetry was the only one which sort of ticked those boxes for us at the time and the only worry we really had at the point was that it was it was a new project. Um there was a lot of risk in going that direction. uh we decided in the end it it wasn't an easy decision. We we actually discussed this a long time internally but we decided that it was worth the risk to to go ahead and adopt that because it looked like it was going to take tick the boxes for what we wanted to do there. Um you know so basically it's it's to isolate basically our code base from from the from what we do with the telemetry afterwards which was sort of the selling point for what we wanted to do there. That is very awesome. Um I don't know I don't know if Jerome has anything. Uh oh yeah I yeah just to add on to that um I can give somewhat of a perspective to what it was like before we had open telemetry like um I was serving on the performance team for a good number of years and um it was very challenging to do uh analysis of specific applications just because like Cal had mentioned things were it was kind of the wild west on how we handled metrics like performance metrics and monitoring are are somewhat look similar, a little different, but enough to where like every time we had to analyze a specific application, we would have to do a deep dive on a set application like what telemetry are you guys putting out? Oh, you're not putting out any at all. And then we'd have to like dig into the code and that would really muck up like how much time we could actually like drill down into what the core problem of said application performance uh perspective was. Usually it would take half the time we would do an analysis it would be like trying to understand what the product was and then actually coming up with a solution would be the the la latter 50 or 40% they have. So um since joining uh observability like and even like even though Cal showed a simplified version of um our reli stack and uh it's a like you said it's a bit more complicated once you drill down into it. I was amazed just how like how much cleaner it is to just understand like what our what relativity is doing as a whole. So, uh take that with what you want. That's very cool. Um now as a as a follow-up question, um one of the things that we are always curious about in the SIG is collector setup. Um what kind of you you mentioned I I believe um Cal mentioned that you guys manage a fleet of collectors. Um how do you manage that fleet of collectors? How many can you give us an idea of how many collectors? Is there a particular setup that you use? Like we we would love to know. Yeah. So, you know, I think um in terms of how things are usually deployed, I think you call it a gateway setup. So, all of our collectors are basically in a gateway setup. Um so, people send their telemetry to us, we process it and send it on. Um over the years, we've had more or less complicated pipelines, but in general, we have essentially a single set of collectors with a common configuration that handles all of that uh all of that uh telemetry. We um so in terms of the numbers of things so we are running over 20 plus regions each uh each region has somewhere between depending on the load between 10 and probably 50 to 100 uh instances themselves. So we're running at any point in time somewhere between 200 and 500 individual collectors. These are all running in Kubernetes. Um we are uh very focused on compute and Kubernetes. So that's where almost all these things are running. In terms of the deployment itself, we actually handle all of our deployments um uh via Helm charts. So we use Helm charts basically to deploy these all out to the um to the um uh Kubernetes clusters. Uh we do take pains to make sure that essentially all of our configurations are isolated. So our collectors except in a very few cases do not reach out to external services. We manage the configurations all locally to make sure that they are as available and uh as as available and robust as possible, right? Um so that's kind of how we do all of that. In terms of the pipelines themselves, um one of the other reasons why we went with open telemetry is we have some internal uh I would say attribute enrichments that we do specifically to our product and the way we use things. We actually implement those inside of the uh collector itself. Um so we have some custom processors which en enhance the data in various ways. We actually validate it. We actually drop data which is not uh which contains things which shouldn't be there for instance. Um so we do a lot of validation on the fly as things go through the the collectors themselves before we write it out to our various data stores. Um I think I I think that hit most of the points you're asking for. Did I miss any of the uh the things you're asking? I think I think you got most of them. And and yeah, actually one one follow-up question I had was uh also since you you mentioned you deploy your your collectors in Kubernetes via Helmcharts, have you used um the hotel operator to manage any of your collectors? Is that something you played with? We have not. The the hotel operator sort of came out after we were already doing stuff. So in fact, we uh we don't uh but not for any particularly good reason. No. Okay. Okay. Fair enough. So, it's it's that we had uh you consider it sort of tech debt. You know, we had the Helm chart and all of that beforehand. Um so, we've just continued doing that and we're kind of used to managing things that way. Um we actually will have an internal shift in in our deployment uh tooling uh coming up for the organization. So, that might be an that might be an opportunity to switch over how we're actually managing things. But at the moment, yeah, we're not we don't have any experience with the uh open telemetry operator at the moment. Got it. Got it. And I I uh another similar question. Um since you do meet manage a fleet of collectors, um have you played around with the opamp protocol for for doing that? We've not again again we've tried very hard to make our our fleet essentially uh standalone. So even for configurations and things like that, we don't actually reach out to anything else. We really deployed uh sort of static config maps for that. Um that that was basically a choice that we made um just to because we have such a large deployment and because it's distributed so widely, we wanted to make sure we were as um resilient against networking problems as possible. So even for things like configuration changes, we we push out new new configs for those things rather than than doing something that Got it. Got it. Do you have a It's not to not to say that's a bad idea. It's just it's a choice. It's a choice that we made. Of course. Of course. That makes sense. That makes sense. What about um maintaining the collector pipeline? Who is there a dedicated team? Is that just whoever is on the observability team or how do you maintain the local configurations and the pipelines? Yeah, so we do have an observability team. Um, so we are uh split between Poland and here um our other big engineering office is actually in Krakow, Poland. Um, so we have two uh two observability teams split between the two regions. Uh, each one has I think around five people at the moment. Um but collectively we are responsible for maintaining those pipelines and that configuration. Um almost I would say almost entirely we are responsible for that. There are a few areas where we share responsibility with certain places where we pull in configurations. One of them is our Kubernetes team. um they actually define part of our configuration for which metrics we actually pull into the global system rather than just keeping it local to their Prometheus instances. But by and large we are responsible for that entire configuration. Um and we try to avoid shared responsibility there just because we want to make sure that uh you know shared responsibility is always means no one's responsible. So we try to really avoid that. So, as you know, the project has matured um and it sounds like you've got a pretty good setup that work is working well for you guys right now. Um how has the implementation evolved over the years, you know, as um things have become GA? Have you added additional instrumentation signals? Um how has that looked as you've kind of grown alongside the project? Yeah. So, um yeah, it's actually an interesting question um in the sense that the thing that surprised me overall is that the deployment that we have hasn't really changed in terms of its architecture since the beginning. It's been the same architecture the entire time. Now, we've made tweaks here and there and we've simplified things, you know, as new features have come out and that kind of stuff, but by and large, it's stayed the same. and and that I think is amazing given the fact that it was really sort of pre-alpha when we started. Um so we've gone through you know we sort of gone through the uh the alpha beta you know and things are starting to become stable now and we've not really noticed uh any really heavy changes that we've needed needed to make in the platform. Now that being said I mean we have added new signals. So at the beginning of course it was metrics and traces that were were there at the beginning. Logs came in afterwards. Um we had logs coming into our system even earlier than that. So we we had some of our own receivers to do some of that before it was an official signal. So we've we've sort of folded those changes into the platform as they as they come along and it hasn't really been terribly painful in doing that. Um, so you know, as far as the community goes, the, you know, the the alpha beta stable kind of thing, you know, we've not seen really any negative consequences from that. We've had to update on occasion. Um, but by and large, those updates are pretty easy. and and Jerome I mean his he's been doing a lot of the following for the collect we have our own uh build of the collector uh which of course we try to keep in sync with the external one and Jerome has been doing quite a lot of that so you can probably give some feedback on how difficult that process is and and where we've run into issues. Oh yeah, I'm sure. I mean, for the most part, um, uh, it's, and I don't know if this speaks to open telemetry or the automation we put around it, but as far as just updating the collectors, it's been a relatively painless process. I mean, sure, um, there have been a few changes that we've had to uh, kind of team uh, just file down on as a team and like, hey, what path do we want to go down? But um as far as just like pushing out those changes, they've been pretty simple, especially with how elaborate and um detailed the white papers or the change logs have been out of open telemetry just explicitly telling hey these libraries or modules you've been using uh they here's an uh we're introducing a breaking change or an API is uh going to be deprecated. Uh but overall it's been uh quite e uh use of ease experience and um uh getting more people in our team to actually utilize that process has been uh fairly easy as well. Um, I I have a followup actually on on um on on building your your collectors because I think uh of I think out of all the people we've talked to, I think you guys might be the ones that have done uh have built your own collector, which makes me very excited. Um did you um in in terms of building like your your own dro um did you have like what was your experience around that like with as far as like the documentation provided on the hotel site because that that's another thing that we we want to hear from from um practitioners of OTEL is how how usable is is the documentation and and have you noticed an improvement in the documentation? So even um building like building your your own collector, how useful was that documentation? Did you have to do a little extra like Sherlock Holmesing to figure stuff out? And and also um were you able to uh build like a nice streamlined process for building the collector that uh uh like did you I guess have to go outside of the confines of of what was already documented to do that? Yeah. So um I'm not sure I'm going to be able to provide necessarily documenta you know feedback on the documentation but the process as a whole I mean we one of the things we started with was we had to start with our own uh our own build of the open open telemetry collector mainly because we do have custom custom receivers we have custom processors we have custom exporters as well and we kind of needed that at the beginning to cover some of our some of our use case so we were forced basically at the beginning to to build our own collector, right? Um, at the beginning it was very painful. It was basically us cloning the entire repository and then making the changes we needed to build it. Um, once once the collector builder came along um that made that whole process so much easier and it was it was a great addition to that uh that process. Um right now basically we have moved to a situation where we have our own repository. Um and basically we have the single config file for the uh for the collector builder that just tells us what we pull in from from contrib and from the uh the main open telemetry collector and the only thing we have in our repo at this point is our own custom stuff. Um so once the once the builder was there everything since then has been really really easy and smooth to to build our own stuff. Um we did that right at the beginning. So before there was documentation so um and it's worked since then. So I I can't say you know whether the current documentation is good there or not but we use the builder and it it works seamlessly for us. And uh just as a followup because you mentioned you you built your own components. Um, how how was that experience of of building your own components? Was that um was that tricky? Like what was the um uh because I that that's definitely something that um I I would say personally feels a little bit lacking in the hotel documentation for from building like your own receivers, processors, exporters kind of thing. How how is that for you? Yeah. So I think in terms of the team itself um I think the biggest hurdle there was was learning go we we weren't uh we weren't um so I think from the team standpoint that was sort of the biggest uh initial obstacle for that um all of us I think have some good experience with go uh now so I don't think that's an obstacle anymore um in terms of how to do it um we again were doing this very early. So it came came at a time when it was basically well let's take one which we know works copy it over and then make the changes make the changes you know that we need to do to make our own stuff right. Um since then since then the documentation has gotten much better and it's much easier to sort of start with with those things and get them done. Um, but you know the I must say the the process of just copying something that works and moving it over and making the changes you need that also worked pretty easily as well. So you know I don't think there's a huge for an experienced programmer I don't think it's a huge barrier to do those things. Um now there are some things which I would love to have better documentation on like in terms of internal telemetry how to you know how to hook in better to the internal telemetry and and that kind of stuff. Um we sort of worked it out uh you know by uh reverse engineering what was there but some some of the things like that would be much more helpful if uh they were better documented for instance. Thanks. Oh, I was just going to say um yes, I remember um that when we were catching up um a bit yesterday and we definitely want to get into more of like the feedback that you have for the project. Um there was a an audience question that I think is interesting from Sumit. uh how much data size do you consume and have you found the need to reduce it for example via tail sampling or other methods and from application logs and security logs perspective. Yeah. So uh in terms of the data that we push through the system we are somewhere around uh we write typically somewhere between 10 and 15 terabytes a day. So that's sort of the the uh the volume of the data that we we consume. Um the in terms of the split between logs and and and traces and metrics, it's not quite an even slip between them, but you know, it's we we we uh take all of those things in, right? Um so it's it's not so different between the between the three of them. Um in terms of the platform itself, for the stuff that flows through the collectors, we've never had an issue with scale. uh we've been able to take whatever people have thrown at us uh because we have autoscaling on all of our collectors and everything else. Um we've not run into any issues at that scale and there are times when that scale goes up by a factor or two because someone's doing something stupid. So you know uh so in terms of of tech and all of that things have just scaled correctly. The place where we've had to reduce data is mainly because we do push this data into New Relic and we pay for what we push into New Relic and we do have budget constraints. So we do manage essentially the volume of data we push into New Relic mainly for budget reasons rather than technical reasons. Um and in those cases yes we do sample down the traces. That's what gets sampled down most heavily in in the things that we we pull in. We obviously manage our logs by log levels. So we really do have dynamic ways of changing the log levels we use for that. Um typically coming from the Kubernetes platform as well. Um and metrics we tend to leave untouched. Um but there are cases where people abuse that and and we and we go after people. It tends to be rather than a proactive thing. It tends to be reactive. uh we see that uh we do look at who's who's pushing out data um and then go after them if we think that they're doing something unreasonable. Um but I will say in terms of tech, we've not run into any issues with scaling which I think is really amazing uh for for this for open telemetry in general. Um and most of the most of the things have been really uh based on budget rather than rather than the actual technical stack. Um, I wanted to uh ask a follow-up question to one of the comments that you made because actually overall um because the the vibe I get from from speaking with both of you guys is that your organization is very much into like very much supportive of of like the observability journey um and and open telemetry adoption. Can you talk a little bit more about like this culture of observability? Uh yeah, so you know the I was brought in basically when I when I started to to change the way we do observability at the at the organization you know and and a major part of that has been open telemetry and it's been really nice to see that the technical aspects of that have been have been actually the easiest parts uh you know the tech has just worked. uh you know not to say that there aren't hiccups and all of that but um you know the the tech stack is actually working really well and the data flows that we have in place I think have been uh have been really good. The harder part of this has been the cultural changes which you mentioned a bit. um and changing the way the organization deals with observability. Um so there's the you know resistant to migration. So you know there there are migrations there and people are resistant to changing code and we have a large code base. So that's one of the reasons why we still have some of our old libraries still producing some telemetry. Um so there there are res there has been resistance there. Um, and the the nice thing I've seen over the last few years is that as people get more and more experienced with the open telemetry and see what it can do for them and how it, you know, sort of simplifies the way they generate the telemetry, there's been less and less resistance going forward, which is which is a nice change to see, right? Um, so people are starting to see the benefits of this and they're starting to really, you know, uh, lean into open telemetry as a platform for that. Um the other thing I've seen on the other side of things is because we can write to various different places very easily. Um being able to do that has uh has sort of enabled people to to do nice new interesting things. And you know one of the things which is you know on the diagram I showed at the beginning was one of the places we write telemetry is launch darkly. Um, and they have a really cool feature where you can say, um, uh, it's called release guardian, and it's a way of actually, uh, changing a flag and having launch darkly ch make those changes automatically and progressively roll it out based on the telemetry it's seeing, right? So, we send spans there and if the spans look like we're getting a lot more errors or that the latency is getting larger or things like that, it will automatically roll back. And the fact that we could actually roll that out by just a configuration change on the open on the observability side is really cool because we've you know incorporated an entire another platform with our telemetry without having to change anything fundamental in our in our code and that uh for me is really really powerful and I think as people see things like that I think they're they're more convinced that this is a good choice. That's great. Um oh sorry go ahead. I don't know just going to give another before story as well. Um like like I mentioned uh I was on the performance team initially and we did have uh like Cal said like uh people are very uh hesitant for change. Uh like to give you some perspective like we did uh we had an initiative uh in the performance sphere to try to get more people to start using more performance telemetry and that dragged out for like almost a year and a half with teams like moaning and complaining all the way through uh just because of how non-industry standard it was and even at the end of it people weren't 100% sure like how much benefit it was bringing but Um right but to bring that to back to today we have another initiative uh along with open telemetry and we've gotten feedback basically just to get more telemetry uh maturity and coverage throughout all of our systems and services and we've gotten feedback from teams like almost immediately like how much they're benefiting from it as they mature their own uh application telemetry. uh and uh we we've seen like just like uh like response times to like issues that have come up uh that uh have brought about this because of open telemetry. So it's at least from someone who's been on both halves of this, it's like night and day. That's really exciting and you actually answered the question I was about to ask you. So that's great. Um Reese, do you have anything else that you wanted to touch upon? Um, I'll say quite a bit, but um, in the interest of time, um, you know, I did want to give some space for you to share additional feedback that we could share with the project maintainers. Um, I know you talked a little bit about, um, the issue with, um, conventions for internal data schemas. Um, so if you want to go into a little bit more about that and anything else that you would like to see improvements in or feature requests, things like that. Yeah. So, um, yeah. So, first let me preface this by, you know, I I think probably from what I've said already, you know, I am 100% in with open telemetry and I think it's a great platform and what the maintainers are doing is fantastic both on the documentation level and the code level, right? So, Um, that's all great. Um, and and this isn't feedback necessarily uh in terms of the actual uh collector implementations or open telemetry project as a whole, but I think it's sort of feedback on I think where we're going in our own journey and probably that has some bearing on other people as well as as we're generating more and more telemetry and it's becoming more and more useful for people. We have a lot of downstream consumers now. So we have an entire AI division who's now looking at our telemetry. We have people managers who are trying to do reporting off of this and getting standard data is becoming more and more important. Um one of the one of the things we built initially and that one of the reasons why we had to have a custom processor is we do have standards for the attributes which come into the system. So we enforce a sort of a global schema on everyone even though that's extensible. Um now how that fits in with the semantic conventions, how that fits in with you know the elastic common uh uh elastic common schema um in general how this fits in with sort of data contracts and how you have contracts between the producers of the data and the consumers of it. Um all those things are things we're thinking very hard about right now and we don't have good ideas on how to you know apply that across the board um and good ways and the level at which we need to do that. So you know I think discussions about uh the semantic conventions and the ECS and you know how to do things like data contracts with with open telemetry data I think is going to become more and more important. uh I see that in our own organization and it'd be good to come up with sort of common common themes there and common tooling and common ideas. Um so a little more convergence I think within the conver within the community there I think would go a long way. Um in terms of other feedback the tech stack I think uh in general is really really solid and you know the way things are managed I think is great. So I don't have a whole lot of feedback there. Um yeah so I think it's mainly looking forward and seeing how we can sort of use the technological base that we have and and come up with better ways of standardizing the data which is produced uh making it more useful for downstream consumers I think is the the main thing I would like to see discussed more right on thank you and I think we have a bit of time to catch up on some audience questions um so let's see one of them is from Doug Doug on YouTube and Doug wants to know with regards to collectors, their dream is that cloud providers will offer as a boring text similar to an SMTP or SFTP. Is Doug crazy to want that? Um we um the first question we ask any vendor now is do they support OTLP? Um, so you know, we treat that as our standard protocol and uh it's a negative uh a negative check mark against someone if they're not supporting it already. So we we're pushing all of our vendors and and various tech products and whatever to support that as much as possible. Um so I don't think that that's um I don't think that's a pipe dream. I think uh pushing people in that direction is is a good thing. Um as I said we are uh very heavily a Microsoft shop so we use Azure uh very much and the feedback we've consistently given them is uh we want you know Azure monitor and those types of things to produce open telemetry signals because we don't want to have to do that translation ourselves. Thank you. And Manas I see you have a question about OPM tutorials. we will respond directly with some resources for you um in the LinkedIn chat. Um in the meantime, there was another question. So, did you ever feel that there were classes of telemetry data that hotel didn't collect well and you needed to add another collection method such as eBPF? we've had to uh we've had to come up with some um some custom receivers and things like that that uh that weren't covered beforehand. One of the early things we did was there was no way to sort of uh provide a web hook into open telemetry at the beginning. There is now. Um so that's one of our one of our legacy receivers that we've had. Um and that was something that was not difficult to write but something that was sort of lacking. Um in terms of other classes of telemetry uh I would say that the one of the biggest gaps we've had internally as an organization is front-end telemetry. Um and that's something that we're actually closing now. We actually have a pro project in uh in currently going on basically to collect more of our front end telemetry uh with the open telemetry SDK the JavaScript one. Um so that was something that we internally was difficult to do for for many reasons and we're now sort of closing that gap. In terms of other types of cle telemetry that's been difficult. I don't think we've had any real issues. most of the things that we've had to collect if it's not OTLP or things like Splunk Hack or Fluent Bit and things like that where we've been able to get things into into our collectors fairly easily. Perfect. Thank you. And I think our time is just about up. Um were there any last well not last parting words? No just uh thanks thanks for inviting us I think is the is the the last thing from our side. Um and you know if anyone wants to reach out or whatever I am in the uh in the slack so feel free to reach out to me directly if you have questions or whatever happy to discuss what uh what we've done in detail if it's if it's helpful. Thank you so much for offering that and thank you so much Kyle and Jerome for being on and sharing your journey with us. Um we will link to our um hotel sig and user channel as well. Um so you can find all of us in there. Um you can also reach out to Cal directly via the CNCF Slack as well. Um, and so we do have a link manus for you um that we'll put up here in a second um that links to OPE documentation and oh well we might have time for one more quick question. This is kind of what do you think about auto instrumentation? Uh me personally I think it's a great thing uh you know and we use it where we can uh and in fact for the front end stuff we are planning to use some of that auto instrumentation there. Um the downside we've seen uh we have tried to use it and we do use it on the net side in the net SDK for uh HTTP requests and things like that. The the problem we've run into there is volume. So it it it tends to be difficult to sort of scale down to exactly which ones you want. So the auto instrumentation tends to be overly broad sometimes, but that's the only sort of downside we've seen with uh with it. Uh so in general, we you know, I think it's a great thing and if it works for you, then yeah, definitely use it. Awesome. Thank you. Well, uh, Adriana and I would like to thank you again so much for being on here. This has been really great and I'm sure I'll have um myself and other people will have more questions to learn more about open s implementation since you guys were early adopters. Um, we do have a couple things we want to share real quick before we head out. Um, hotel community day is coming up on June 25th, I believe, and we are currently accepting uh talk proposals. So, if you would like to submit a talk, we definitely encourage you to submit a topic for open telemetry day. It's going to be in Colorado as part of open source in Denver. It it's a collocca I think it's colllocated event of opensource summit North America in Denver, Colorado. Yes. So if you want to come out get um well I guess that's too late to ski but the beautiful mountains I guess. Yes. Um, also if you are going to or if you are already in Europe and you're planning to um head out to CubeCon EU in London in two weeks, less than two weeks. Yeah. Um, we would love to see you there. Adrienne and I will be speaking um at the event and we have a blog post. Yes. and we have a blog post um about the event that outlines all the open specific talks. Um the recordings if you're not able to make it will be available on the CNCF YouTube channel I think about one to two weeks after I think um depending. So yeah, they're they're usually pretty fast. Sometimes they get it within a couple of days. So just keep an eye out. Yeah. So if you can make it um in person, you are definitely welcome to check out the recordings on YouTube. And I believe that's it. Um, do you want to mention also if you if you are at CubeCon uh EU, um, we are going to be hanging out at the hotel observatory off and on. So if you're there, come say hi. Um, the observatory is always lots of fun. Um, because there's usually tons of hotel contributors, maintainers, etc. hanging out. So it's great to have like ad hoc conversations with folks. There's also going to be um if you're a CubeCon hotel project updates. Um so I strongly encourage folks to join that as well if they want to see like what's new and exciting in hotel. That's always a a fun session to attend. I think it's it's usually like half an hour um that it's slated for. So, and and also if you um if you made this recording but you have a friend who couldn't make it um you can replay on our LinkedIn um just share this the same link for the link the live um recording um with your friends or um this will also be available on our hotel YouTube channel um so tell your friends- official tell your friends. Tell your friends. All right. Thank you all so much and we will see you next time. Bye.

