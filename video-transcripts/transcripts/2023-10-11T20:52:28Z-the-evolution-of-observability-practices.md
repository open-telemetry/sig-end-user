# The Evolution of Observability Practices

Published on 2023-10-11T20:52:28Z

## Description

OpenTelemetry has greatly impacted the observability landscape over the past few years. While it introduces an open standard for ...

URL: https://www.youtube.com/watch?v=zSeKL2-_sVg

## Summary

The video features a panel discussion on the evolution of observability practices, hosted by the OpenTelemetry and User Working Group. Panelists include David Win, Iris Dear Mishi, VJ Samuel, Austin Parker, and Noika Melera. They share their experiences and insights on the transition to OpenTelemetry, discussing the challenges and benefits of adopting this standard in their respective organizations. Key points include the importance of developer engagement with observability tools, the need for standardization across platforms, and the shift in focus from merely collecting data to making it actionable. The panel also highlights the ongoing improvements in OpenTelemetry, such as the Collector Builder tool, and anticipates future advancements that will further integrate observability into development workflows. Throughout the discussion, panelists emphasize that the real challenges are often people-related rather than purely technical.

## Chapters

00:00:00 Welcome and introductions
00:05:10 State of observability before OpenTelemetry
00:10:50 Challenges in observability practices
00:14:40 Evolution of observability tools
00:19:30 OpenTelemetry standardization benefits
00:24:00 Observability culture and leadership buy-in
00:29:00 Collector Builder introduction
00:33:56 Surprising challenges in implementation
00:40:06 Observability practice focus changes
00:45:00 Future of OpenTelemetry and industry impact

**Moderator:** All right, I think we can go ahead and get started. Thank you all so much for being here. The panelists we have today are David W., Austin Parker, VJ Samuel, Iris Dear, Mishi, and Noika Melera. I'm hoping I got all those correct. We'll do a quick run of introductions. This discussion is hosted by the OpenTelemetry and User Working Group, which I am part of, as is Adriana. I see her there. Today we're going to just have a casual conversation. Feel free to get as opinionated as possible about basically the evolution of observability practices. 

**Moderator:** David, since you kind of inspired this, I would love for you to do a quick introduction, and after that, we'll just kind of go through the rest of the panelists. Yeah, let's hear a little bit more from David, whose brainchild this was.

**David:** Hello everyone, my name is David W. I am Principal at Edge Delta, which is in the observability pipeline space. The thing about being a startup is you just sort of do whatever needs to be done, so I flex the title appropriately as such. Ree and I were chatting about different ideas that might be fun to discuss, and one of them that seemed very apropos to the group would be sort of the evolution of observability and where things are going, how people are tackling the challenge of shifting to OpenTelemetry, and what are some of the interesting lessons learned along the way. Not only from the 10,000-foot view of like we see where the mountains will go but also at the 10-foot view of boy, this grass is tall sometimes, trying to get a little bit of feedback on all different directions of it. 

**David:** To continue with introductions, I'll go ahead and do it popcorn style. Iris, why don't you introduce yourself next?

**Iris:** Hello everyone, my name is Iris, Iris Dear, depending on the country where I am. Currently, I'm based in Portugal. I'm a Platform Engineer, Observability Engineer at Farfetch. My day-to-day is building an observability platform, maintaining it, modernizing it, and offering this kind of service to the engineers in my company. I don't know, that's all about it. Go ahead and call someone else out.

**VJ:** Hi everyone, I'm VJ Samuel. I work at eBay. My day job predominantly revolves around doing architecture for the observability platform internally. Everything—logs, metrics, events, tracing—helping all our developers do alerting, visualization, anomaly detection, the whole shebang with regards to observability. That's pretty much what I do.

**Austin:** Hi everybody, I'm Austin Parker, Community Maintainer for OpenTelemetry. Formerly at LightStep, part of ServiceNow, and currently—it's a surprise, and you'll find out very soon what I'm currently doing. I've been a part of OpenTelemetry since it was created. I was an OpenTracing maintainer. I've been working in observability for over five years now and got a lot of thoughts from seeing it kind of grow and evolve from what it was to what it is.

**Noika:** Hi everybody, I'm Noika. I'm at the open-source startup Signos and have been working with observability stuff from back when we called it Real's Performance Management. Mainly now working with OpenTelemetry and Kubernetes stuff.

**Moderator:** Excellent, thank you all so much again. We have a list of questions that are kind of intended to help guide the conversation, but once everyone gets going, I expect it to become a lot more dynamic. I think we're all totally happy to see where this takes us. 

[00:05:10] **Moderator:** To get us going, we want to know about the state of the world before you all undertook your OpenTelemetry journey. What was working pretty well? What did not work, or what sucked? What was the moment that prompted you to change? Feel free to raise your hand. All the panelists at least are on camera, so if you need visual cues as to when you can step in, hopefully that helps, but feel free to raise your hand too. 

**David:** I'll actually start because I think I have what is probably not a very unique story but an interesting one. Before I got into observability as a field, I started out in software doing QA. I was a software developer in test, and this was, you know, 2013, 2014, I guess is when I started really getting into technology as a career, or software as a career, I should say. The cloud was a thing, but Cloud Native wasn't quite a word yet. We didn't have this concept of like, oh, we're just building all these things with all these cool APIs and this idea of infrastructure on demand or whatever. 

**David:** I saw the company I was at go through these various transformations, and one of them was a DevOps transformation where we went from, okay, when you build your code and you deploy your CI, you write, you pull a ticket, you write some code, it works on your machine, great, you push it, and then that night someone else gets to deploy it and see if it actually worked. One of the things we wanted to do was really tighten up the feedback loop here. We wanted to get from 24, 48 hours before changes got into test to minutes or hours. 

**David:** A big part of that was getting on-demand infrastructure rather than sort of static infrastructure. We're going through this, and we're building all this out, getting stuff into the cloud, and it's great. What we started to see, though, was it wasn't actually fixing a lot of the problems we had. There was kind of this ground truth that everyone had agreed on beforehand: the problem is that we have bad infrastructure. These servers are not properly cared for. We're just wiping stuff and recreating it rather than actually getting fresh images every time. 

**David:** So it must be some config thing; it's probably not the code. Then something would go into production, and we would make it into a patch, and then the customer would come back and say, "Hey, this is actually broken," and we missed it because we thought this was because of our testing infrastructure. When we started going into the cloud, we had fresh images, all this stuff, and we're finding all these problems that we really didn't even know about before. 

**David:** The question came back, "How do we know what's going on? How do we know what's breaking our product?" It was a platform as a service, so we had hundreds and hundreds of nodes, various logs in all sorts of different places. It was Windows servers, it was Linux servers, we had all these different databases, and it was very difficult; it was kind of big. It was hard to keep your head around. Someone, one of the engineers, actually came back and said, "Okay, I made a topology service topology," and it looked like if you've seen one of those nail art things where someone will make a picture by putting a bunch of nails in a piece of wood and then tying string together—it was like that, where you have just lines everywhere and things connecting to each other. 

**David:** Nobody could keep this in their head. Nobody could understand how services actually talk to one another. You could look at a very small section and say, "Okay, I get this," but looking at it holistically was impossible. I brought in, at the time, we tried New Relic, we tried Datadog, we tried a couple of things. What I found was, ironically enough, that it didn't matter what tools I brought in; the developers weren't interested in using them just because it wasn't data, it wasn't information that was kind of like at their level. 

**David:** They didn't understand how do these things correlate, what does it mean when SQL Server spikes and memory usage increases, but there's no way to really tell what was happening, and that got me into observability—trying to answer that fundamental question. 

**Austin:** Right off of that, I remember working in New Relic during my time there, and there was this really fundamental thing of like, oh, this shows how this request hit all these spots, and it shows it as a trace with a bunch of time spans including individual function calls on all these different services. Pretty cool, but we get these questions back that were like, "Well, just show me where the request went, show me what services were hit, and also like in an interpretable version." 

[00:10:50] **Austin:** It was an example where there was a lot of focus on getting a certain piece of information back, right? But the developers wanted information that was at a different level. This was exactly it: I just want to know where the request is going, which services are involved, and how the failure on the SQL Server might come back up and affect the front end in these ways. That was very hard to tease out, whatever this was seven years ago, but it's a similar theme, right? It's really about making sense of a pile of stuff again. 

**Austin:** Trying to zero in on that moment before we get into these things, there's the point where you have everything, and then you realize that having everything was the problem, and then it's not. Then you're like, "Wow, this computer doesn't know how to draw maps," and somehow it just looks like a bowl of spaghetti. Then you're like, "Cool, how do I zone this back down again?" 

**David:** It seems like sense-making is at least one of the unifying concepts we see there. VJ, Iris, whoever wants to take it— is that a similar feeling that you guys got when you were hitting this inflection point?

**VJ:** For us, the problem space was a little bit different in the sense that pre-OpenTelemetry, we had a pre-cloud native era and during Cloud native as well. If you take the pre-cloud native era, we had something called the Centralized Application Logging Platform inside of eBay for more than 20 years, and it had the concept of transactional logging, where you have a root transaction, nested transaction events, very similar to what we have in the tracing world today. 

**VJ:** But the problem with the pre-cloud native era is always that developers come into eBay; they have to learn proprietary clients. We had clients only for a few languages, so if they are not using that or writing their own code, you cannot observe things inside the company or you're on your own to figure out—spin up your own ELK stack or anything that you can do to monitor the system. 

**VJ:** When Cloud native came in and we had the large-scale Kubernetes adoption, we had the Prometheus endpoints scraping from log files, a little more flexible, but you do not have standardized SDKs across the board. Even for metrics, it used to be that a few people used the official Prometheus client, some used Micrometer, and for Node.js, you didn't even have an official community-supported SDK. 

**VJ:** I think that's where OpenTelemetry came in with the promise of standardization. It was like, "Okay, now we can just offer all our developers one standard, and it's community-managed. They can hop from any company into eBay, and they should be able to use the observability platform as long as we are OpenTelemetry compliant." 

**Iris:** I would say that my viewpoint, my story, is kind of a bit like VJ. My observability experience first started with some companies that had zero observability in place, and that's how I got to know it. Where I'm currently working, I jumped into a completely different world that had a very nice observability platform, a very nice observability culture, which was a big shock. Our job was actually not to come up with ways to monitor but to just keep improving. 

[00:14:40] **Iris:** Of course, we came to that bottleneck that we were using a lot of open source, we were using APM vendors, and to get the information, you have to go in 10 different places, which is not great. I would say that it is from a developer productivity viewpoint as well why we went the OpenTelemetry route, because everything is centralized, and we can move from vendor to vendor if we need. We are collecting everything, standardizing everything, so I would say we have kind of the same case here. 

**Austin:** I think to both of your points, like that to me is what is really transformative about OpenTelemetry. It offers, for the first time, this truly universal idea of how should I, as a developer, emit this telemetry information. I think your story, VJ, is not unique. Most especially in large distributed systems, companies that have large distributed systems have had some sort of transactional tracing, some sort of structured logs with transaction IDs that they can look at for a decade or more. 

**Austin:** When we think about an OpenTelemetry trace, the actual data model is very similar to what Google produced in Dapper almost 25 years ago now. There’s nothing new under the sun. What's been missing is the idea that this is actually a core part of being a developer—a core tool in the toolbox, a core part of your trade is learning how to emit good telemetry about what your system is doing. OpenTelemetry provides a standards-based way to do this that also can be natively integrated and available through your service framework, your RPC library, or whatever else. 

**Austin:** To me, we're at the beginning of the beginning almost of seeing how that actually impacts the industry. 

**Iris:** I just want to add a little bit to what Austin was saying. It's absolutely true. The other good thing about OpenTelemetry is that you don't have to break your whole system to implement it as well. That is one of the great things because it's compatible with almost all technologies that we currently have, especially the open source ones. You don't have to cause downtime or have your developers blind for hours and days; you can just put it there flawlessly. 

**Austin:** I think there's really two fundamental shifts. One is we started adopting architectures that became much harder to diagnose with the stuff that you taught yourself when you taught yourself to code. Adding in some logging lines and such is not going to work on a large microservice architecture. 

**Austin:** The other change—and this has been a while, but it's still there—is that when you have conversations about funding, when you have conversations with VCs, they're going to ask you about your observability setup. Those two changes combined have really changed where the conversation is with observability, where it went from an internal discussion about maybe developer velocity and rollback times to, “Yeah, we have to have this, and it has to be applicable across this stack.” 

**David:** I think there is a fair bit of standardization that gets promoted on the consumer side of things as well, given that there is a standard in how you instrument tech metrics, for example, gauges, counters, exponential histograms, whatever. Earlier, it used to be that if you picked one time series database, you would get certain capabilities defined in a certain way. If you have to switch, then the likelihood of that capability existing may not be always possible. 

**David:** But now since there is a standardization, eventually there will be a good amount of standardization on what technology the providers—either open source projects or vendors—provide. The net benefits are definitely there across the board, not just on the instrumentation side.

[00:19:30] **Austin:** You bring up a really interesting point, right? Historically, everything about telemetry has really been a pretty binding abstraction to the data store. If you were using StatsD, metrics worked in the way they did because of the way that StatsD, you know, stored and let you query that. Prometheus metrics work the way they do because of how you store and query them. Logging databases tend to, you know, if you're using Elasticsearch or something, the format mattered, and that influenced how the client libraries were designed and so on and so forth. 

**Austin:** With OpenTelemetry, we kind of break that, right? OpenTelemetry tells you, “Hey, this is what a histogram looks like; this is what a structured event of any kind looks like.” I think it's interesting that you also see this rise in popularity of column stores for storing observability data—like metrics, logs, traces, sessions, whatever. You throw a rock, and you'll hit a new column store-based or heck, new ClickHouse-based telemetry thing. 

**Austin:** It's great, and I think OpenTelemetry has actually been a huge factor in this because it used to be if you wanted to go and build an observability tool or build some sort of analysis tool, you would have to come up with all this stuff or no one would use it. You'd have to come up with your API, you'd have to come up with your SDKs, you'd have to build integrations for 40 billion things, and OpenTelemetry means, actually, that's all a commodity now. You just get that for free with OpenTelemetry, and you can build really interesting workflows on top of that data. 

**Austin:** I think that's very friendly for developers, right? Going forward, we haven't even really started to see how do these tools become integral in development workflows, right? When do we get to IDE-level integration and all that? 

**Iris:** I know it's very easy to feel like OpenTelemetry is done or like that it's kind of hitting this plateau, but I think we're really, really still in the early days with what people can actually do with it.

**David:** I have to agree on this being early days for OpenTelemetry. A little bit about my background is I was in observability before I hopped over to CL for a bunch of years, and now I'm back in the observability space. The conversations that I'm having with people today remind me a lot about conversations about Git, like 15 years ago or something like that, when it was sort of a perennial joke that everyone's like, “Oh yeah, Git looks really interesting, we're thinking about moving to Git, but we're still on, you know, insert system here.” 

**David:** It feels almost every conversation I have—and again we're in the pipeline space—it's about routing and moving and transforming all the data to the right spot. But everything that comes out of us is OpenTelemetry. If they just need to bolt on something, they want to do that. But the first thing they talk to us about is something with some sort of data not being in the right place, and the second thing is, “Oh, also we're starting to look at OpenTelemetry. Is that good?” Being the second question, I think is a good sign of climbing the curve. 

**Moderator:** Speaking of that, how did you first hear about the projects? I think it’s kind of clear like some of the things that have drawn you all to it. Something I think, too, that people might be interested in is how did you get leadership buy-in?

**Iris:** I can say something about this because for me, it’s easy. When I started working with observability, I became obsessed with tracing. I’m still fascinated by tracing, so I was like, “Oh, tracing is so amazing,” and I started just searching online about it. Then I came across OpenTelemetry. I started playing with it for a long time and not actually implementing anything. 

[00:24:00] **Iris:** When I joined Farfetch, we were like, “Okay, we need something new, something better,” and we thought about OpenTelemetry. It was very surprising because there was immediate support. I like to say and brag that we have a very good observability culture, but maybe everyone had heard about it. We just made a small presentation, and our leadership was on board. Of course, it has a lot of benefits, and we had to present what we were lacking, the issues that we were currently facing, and how MRI can help us with. It was immediate support, so every time I tell the story, some people don't believe me, but that's exactly how it happened. It was very interesting, very fast support.

**VJ:** I can maybe go next. At least in our case, I first came to know about it as part of the OpenTelemetry will basically be the standard, and we would eventually retire OpenTracing and OpenCensus. I think we were passively trying out OpenTracing around that time, and this announcement came, and it was like, “Hey, interesting, this is something that we need to watch out for.” 

**VJ:** We were doing passive experiments with the OpenTelemetry SDK and the Collector for a few months, and once it hit stability, that’s when we made the informed call that, “Okay, we’ll start tracing with OpenTelemetry as an offering inside the company.” So now we had the OpenTelemetry SDK for traces and the OpenTelemetry Collector for accepting all the spans, but on the other hand, we had Metricbeat and Filebeat for collecting logs, events, and metrics. 

**VJ:** At that point, we were at a crossroads of if we should support multiple agents or just standardize across OpenTelemetry Collector for everything. I wrote a very big memo internally on what it would take to migrate out of Beats for metrics, logs, and events, presented it to leadership, and I think we felt that being on a vendor-neutral industry standard would be the better thing for us in the long run. It’s best to do it now than later on, so that’s how we did the migration. 

**VJ:** The metric migration was a massive one because we, at that time, if I'm not wrong, we were already at 32 million samples per second across more than 100 Kubernetes clusters—million-plus Prometheus endpoints and whatnot. It was a very, very involved migration that we did, but the benefits are definitely there now.

**Austin:** Were there any other challenges when you're coming from a culture that has a strong kind of build vibe? What I'm trying to get is, do you guys run your own Collector or are you taking something? How do you manage that in terms of the particular flavor that you decided to land on?

**VJ:** We do run our own Collector internally, but I think the number of custom processors that we run is very less. It's mostly that we don't need all the exporters for all the various vendors that are packaged in, so we create only the ones that we absolutely need. For now, we export using either OpenTelemetry Protocol or Prometheus Remote Write, which are pretty much open standards right now, so it's about being lean.

**Iris:** Are you all using the Collector Builder to make your images?

**VJ:** Right now, no. We just craft our own Go mod and then just do it ourselves.

**Austin:** Y’all thinking about using the Collector Builder?

**VJ:** I think we can. We haven't really explored it, but yeah, we should.

[00:45:00] **Austin:** I'm just trying to get more people aware of the Collector Builder, because I think it's really cool. The Collector Builder, or OCB, is a tool that we provide from the Collector repo, and what you can do is you can give it a manifest of Go modules, and it will create a custom image or a custom build of the OpenTelemetry Collector for you. So you, like eBay, can get something that has just the receivers, processors, exporters, connectors, and extensions that you want. 

[00:29:00] **Austin:** It’s also a really great way to extend the Collector because in this way, instead of having to fork contrib or fork whichever raw base you want and then bring in the code, you can simply have a Go module published in wherever GitHub and pull that in through the Collector Builder and bring in your custom extensions and things that way. 

**Austin:** It’s a really great tool. It’s also very friendly for your security teams because it gives them a manifest that they can kind of look at and say, “Hey, this is exactly what's in there,” and that way I know if you're at a larger security review can be extremely taxing, and if you're trying to use the contrib image, there's an awful lot of dependencies. 

**Austin:** Something to kind of cut that down to size is very friendly to your friends in security.

**Iris:** There's a new user now for the Builder. I actually had missed this, so there’s actually some—there’s check out the website, OpenTelemetry.io does have a little tutorial on building with the Collector. 

**Iris:** I actually recently used it for a personal project where I was making some custom processors to talk to an API and do some data, so it was really, really fun to use and makes it really easy to use the Collector, maybe the way it's sort of intended. 

**VJ:** As long as we—oh, go on.

**Austin:** I was just going to point out that VJ and Seth very helpfully shared a couple of links about the custom Collector Builder.

**VJ:** Perfect.

**Austin:** The other, sorry, go ahead.

**Iris:** Go on first; you were talking.

**VJ:** Yeah, I'm just going to mention we’ve found it very useful. We have forked just an exporter and are working on trying to get a whole request approved for it. But in the meantime, we continue to need to persist those features and newer versions of the Collector, so this helps us to kind of drag our single exporter along the way and keep rebuilding it with new releases, so it's been really helpful.

**Austin:** That’s another really great use case. If you have custom stuff or modifications, I know that PRs can sit for a bit sometimes.

**Moderator:** I think it’s going to require several moving parts. 

**Austin:** Moving the conversation along since we’re coming up on time almost, what was most surprising as your observability journey got underway? What were the trickiest stakeholders in your case? 

[00:33:56] **David:** I think I should stop saying that, even though I'm probably going to keep saying that, but there's particularly in this space, the technical problems mostly are straightforward. It's all the people problems. To the whole panel, what were some of the surprising things that you found either that were sticky and a little bit tricky and had to navigate or that were surprisingly smooth? Both of those are valid surprises because we've gone over some of the key selling points here that I think a lot of people would recognize as being useful with going in an OpenTelemetry direction, but what's the part we didn't say that was either good or that was almost good?

**VJ:** I'm currently running a poll on LinkedIn where I ask people if they think OpenTelemetry is easy to use. I won't disclose the results of that poll quite yet. I think there's a—I’d be interested to hear from people that have gone through implementation journeys. 

**VJ:** I can say we're in a pilot for integration with applications, and the .NET library is not quite having the log exporter stable until just very recently. That’s been a large deterrent for a very long time. About two years ago, I started trying to convince the application teams to all integrate with OpenTelemetry, and there’s been kind of an appetite for some of the more cutting-edge teams in our company to try out something that still doesn’t have a stable label. 

**VJ:** That was a big deterrent, but now, with I think it’s the 16 release now that that’s stable on the .NET instrumentation library, there’s a better story there, and there's more acceptance now. Outside of that, the Collector itself, just getting lots of new features for free has been amazing. So that’s part of the reason we know about the Builder, just because we have these custom features that we really want, and we keep seeing better, newer versions of the Collector come out. It's like, “Oh wow, we better upgrade because we don't want to sit on this old stuff,” and all these other great people have been contributing a lot of good enhancements, so that's been an easy sell to say here's why part of the why of helping teams move to this instrumentation challenge that will take some time for them to work through.

**Austin:** Actually, I have a—can we play a fun game? Can I get everyone to open up your Zoom chat and tell me what language you use OpenTelemetry in, and do you think it's good? Do you think the SDK is good in that language? This data may be used by the project.

**David:** You don't know is that the part of this request that's hard for me is that the Zoom client on my machine is very temperamental in terms of opening the chat window, so that's the hard part.

**Austin:** Okay, well, I can see the chat node.

**David:** Yes, it’s good. I think JS has gotten a lot better. I think most of the problems in JS right now have a lot more to do with like ESM and CJS and various JavaScript ecosystem things rather than any problem with the language SDK.

**Austin:** Go needs some love. I feel like Go was written by several dear friends of mine who know Go in and out left right and indifferent, and it's great, and it's actually pretty idiomatic Go, I think, but it's kind of hard to use because it's idiomatic Go, personal opinion.

**Iris:** Exemplars not getting implemented in what? Which language? All of them? Some of them? Most of them?

**Austin:** I think the only one where it’s actually implemented is Java, and then from what I've asked around, not really anywhere else. I've seen a bunch of open PRs around that, but no actual implementations. I think we need to get better about closing PRs.

**Iris:** .NET, Python, Java—pretty good.

**Austin:** Yeah, I’d say Java—Kotlin, multiplatform, nonexistent.

**VJ:** Have you looked at the—Derek, there's—I don't know if it actually is in the repos yet, but I know Splunk donated their Android client SDK thing that I think targets Kotlin. I know they want—it either has been donated or is being donated. 

**David:** I’ve been watching that donation process.

**Austin:** Okay, yeah. I've been looking for an SDK that could be used by Kotlin multiplatform to go both on iOS and Android clients. I know some people are using the Java SDK, you know, purely for Android, but we have to just implement that separately.

**VJ:** That is interesting. Will you be at KubeCon, by any chance?

**Austin:** I will.

**VJ:** Look me up when you're there. We’ll talk about it.

**Austin:** Cool. 

**VJ:** Go—terrible. 

**Austin:** Yeah, Go's right now a little bit of a hobby horse for me. I think it's high on my list of languages that feels like it needs some developer experience love.

**Iris:** I'm sorry for taking over your panel, Reese.

**Moderator:** Good. I kind of like it. It's always—I feel like it's hard to get a big group of people online to participate.

**Austin:** So down API.

[00:40:06] **VJ:** Kubernetes is also an interesting one. When I went to Kubernetes years ago, if you're not aware, Kubernetes has started to feature gate an alpha and beta native OpenTelemetry traces from things like CUE and a few other components, and the API server. So there is more support for OpenTelemetry coming in. 

**VJ:** The biggest thing is Kubernetes metrics are all super Prometheus out, and that is probably not going to change anytime soon. But there is some interesting stuff with Prometheus talking about should Prometheus just use OpenTelemetry libraries and stuff like that, so we might see some changes there. 

**David:** Hopefully that can lead to a more unified telemetry story for Kubernetes.

**Austin:** Your functions as a service—I'm sure the majority of people are using Lambda, and that seems to be the main focus of the community at this point.

**David:** I think we really just need people to—I actually—I mean, I will also plead ignorance. I don't know if they have an equivalent for Lambda layers or extension layers. Do you know?

**Austin:** I've looked across the project, and while at the top of the OpenTelemetry site, if you look at the documentation, it says, for example, for Azure Functions, AWS Lambda, there's really not much integration, if any, with Azure itself.

**David:** So my experience on the pipeline side with Azure Functions, I believe is the official name—spending a lot on that naming convention—is there's a dual writer that you can enable, but then you essentially have to set up something that collects all of that information. 

**David:** We recommend throwing up a small Cassandra cluster depending on how much traffic you want to throw at it. But you could be running anything in there, and it's basically a pretty good dual writing mechanism, whereas the Lambda layer stuff tends to have startup and cooldown times that I think—I think actually Azure Functions is probably a more scalable approach, particularly if you get really—

**Austin:** Is it function to App Insights, and then App Insights split out into OpenTelemetry?

**David:** It doesn't bounce off of App Insights; it's from the function itself, I believe.

**Austin:** Yeah, a writer SDK that Microsoft provides to write out to wherever. Part of the challenge was like, "Well, is there a way from Azure Monitor to monitor the actual use of it rather than incurring extra cycles within the function itself?" 

**Moderator:** Sorry, very interesting discussion. Feel free to ping me on the CNCF Slack if you'd like to continue it, but I think it's going to require several moving parts.

**Austin:** Thank you for that. We can definitely host this in another conversation around this another time. 

**Moderator:** To bring it back to this panel, I would like to know how has the focus of your observability practice changed or not changed after implementing OpenTelemetry, and what are you looking forward to next, either from the project or as a result of your observability efforts? 

**David:** I think there are two things we're seeing. One, of course, is we're seeing enterprises really see that the amount of control they have with running OpenTelemetry sort of compensates for some of the grit that exists in the gearbox as they try to make it meet their needs. And then on the other side, we're seeing new developers realize that part of being a competent new developer—and I think the Git comparison is very apt—is, you know, like, "Yeah, of course you know source control if you're going to write code of any kind for production," right? 

**David:** The same thing's happening with observability where previously, maybe, "Hey, you were just entering a few log lines, and then you were ready to go." It's like, "No, we need to know how to actually implement one of these tools from the start and implement tracing from the start." So, sort of from two directions, I think OpenTelemetry is getting a lot of momentum in the next year.

**Austin:** I am looking forward to the day when you have to have a reason to be off OpenTelemetry instead of to be on OpenTelemetry, and to see how that affects the vendor landscape. I’m here on behalf of a vendor, but because we’re in pipelines, if I could get rid of all of our integrations that weren't OpenTelemetry, it would just be so much easier instead of having to do something really bespoke for everything that's out there. 

**Austin:** There was a great article not that long ago that you wrote, right, Noika? Isn't that right? About how OpenTelemetry, at the moment, is being treated in various tools differently? 

**Noika:** We need to talk about that.

**Austin:** I've got some great stuff coming out next week that is not quite so mean to my good friends at New Relic, but I think we all need a little bit of a kick in the pants sometimes. 

**VJ:** One of the really fundamental things, like when I was working at New Relic on the integration with X-Ray data, right, is that it's like, "Hey, yes, we can get—we can shove these things together, and we can make it kind of work." But until we all adopt one open standard, this is just too complex a data type to just really fluidly chart in one view and see from one view and do things like what is the average amount of time consumed by this single function, by the single DB call, right? That is going to be very tough to do until you just say, "Look, we all have to be on these open standards." 

**David:** Some really weird LLM training task, yes. Okay, it's going to use some super strange system for—for this doesn't really look like tracing or time spans at all, cool. But everything else, we need to adopt an open standard to really be able to have OpenTelemetry data be a first-class citizen, so I'm looking forward to OpenTelemetry winning. 

**Austin:** Same. I think it's already won in many ways, right? Just not to toot my own horn or to toot the project's horn, but in, you know, everyone likes to refer to the XKCD, like there's 14 competing standards. I'm going to make the unifying one now; there are 15 competing standards. 

**Austin:** I think, whatever, you know, I’m under no delusion that this is the apex of innovation in the space. There will be future developments and other things, but I think in terms of consolidating the last two, three decades' worth of thinking about application and infrastructure telemetry data and how it can be used together, I think OpenTelemetry is going to be foundational for a generation of observability practitioners, for engineers, right? There will be something in the future, but I think in terms of where we are now, this is kind of the thing. 

**Austin:** Whatever comes next is going to be influenced by things that we don't even see. I've been doing a lot of research on AI and large language models and stuff, and if you look at what you're actually doing when you make a request to a large language model, you're making a trace. It's literally a trace. It is a Dapper-style trace where you have requests and responses. Even the most advanced thing we can think about in terms of software architectures right now, and whatever looks like stuff that we have a way to model in OpenTelemetry today. 

**David:** So until that changes, I feel like OpenTelemetry is, you know, long term going to be what—at least until I am retired, so give it 30 years, but like it's going to be the thing.

**Austin:** Justin in the chat asks any thought on Micrometer’s approach of blending the signals into a single observation API from which traces, logs, and metrics are all derived? 

**David:** I think that's fine. I think it's—I do think that the thing you actually need is we need as a project for people to commit to OTL as being an output format, and we need people to respect, you know, hey, this is trace data, and it should be interpreted differently than metric data or whatever. Even if you are going to like splay those out into a generic sort of event data structure, there are use cases for keeping those things separate in terms of observability pipelining. 

**David:** Yeah, it's—there's maybe not a cut-and-dry answer to this. 

**Moderator:** I would like to point out that it is the top of the hour, so if you have to go, obviously feel free to jump. 

**Austin:** See you all. Have a good day, everyone.

**David:** Thank you all so much. 

**Iris:** Bye-bye!

## Raw YouTube Transcript

all right I think we can go ahead and get started thank you all so much for being here um the panelists we have today are David W Austin Parker VJ Samuel Iris dear Mishi and noika melera I'm hoping I got all those correct and we'll do a quick run of introductions um this discussion is hosted by the open tet tree and user working group which I am part of as um as is Adriana um I see her there and yeah today we're gonna just have a casual conversation um feel free to get as opinionated as possible um about basically the OB uh evolution of observability um practices and actually David since you kind since you the that inspired this um I would love for you to do a quick introduction and um after that we'll just kind of go through the rest of the panalysts but um yeah let's hear a little bit more from David who whose brainchild this was hello everyone my name is David win I am principal people machine something uh at Edge Delta which is in obser the observability pipeline space the thing about being a startup is you just sort of do whatever it needs to be doing so I flexx the title appropriately as such uh but yes we uh Ree and I were chatting about different ideas that might be fun to discuss and one of them that seemed very appropo to the group would be sort of the evolution of observability and where things are going how people are tackling the challenge of Shifting to otel and and what are some of the interesting Lessons Learned along the way not only from the the 10,000 foot view of like we see where the mountains will go but also at the 10 foot view of boy this grass is tall sometimes and trying to get a little bit of a feedback on all different directions of it um yeah so to continue with introductions why don't I'll go ahead and do it popcorn style Iris why don't you introduce yourself next hello everyone uh my name is Iris Iris Irish depending on the country where I am currently I'm based in Portugal uh I'm a platform engineer observability engineer at farfetch uh so my day-to-day is building an observability platform maintaining it modernizing it and offering this kind of service to the engineers in my company I don't know that's that's all about it go ahead and call someone else out Vijay hi everyone uh I'm Vijay Samuel uh I work at eBay uh my day job predominantly revolves around doing architecture for observability platform internally so everything logs metric events tracing helping all our developers uh do uh alerting visualization anomaly detection the whole shebang uh with regards to observability that's that's pretty much what I do Austin hi everybody so I'm Austin Parker uh Community maintainer for open Telemetry um formerly uh light step a part of service now and currently uh it's a surprise and you'll find out very soon what I'm currently doing so yeah I've been a part of open Telemetry since it was created I was a open tracing maintainer I've been working in observability um for over over five years now now and got a lot of thoughts from seeing it kind of grow and evolve from you know what it was to what it is and am I the last person NAA or did NAA that's NAA no I didn't go hi everybody I'm noika I'm at the um open source startup signos and uh have been working with observability stuff from back when we called it Real's Performance Management uh so uh yeah um mainly now working with uh open Telemetry and kubernetes stuff excellent thank you all so much again um and yeah I we have a list of questions that are kind of like intended to help guide the conversation but once everyone gets going I expect it to become a lot more Dynamic so you know I am think we're all totally happy to see where this takes us um so so I guess to get us going um we want to know about the state of the world before you all undertook your open solum to Journey what was working pretty well what did not work or slash what sucked um and kind of what was the moment that prompted you to change and feel free to raise your hand um I think yeah all the panels at least are on camera so if you need visual cues us to when you can step and hopefully that helps but feel free to raise your hand too and then we'll do that way as long as Austin doesn't thumbs up we'll be good I turned off the uh the reaction thing I'll actually start um because I think I have what is probably not a very hope maybe not a very unique story but an interesting one so before I got into observability as a field I um I started out in software doing QA uh I was a soft estet software developer and test and this was you know uh 20 2013 2014 I guess is when I started really getting into into technology as a career or software as a career I should say and the cloud was you know a thing but Cloud native wasn't quite a word yet right like we we didn't have this concept of like oh we're just building all these things with all these cool apis and this idea of like infrastructure on demand or whatever and so I saw you know the company I was that go through these various Transformations and one of them and what I helped kind of lead there was a devops transformation where we went from okay um when you build your code and you deploy your C you know you you write you pull a ticket you write some code works on your machine great you push it and then that night someone else gets to to deploy it and see if it actually worked and one of the things we wanted to do was really tighten up the feedback loop here right we wanted to get from 24 48 hours before changes got into test to minutes or you know hours and minutes and a big part of that was getting on demand infrastructure rather than sort of static infrastructure so we're going through this and we're building all this out and we're we're getting stuff into the cloud and it's great and what we started to see though was it wasn't actually fixing a lot of the problems we had like there was kind of this you know ground truth that everyone had agreed on beforehand it's like well the problem is is that we have you know the infrastructure is bad right like these servers are not properly cared for uh we're just doing we're wiping stuff and recreating it rather than you know actually getting fresh um images every time so it must be some just config thing it's probably not the code and then you know something we go into production it' make it into a patch and uh then the customer come back and say like hey this is actually broken and we missed it because we thought this is because of the you know our testing infrastructure so when we start going into the cloud we have fresh images we have all this stuff and we're finding all these problems that we really didn't even know about before and the question came back it's like well what you know we don't how do we know what's going on how do we know what's breaking our product was a um platform as a service right so we had you know hundreds or you know hundreds and hundreds of nodes um various logs in all sorts of different places it was Windows servers it was Linux servers you know we had all these different databases and it was a very diffic you know it was kind of big it was hard to keep your your head around and someone one of the engineers actually came back and said okay I made a topology service topology and it looked like um you know if you seen one of those like nail art things where someone will make a picture by putting a bunch of nails and a piece of wood and then tying string together it was like that right where you have just like lines everywhere and things connecting each other nobody can keep this in their head nobody could understand how Services actually talk to one another you could look at like a very small section and you could say like okay I I get this but looking at it holistically was impossible and I brought in um at the time you know we tried New Relic we tried data dog we tried a couple things and what I found was ironically enough that it didn't matter what tools I brought in the developers weren't interested in using them just because it wasn't data it wasn't information that was kind of like at their level right like they didn't understand you know how do these things correlate how do these you know how what does it mean when SQL Server spikes and memory usage increases but there's no way to really tell like what was happening and that's got me into observability right is is trying to answer that fundamental question uh someone was piggybacking yeah that so so right off of that I remember working in New Relic uh TR thean of my time there and there was this really fundamental thing of like oh this shows like how this request hit all these these spots and it shows it as like a a trace right with a bunch of time spans including like individual fun calls on all these different Services pretty cool but we we get these questions back that were like well just show me where the request went show me what services were hit and also like in an interpretable version and it was an example where there's a lot of focus on getting a certain piece of information back right but the developers wanted information that was at a different level this is was exactly it is like hey I just want to know where the request is going what services are involved and how the failure on the SQL Server might come back up and affect the front end in these ways and that that was very very hard to tease out whatever this was seven years ago but it's a similar theme right it's it's really about making sense of a pile of stuff again trying to zero in on that moment before we get into these things there's there's the point where you have everything and then you realize that have you thought that having everything was the problem and then it's not and then you're like no now I have everything and I still don't understand everything well maybe I need to actually draw a map and then you draw a map and you're like wow this computer doesn't know how to draw maps and somhow this just looks like a bowl of spaghetti um and then you're like cool how do I Zone this back down again in and out and in and out so it seems like sense making is at least one of the unifying Concepts we see there BJ IRS whoever wants to take it is that a similar feeling that you guys got when you were hitting this this inflection point uh for us the problem space was a little bit different in the in the sense that um pre-open Telemetry we have a pre- cloud native uh era and during Cloud native as well so if you take the pre- cloud native like we we have had something called the centralized application logging platform inside of eBay for more than 20 years and it had the concept of transactional logging where you have a root transaction nested transaction ction events very similar to what we have in the tracing world today uh but the problem with the with the pre- cloud night era is always that developers come into eBay they have to learn proprietary clients uh we had clients only for a few languages so if they are not using that or writing their own code you cannot observe things inside uh inside the company or you're on your own to figure out uh spin up your own elk stack or uh anything that you can do to to monitor the system and when Cloud native H uh and we had the large scale kuus adoption we had the Prometheus end points scraping from Lock files a little more flexible but you do not have standardized sdks across the board even for metrics it used to be that a few people used to use the official Prometheus client some used micrometer and for nodejs you didn't even have an official community supported SDK so I think that's where like when open telary came in with the promise of standardization it was like okay now we can just offer all our developers one standard um uh and it's Community managed they can hop from any company into eBay and they should be able to uh be able to use the observ platform as long as we are open Elementary compliant so I think ours was our story is a lot more developer uh productivity focused at least in the beginning yeah I would say that um my my viewpoint my story is kind of a bit like VJ uh my observability experience first started with some companies that had zero observability in place and that's how I I I got to know it I was like okay what is happening so that was my my job and then where I'm currently working I jumped on a completely different world that had a very nice observability platform very nice observability culture which was a big shock and our job was actually not to come up with waste to monitor but to just keep improving and of course we came to that uh bottleneck that we were using a lot of Open Source we were using APM vendors and to get the information you have to go in 10 different places which is not not great and I would say that it is in developer productivity Viewpoint as well uh why we went the open Telemetry uh route because everything is centralized and we can move from vendor to vendor if we need we are collecting everything standardizing everything so I would say we have have kind of the the same case here I think to both of your points like that to me is what is really transformative about open Telemetry is that it it offers a you know for the first time I think um This truly Universal idea of like how should I as a developer emit this Telemetry information right like I think VJ your story you know is not unique like most especially large distributed um most companies that have like large distributed systems have had some sort of transactional tracing you know some sort of structured lws with transaction IDs that they can you know look at for a decade or more like you know when we think about an open Telemetry Trace you know the actual data model is very similar to what um Google produced in Dapper you know almost 25 years ago now like there there's nothing new Under the Sun what's been missing is the idea that this is actually like a core part of being a developer like a core um tool in the toolbox a core part of your trade is learning how to emit good Telemetry about what your system is doing and open Telemetry provides a standards-based way to do this that also can be natively integrated and available through your MOS service framework or your RPC library or whatever else and that to me is you know we're we're we're at the beginning of the beginning almost of seeing how that actually impacts uh the industry n oh yeah see Iris also has her hand up as well let let Iris go first I've talk too much there is a problem I just want to add a little bit of what Austin was saying as well it's absolutely I mean nothing to add on top of that but uh the other good thing about of telemetry is that you don't have to break your whole system to implement it as well that is one of the the great things because it's compatible with almost all technologies that we currently have especially the open source ones so you don't have to cause downtime have your developers blind for hours and days you can just put it there flawlessly so that's the best the best of all for for me yeah I think there's really two fundamental shifts right there's one is we started adopting architectures that became much harder to diagnose with the stuff that you taught yourself when you taught yourself to code right that like hey adding in some locking lines and such is not going to work on a large microservice Arch Ure the other change and this is bit it's been a while but but it's still there is that when you have conversations about funding when you have conversation with VC they're going to ask you about your observability setup right so those two changes combined have really changed what where the conversation is with observability where it went from an internal discussion about maybe developer velocity and roll back times and these other pieces to like yeah we we we have to have this and it has to be applicable across this this stack I think uh there is some Fair bit of standardization that gets promoted uh on the on the consumer side of things as well uh given that there is a standard in how you instrument Tech metrics for example gauges counters exponential histograms U whatever like earlier it used to be that if you picked one time series database you would get certain capabilities defined in a certain way if you have to switch then the likelihood of of that capability existing may not be um always possible but now since there is a standardization in the inest eventually there will be a good amount of standardization on what technology that the providers either open source projects or vendors provide um so I think uh the net benefits are definitely there uh across the board not just on the on the instrumentation side so I think that's actually you bring up a really interesting point right like historically um everything about Telemetry has really been a a pretty binding abstraction to the data store so if you were use you know statsd metrics worked in the way they did because of the way that stats the you know store and let you query that Prometheus metrics work the way they do because of how you store and query them um logging databases tend to you know if you're using elastic search or something like the format mattered and that influenced how the client libraries were designed and so on and so on and so on and so forth with open Telemetry you know we kind of break that right like you have to open symetry tells you like hey this is what a histogram looks like this is what a structured me event of any kind looks like and I think it's interesting that you also see um this rise in popularity of column stores for storing observability data like metrics logs traces session whatever um you throw a rock and you'll hit a new you know column store based or heck new click house based um Telemetry thing like and it's great and I think open Telemetry has actually been a huge factor in this because it used to be if you wanted to go and build like an observability tool or build some sort of analysis tool you would have to like come up with all this stuff or no one would use it you'd have to come up with your API you'd have to come up with your s you'd have to build Integrations for 40 billion things and open Telemetry means like actually that's just that's all a commodity now you just get that for free with otel and you can build really interesting um workflows on top of that data and I think that's very friendly for developers right like going forward we haven't even really started to see like how do these tools become integral in you know devel vment workflows right when do we get to you know IDE level integration and all that so it's I know it's very easy to feel like open Telemetry is done or or like that it's kind of like hitting this Plateau but I think we're really really still in early days um with what people can actually do with it I I have to agree on this being early days for otel um so a little bit about my background is I was in observability before I hopped over to CL for a bunch of years and now I'm back in the observability space and the conversations that I'm having with people today are they remind me a lot about conversations about get like 15 years ago or something like that when it was sort of a perennial joke that everyone's like oh yeah get looks really interesting we're thinking about moving to get I mean we're still on you know we're still on insert you know uh insert system here but uh it feels almost every conversation I have and again we're in the p blind space so it's about um it's about routing and moving and transforming all the data to the right spot but everything that comes out of us is otel so if they just need to bolt on something and they want to do that they can do that but the first thing they talk to us about is something with some sort of data not being in the right place and the second thing is oh also we're we're starting to look at otel is that good and being the second question I think is a good sign of like climbing the curve uh but I think it's I think there's still plenty of room to grow and I guess speaking of that um you know how did you first hear about the projects um I think it's kind of clear like some of the things have drawn you all to it um something I think too that people might be interested in is how did you get leadership Buy in um I can say something about this because for me it's it's easy so how I heard about the project I have um when I start started working with observability I became obsessed with tracing I'm still fascinated by tracing so I was like oh tracing is so amazing and I started just searching online about then I came across open Telemetry I started playing with it uh for for a long time and not actually implementing anything so when I joined farfetch uh we were like okay we need something new something better okay about open Telemetry and it was very surprising because there was an immediate support uh I was I like to say and brag that we have very good observability culture but I maybe everyone had heard about it we just made a small presentation and our leadership was on board of course it has a lot of benefits and we had to present what we're lacking uh the issues that we're currently facing it what how and what MRI can help us with and it was immediate support so every time I tell the story some people don't believe me but that's that's exactly how it happened it was it was very interesting very very fast support in my case um I can maybe go next um at least in um our case U uh I think uh I first came to know about it uh as part of the uh open telary will will basically be the standard uh and uh we would eventually retire open tracing and open census I think that uh we were passively uh trying out open tracing around that time um and this announcement came and it was like Hey interesting this is something that we need to watch out for and uh we were doing passive experiments with the the open telary SDK and uh The Collector uh for a few months and once the the hit stability that's when we made the informed call that okay we'll we'll start tracing with the open Telemetry as an offering inside the company so now we had the open Telemetry SDK for traces and open uh Telemetry collector for the for for the accepting of all the spans but on the other hand we had uh metric beat and file beat for uh collecting uh logs events and metrics so uh at that point we were at a cross roads of if we should support multiple agents or just standardize across open Telemetry collector for everything and I wrote a very big memo internally on what it would take to migrate out of uh uh beats for uh metrix logs and events um presented it to leadership and I think we felt that uh being on a vendor neutral industry standard would be the better thing for us in the long run and it's best to do it now than than later on so that's that's how we did the migration but and on on our end the metric migration was a massive one because uh we we at that time if I'm not wrong we were already at 32 million samples per second in just across more than 100 kubernetes clusters um million plus Prometheus and points and whatnot so it was a very very involved migration that we did but the but the but the benefits are definitely there now BJ were there any other when you're coming from a culture that has a strong kind of build Vibe uh I'm what I'm trying to get is do you guys run your own collector drro or are you taking something like how how do you managing that in terms of the particular flavor that you decided to land on uh we do run our own um drro internally but I think uh uh the number of custom processors that we run um are very less uh it's mostly that we don't need all the exporters for all the various vendors that are there being packaged in so we create only the ones that we absolutely need uh but for now like we export using either OTP or Prometheus remote right um protocol which are pretty much open standards right now uh so it's about being lean actually I are you all using the collector Builder to make your images uh right now no uh we just craft our own uh go do mod and then just just do it ourself yall thinking about using the collector Builder I think we can like we haven't we haven't really explored it uh uh but yeah we should yeah I'm just trying to get more peopleware The Collector Builder because I think it's really cool aw is it could you say what the collector Builder is so the collector Builder or OCB is a tool that we provide from The Collector repo and what you can do is you can give it a manifest of go modules and it will create a custom image or a custom build of the open Telemetry collector for you so you like eBay can you know get something that has just the receivers processors exporters connectors and extensions that you want it's also a really great way to extend the collector because in this way instead of having to fork contrib or Fork whichever you know raw uh base you want and then bringing in the code you can simply have a go module published in wherever GitHub and pull that in through the collector Builder um and bring in your custom extensions and and things that way so it's a really great Tool uh it's also very friendly for your security teams because it gives them a manifest that they can kind of look at and say like hey this is exactly what's in there and that way I know if you're you know at a larger or um Security review can be extremely taxing and if you're trying to use the contrib image there's an awful lot of dependencies so something to kind of cut that down to size is very friendly to your friends in security wow there's a new user now for the Builder I actually had missed this so there's actually some there is so check out the website open open Telemetry doio does have some has a little tutorial on doing building with the collector um I actually recently used it for a personal project where I was making some custom processors to talk to an API um and and do some stuff some data so it was really really fun to use and it makes it really easy to you know it makes it a lot easier I think to kind of use the collector maybe the way it's sort of intended um I don't know necessarily know if Our intention was for people to just be pulling this giant contrib image constantly um as long as we oh go on oh I just gonna point out that um VJ and Seth very helpfully uh shared a couple links about the custom collector Builder ah perfect so um the other sorry go ahead go on first you were talking sure uh yeah I'm just going to mention we' found it very useful we um have forked just a exporter and are working on trying to get a whole request approved for it but in the meantime we continue to need to persist those features and newer versions of The Collector so this helps us to kind of drag our our single exporter along the way and keep rebuilding it with new uh releases so it's been really helpful yeah that that's another really great use case is if you have custom stuff and you or if you have just Mo modifications I know that PRS can um sit for a bit sometimes so yeah um but the other cool use case the other and also my current one of my current hobby horses is if you are using you know uh large language models to assist you in development so things like co-pilot or chat GP then you know it's not perfect because of the data cut offs on that but over time like as um especially as things like P data and other um structures in The Collector repo stabilize then you know it's actually pretty easy to ask chat you can ask chat GP chat GPT like hey scaffold me a collector extension uh it'll use the wrong apis but they're actually but um it'll do 80% of the for you and then you can ask it like hey I need to transform this data or do what you know do whatever and it's actually a pretty good use case for uh AI assistants I think in in programming so yeah especially that once you get to the point of like just wanting the Rex to like transform it in a certain way it's a good good moment for the co-pilot yeah um I don't know if it understands OTL open tary transform language yet probably not but you know the models will get better and um I think the biggest thing is just St you know once we get to the point where things are changing a little less frequently and it can get into the model so stay tuned maybe maybe we'll have something to say about that as a project in the future um okay so moving the conversation along since um we've coming up on time almost um what was most surprising as your opary journey got under way so like what were the trickiest stakeholders in your case um I think David you like to say in observability there's no technical problems only people problems yeah yeah I should stop saying that even though I'm probably gonna keep saying that but there's you know particularly in this space the the technical problems mostly are straightforward it's all the people problem so what to the whole panel what were some of the surprising things that you found either that were sticky and a little bit tricky and had to navigate or that were surprisingly smooth both of those are valid surprises because um we've we've gone over some of the key selling points here that I think a lot of people would recognize as being uh useful with going in an Hotel Direction but what's what's the part we didn't say that was either good or that was almost good yeah I'm currently running a poll on LinkedIn where I ask people think if they think Opa telemeter is easy to use um I won't disclose the results of that poll quite yet I think there's a yeah I'd be interested to hear from people that have gone through implementation Journeys I can say we're in a a pilot for integration with applications and U the the net library is not quite having the uh log exporter stable until just very recently has been a large deterrent for a very long time um so about two years ago I started trying to convince the application teams to all integrate with open Telemetry and there's been kind of a an appetite for some of the more Cutting Edge teams in our company to try out something that still doesn't have a stable label but uh that that was a big Det current but now that with I think it's the 16 release now that that's that's stable on the net uh instrumentation Library there's a better story there um and there's more Acceptance Now um outside of that the The Collector itself just getting lots of new features for free has been amazing so that's part of the reason we know about the Builder just because we have these custom features that we really want and we keep seeing better newer versions of The Collector come out it's like oh wow we better upgrade because we don't want to sit on this old stuff and all these other great people have been contributing a lot of good enhancements so that's been an easy cell to say here's why part of the why of helping teams move to this instrumentation challenge that will take some time for them to work through um actually I have a can we play a fun game can I get everyone to open up your Zoom chat and tell me um what is what what language do you use otel in and do you think it's good do you think the SDK is good in that language this data may be used by the project see what you don't know is that the part of this request that's hard for me is that the zoom client in on my machine is very temperamental in terms of opening the chat window so that's the hard part okay well I can see the chat node yes it's good I think node's got I think JS has gotten a lot better I think most of the problems in JS right now have a lot more to do with like esm and CJs and various um JavaScript them things ecosystem things rather than any problem with the language s De uh go lot of changes for metrics yeah go needs some love I feel like go was written by um several dear friends of mine um who know go in out left right and indifferent and uh it's great and it's actually pretty idiomatic go I think but it's kind of hard to use because it's idiomatic go personal opinion uh exemplars not getting implemented in what which l language all of them some of them most of them I think the the only one where it's actually implemented is is Java and Java inet and then from what I've asked around like not really anywhere else like I've seen a bunch of open PRS around that but no actual implementations I think we need to get better about closing PRS um net python Java pretty good yeah I'd say Java Java cotland multiplat form nonexistent um have you looked at the uh to Derek there's uh I don't know if it actually is in the repos yet but I know Splunk donated their Android uh client SDK thing that I think targets cotlin I know they want they either either it has been donated or is being donated um I didn't check yeah I've been watching that donation process um okay yeah for us we were looking for an SDK that could be used by cotland multiplatform to go both on IOS and Android clients um I I know some people are using the Java SDK you know for purely for Android but uh we have to just Implement that separately yeah um that is interesting uh will you be a come by any chance I will uh look me up when you're there we'll talk about it cool uh go terrible yeah go go is right now a little bit of a hobby horse for me I'm I'm I think it's is high on my list of languages that feels like it needs some developer experience love I'm sorry for taking over your panel Reese good I kind of like the in it's always you know know I feel like it's hard to get a big group of people online to participate so down API yeah kades is also an interesting one um when I went to when we went to kubernetes years ago you if you're not aware U kubernetes uh has started to feature gate an alpha and beta um Native oel traces from things like cuet and a few other comp and the API server so there is more support for otel coming in the biggest thing is uh kubernetes metrics are all super Prometheus out and and that is probably not going to change anytime soon um but there is some interesting stuff with Prometheus talking about like how you know should Prometheus just use like Hotel libraries and stuff like that so we might see some changes there um and hopefully that can lead to a more unified sort of met Telemetry story for kubernetes a your functions as a service um I'm sure the majority of people are using Lambda and that seems to be the main focus of the community at this point yeah I think we really just need people to I actually I mean I will also plead ignorance I don't know if they have an equivalent for uh Lambda layers or extension layers do you know i' I've looked across the project and while at the top of the just the Faz offering of the open ometry if you look at the documentation it says for example Azure functions AWS Lambda there's really not much integration if any with Azure itself so my my experience on the pipeline side with Azure functions I believe is the official name spending a lot on that naming convention uh is there's a dual writer that you can enable but then you essentially have to set up something that collects all of that information so you would we recommend like throwing up a small Cas cluster depending on how much traffic you want to throw at it um but you could be running anything in there and it's basically a pretty good dual writing mechanism whereas the lamb layer stuff tends to have start up and cool down times that I think I think actually Azure functions is probably a more scalable approach particularly if you get really is it is it function to app insights and then app insights split out into otel it's not it doesn't bounce off of app insights it's from the function itself I believe yeah a writer SDK that Microsoft provides to right out to wherever um part of the challenge was like well is there a way from Azure Monitor to Monitor the actual use of it rather than incurring extra Cycles within the function itself toit its own symetry but yeah okay I think the only tricky part is if you definitely want to use an alternative Source you have to essentially restrict the IM permissions of the function to not write to app insites and that's the only way to actually cut it off I think okay also I'm just gonna step in real quick to to bring around wor sorry very this is a very interesting discussion uh feel free to ping me on the cncf slack if you'd like to continue it but I think it's going to require um several moving Parts yeah okay thanks we can definitely um Host this another conversation around this and another time um but to bring it back to to this panel um I would like to know how has the focus of your observability practice changed or not changed after implementing open Telemetry and what are you looking forward to next either from the project or as a result of your observability efforts and David is not very emphatically SU I'm gonna go with David no no I was mostly nodding because I definitely want to hear this answer because there's a I alluded to this earlier but there's there's sort of this moment where we think if we have everything that definitely is valuable but it feels like it's not enough and then if we feel like we could put it all I'll I'll use the phrase single paint of glass if we single paint of glass something then something will be better and and then it's not and so then we got to figure out the next next thing so I very interested to hear from the rest of the panel VJ I'll pick on you okay uh so um um uh starting from the previous question right like uh when we did the migration there was one feature that we were shocked that did not exist and I don't think it still exists which is dynamic reloading of configurations uh that was very crucial for us especially while scraping Prometheus and points in kubernetes so that was something that we had to build on our own as a custom receiver uh but but that is a feature that I would I would really like to see on the collector um uh because if that exists then we don't have to have a custom receiver that can like watch on the APA server look for annotations create configurations when the Pod comes up and then tear it down when the Pod goes down um you should definitely pay attention to cubec con Hotel announcements then okay okay yeah I'd be I'd be definitely interested in that uh that that that was one uh but uh the other one is Asian management at scale I know we have opamp but we don't have like uh the an offthe shelf implementation within the open Elementary community that people can uh spin up and then do agent management uh we we have we'd probably have to uh do it ourselves um for larger companies it might might be still uh easy to do but uh I think for the general public I think it would be easy to just have something that they can take up and run it run with it uh with tracing one of the challenges that we had was not enough uh reference implementations uh especially around the collector uh saying okay where does enrich uh K enrichment sit in the pipeline where does red metric uh or span metric connector sit and what are the parameters that we could potentially dun uh not enough people have talked about it uh at least for metrics we did a case study ourself to say that these are the challenges that we might people might run into but for tracing it would be really good if there were more case studies on how people are using exemplars how they're setting up the um collectors um anything that people need to be aware of with regards to grpc I've heard many folks uh complain about uh grpc pinning the connection to a certain collector and then killing it um but like we haven't seen good enough good blog post that actually solve it um so I think like yeah a lot lot more case studies would definitely be helpful um for our part like we are doing two talks in cucon uh to share about what we are we have learned specifically with tracing in metrics around open Telemetry but uh it would be great if we hear a lot more Aris well from our side I think there hasn't been a challenge so far that we haven't been able to find the solution and that goes back to also the poll that Austin was mentioning if you were to choose how is it is open Tel easy I would say in our case has been pretty easy um the challenges that we're working on and that of course I'd like to see improved in the future uh we're mostly using the op Telemetry operator and I have a LoveHate relationship with with a collector that is being deployed by The Collector there's a lot of things missing there especially the presets uh you test something on the normal collector and then you go to implement it on the out of the box and it's not there so that's something that has been a challenge of course uh we can can do it in with custom configurations U and I would say that the other challenge is logging uh we're doing pretty well with and traces and generally we get the good information that we need if we're missing something here on or there we find the work around R it but logging is not part of uh is not being touched by op Telemetry at all for us so that's something that I would like to see it a bit more stable uh and hopefully have it uh full full unified observability with open Telemetry not just the metrics and traces which of course have been the the center of of the project right now so just waiting for more changes on the logging part Austin did you have anything to add to that um I think that uh I think everyone will be really excited to hear about the stuff that we will be announcing and talking about at cubec con this year coming up in November live from Chicago if you are attending in person there will be a ton of great open Telemetry announcements at activities and various other things um and we're really excited about you know not only the progress that the project has made this year but also what is coming next so stay tuned W activities too ni or David did you have anything to add as far as like the focus of your observably practice um and or what are you looking forward to next from otel or your Ali efforts I think there's I think there's two things we're seeing one of course we're seeing Enterprises really see that the amount of control they have with running open Telemetry sort of compensates for some of the some of the grit that exists in the gearbox as they you know try to make it meet their needs um and then on the other side we're seeing new developers realize that like part of being a competent new developer and I think the get comparison is very apt like you know like yeah of course you know Source control if you're going to write code of any kind for production right and so the same thing's happen with absorbability where previously maybe hey you were just entering a few log lines and then you were ready to go it's like no we need to know how to actually uh you know Implement one of these tools from the start and Implement tracing from the start so sort of from two directions I think open Telemetry is getting a lot a lot of momentum in the next year yeah I I am looking forward to the day when you have to have a reason to be off hotel instead of to be on Hotel um because and and to see how that how that affects the vendor landscape so I'm I'm here on behalf of a vendor but because we're in pipelines like if I could get rid of all of our Integrations that weren't otel it would just be so much easier um instead of having to do something really bespoke for everything that's out there but there was a great article not that long ago that you wrote right NOA isn't that right about how otel at the moment is being treated in various uh tools differently need to talk about that I've got some great stuff coming out next week that is not quite so mean to my good friends attended I can do Relic but I I think we all we all need a little bit of a kick in the pants sometimes I think it's allowed yeah one of the really fundamental things like when I was working at New Relic on the integration with x-ray data right is that it's like hey yes we can get we can shove these things together and we can we can make it kind of work but until we all adopt one open standard this is just too complex a data type to just really fluidly chart in one View and see from one View and do things like what is the average amount of time consumed by this single function by the single DV call right that is going to be very tough to do until you just say look we all had to be on these Open Standards some really weird llm training task yes okay it's going to use some some super strange system for for this doesn't really look like tracing or time spans at all cool but everything else right yeah we need to we need to adopt an open CER to really be able to have otel data be a first class citizen so I'm looking forward to otel winning that's all same I think it's already won in many ways right like just not to toot my own horn or to toot the Project's horn but in you know Everyone likes to refer to the x kcd like there's 14 competing standards I'm going to make the unifying one now there are 15 competing standards like I think whatever you know I'm I'm under no delusion that like this is the Apex of innovation in the space like there will be you know future developments and other things but I think in terms of you know consolidating the last two three decades worth of thinking about application and INF Str Telemetry data and how it can be you know used together like I I think open Telemetry is going to be foundational for a generation of you know observability practitioners for engineers right like there will be something in the future but I think in terms of where we are now this is kind of the this is the thing um and whatever comes next is going to be influenced by things that we don't even see like you I've been doing a lot of research on AI and large language models and stuff and if you look at what you know what are you actually doing when you make a request to a large language model you're making a trace it's literally a trace it is a Dapper style Trace where you have requests and responses you know even the most advanced thing we can think about in terms of software architectures right now and and whatever looks like stuff that we have a way to model in otel today so until that changes um I I feel like open Telemetry is you know long term going to be what at least at least until I am retired so you know give it 30 years but like it's going to be the thing so Justin in the chat asks any thought on micrometer approach of blending the signals into a single observation API from which traces logs and metrics are all derived I think that's fine like I think it's I do think that like the thing you actually like the thing we actually need is we need as a project for people to commit to OTL as being an output format and we need people to respect you know hey this is Trace data and it should be interpreted differently the metric data or whatever even if you are going to like splay those out into a generic sort of event um data structure there are use cases for keeping those things separate in terms of observability pipelining um yeah it's there's maybe not a cut and dry answer to this um I would like to point out that it is the top of the hour so if you have to go um obviously feel free to jump see you have a good day everyone take everyone care everybody so much everyone byebye

