# OTEL in Practice Presents: A Hands-On OTel Migration Story with Jacob Aronoff

Published on 2023-10-02T15:08:33Z

## Description

What happens when you're an Observability vendor migrating to OpenTelemetry? Jacob Aronoff knows exactly what that's like, ...

URL: https://www.youtube.com/watch?v=pHHINe9D94w

## Summary

In this video from the "otel in practice" series, Jacob Arof, a staff software engineer from Lightstep, discusses his experience leading a migration from StatsD and OpenTracing to OpenTelemetry. The session covers the various migration paths, including an "All or Nothing" approach and a "Slow Tail" method, weighing their pros and cons. Jacob emphasizes the importance of obtaining organizational buy-in for migration efforts and outlines specific challenges faced during the process, such as performance issues and maintaining compatibility. He encourages audience participation and shares strategies for overcoming resistance from developer teams when transitioning to new observability tools. The discussion concludes with insights on future plans, including potential log migrations and enhancements to metrics collection. The video aims to provide practical advice for teams considering similar migrations in their observability practices.

## Chapters

00:00:00 Introductions
00:01:50 Migration overview
00:03:40 Challenges in migration
00:05:30 Discussion on migration paths
00:07:55 All or Nothing approach
00:12:10 Slow Tail migration method
00:22:45 Discussion on performance issues
00:30:15 OpenTelemetry protocol discussion
00:40:20 Managing collectors in production
00:48:35 Future plans for log migration

## Transcript

### [00:00:00] Introductions

**Speaker 1:** Welcome everyone to Otel in Practice. Really stoked for such a good turnout. I think this is one of our best turnouts! Yay! And for anyone who could make it today, there will be a recording. So if you have friends who are like, "Damn it! I missed this," you can let them know that we will be posting the recording and prettying it up once it's available. Jacob presented, I want to say, last month for Otel Q&A. We worked together at Lightstep from ServiceNow. I'll let Jacob take it away.

**Jacob:** Thank you! So yeah, my name is Jacob Arof. I am a staff software engineer. I work on our Telemetry pipeline team. Our team is sort of in charge of our internal and external open source Telemetry efforts, whether it's like OTL SDKs in Go, The Collector, or the Operator. Sort of anything that you would be using to collect and send data to your vendor is what we do. 

### [00:01:50] Migration overview

Today, I'm going to be talking about a migration that we led last year and a little bit over a year and a half ago, for when we migrated from StatsD to OpenTelemetry and also from OpenTracing to OpenTelemetry. I'm going to talk about the various migration paths that are out there, some of the things that went well and went wrong during our migration. But I also would like this to be, you know, feel free to ask as many questions as you have. My slides are pretty light, so I'm really interested to hear, you know, where people struggle currently, what type of challenges you're running into, what features you're interested in. So please, you know, raise a hand or comment in the chat, and I'd be happy to answer any questions and go down any interesting avenues that you might have.

With that, I will share my screen. Can we all see my slides? Yes? Great, thank you very much! 

So yeah, today I'll be talking about that OTEL migration story. 

First, let's talk about a problem. I've been in the industry for, you know, many years. I’ve had to lead a few of these migrations where your hand-rolled metrics or tracing or logging library just isn't cutting it anymore. Something where it's not performant enough; maybe the maintenance is really expensive. Maybe there's a feature that everybody wants that you just can't implement. You know, whatever it might be, you want to do a migration. It can be very challenging and daunting depending on the amount of services that your company runs, depending on what the organization's willingness to undergo this migration might be. 

### [00:03:40] Challenges in migration

All of these are problems that I've faced in various jobs, not just where I am now, but you know, a few past roles as well. It can be really hard to make this happen, especially when you, as maybe an SRE or a DevOps person, or maybe just an engineer, just want to make this work. When you feel very passionate about making it happen and you just can't seem to get the buy-in that you need. So that is sort of the theme for today. 

And there is a solution, right? It's like migrating to this new thing. OTEL's Metric API, I'm going to be talking about metrics, traces, but you know, this is really true about a lot of stuff. For OTEL, the Metric API, the Traces API, and even the Logging API is accepted by most major vendors out of the box now. The API has support for, you know, all these instrument types in all of your favorite languages, probably. There are very few that don't. And the performance and compatibility are always top of mind for us throughout the stack of OTEL components. Everybody is always thinking about performance.

So, you know, with all this in mind, you're like, "Yeah, this sounds great! I want to start using this." But there's a problem, right? Like, using these new tools, migrating these new tools is hard. How do you break up the work? How do you know all of your telemetry is working the same as before? What are the risks of these different migration models? What even are the different migration models? And probably the most important one here is how do you convince leadership that this is worth doing? 

### [00:05:30] Discussion on migration paths

You can try and, you know, brute force your way into doing a migration, but that to me is a recipe for burnout. Doing a migration in general, where you know there’s a new architecture that you think is really going to improve the quality of life for your team, for your company, without getting organizational buy-in is how you spend months on a project that may never see the light of day. That is really demoralizing and very difficult to work around. 

So the first thing that you have to do when you want to migrate to a new tool is make the case. It’s not even, you know, show a proof of concept; it’s just can you convince people that this is worth doing. Sometimes a proof of concept helps, but that’s not the key to success. The key to success is really getting a group of people who agree with you that you're able to sell one-on-one who can help you really make it clear to the stakeholders, you know, in your leadership that this is worth doing. 

For me, it was pretty straightforward. We’re, you know, a main OpenTelemetry contributor. The OTEL Metrics API and SDK wanted to go into their, you know, 1.0 stable, and they really wanted some quick feedback from someone who has a lot of traffic to test it out and see where the edges are. So in that case, it was very easy to sell the migration. 

In past jobs, the selling point is usually, you know, you look at maintenance cost and performance overall. I worked with a hand-rolled solution in a few jobs ago, and when it came time to do a migration, the thing that really sold people was just counting the amount of tickets and hours that we had to spend on, you know, new features and maintenance on our internal library, as well as the amount of times where, you know, we’ve had an incident because something related to our instrumentation is just incorrect, which, you know, does happen a lot with your own hand-rolled stuff. 

So that is maybe a good basis to go off of. Does anyone have any questions sort of before I go into more specifics about this section? 

**Speaker 1:** I'm going to do like a five count, something I learned from a teacher of mine. I’m just going to, we’re going to do five seconds of silence until someone raises their hand or I’ll just keep going. 

**Jacob:** So no cool. Oh, is there something in the chat? Oh, that's just me moving it a little. Cool, no worries. Thank you! 

### [00:07:55] All or Nothing approach

So let's talk about the migration paths. The first one is what I call the All or Nothing. This method has you entirely rip out your existing instrumentation in favor of OTEL. This would be, you know, a lot of people talk about you know, replacing the engine of a running plane or running car. A lot of people will say that. This would be like, you know, just getting a new plane and having everyone hop to the new one while it's flying. 

So it's difficult, but you know, maybe it has its benefits. One of the pros is that, you know, once you've pushed your code and you've confirmed things are working and you have a good enough CI/CD system, your work is really done, right? It just rolls out and everybody's happy. This reduces the time for split brain. Split brain is what happens when you're on two systems that may not be compatible together. You could imagine, you know, if you're on StatsD or if you're on Prometheus even. 

If most of your metrics come from Prometheus, they don't have periods in them; they have, you know, pretty strict requirements about their shape overall. You can't do things like up/down counters; like that's not a type that they have. They used to not have a proper histogram; they now do. They now have exponential histogram support, which is experimental, but you know, there are things that OTEL has that Prometheus or StatsD just does not have and doesn’t have the ability to do. 

Being in a split brain where some services have it and some services don’t and they're emitting different metrics and different shapes, different variables, that can get pretty unwieldy pretty fast. And so if you're not very deliberate about planning how you're going to migrate your dashboards and alerts, then you're going to be stuck with both of them for some time, which, if you're on call and if you've been on call during a migration for this type of stuff, that gets really painful. 

If you get paged at 3:00 a.m., you have to wake up and go, "Oh, you know, which dashboard should I check? Is this service on OTEL or is the service on StatsD?" That's really frustrating. That's the type of thing that you don't want to have an on-call engineer think about. 

So the other benefit of doing this All or Nothing approach is that the issues are incredibly visible. If a dashboard breaks, if an alert pages or a service crashes, it’s pretty obvious, right? Hopefully, you're looking at a dashboard or, you know, the person you take the pager when you're doing this migration so that you experience that pain and hopefully you also page on no data. It’s very important. And then service crashes, you should have an alert on that ideally. 

So all of these things, though, make it really clear that as soon as you do this All or Nothing thing, you know, if all of your services are rolling out within an hour with this new change and everything is looking good, that’s a very good sign and that gives a lot of confidence. This does though, you know, the clauses here, this requires a lot more thorough testing in a really good development environment. 

If you have a lot of environment drift, where your staging environment is entirely different than your public environment, this might be really challenging because this means you might have production bugs that you're just unable to catch in staging. And if the blast radius is all of your services, that can be really dangerous. 

Also, you know, you do have those compatibility problems I mentioned, so you have to be very deliberate about observing which dashboards and alerts break and fixing them proactively or even in advance if you know what the metrics are going to be. The example I have here is definitely a common one where you could imagine a metric type changes but a metric name doesn’t, which most vendors will just reject. 

### [00:12:10] Slow Tail migration method

Or the dashboard that you're looking at will look very strange. This also does mean that there's more upfront effort to migrate your services. It's going to take, you know, probably a group of people to help you monitor this, depending on how many services you have to migrate. If you're, you know, a company with maybe five to ten services, that's not too bad to do with one person, but if you're a company with hundreds of services, you're going to want a team of people to monitor this with you.

So on to the next one, we have the slow tail. This method gives your application developers the ability to migrate themselves by usually flipping an environment variable on and off for whichever method they want. The benefit here is that there's less upfront work. You can confirm that it works for one service and push it out for that service only, and you can do that in all of your environments. So if you have that, you know, if you don’t have that confidence that I mentioned earlier about your staging environment versus your production environment, this would be really helpful because you could actually just push a single service without the fear of, you know, all these other services rolling out to verify that your change has worked as expected. 

Finally, this also allows you to have more time to develop dashboards and alerts to handle those compatibility issues. The problem is that this can be very slow. If you have more than 50 services in, you know, at least two environments and it takes an hour to migrate a single service because you're trying to be extra careful, then you're looking at a multiple weeks or months-long process. 

If you leave a migration to app developers without a strong why, also, they'll never do it, so it's usually going to fall on your team to make that happen. Bugs also may not reveal themselves if your services are not uniform. Imagine you have something like a queue worker that works off of Kafka, whose topology looks entirely different than something like, you know, a classic API server. If you're only, you know, instrumenting your API servers and then you go to instrument one of those Kafka servers, if there's something that's significantly different in how you did your instrumentation, that might take some time to figure out what's going wrong. 

You know, ideally, OTEL has figured out a lot of this, but doing any of these migrations, you always need to check. You know, we just cannot and honestly should not know how you instrument your services. You know, you don’t want to have to explain your whole observability backend to us; that doesn't really make much sense. 

So ultimately, it's important that you do some work to check that the migration is working as you expected. One thing that I did just think of is that you could do a combination of this slow tail all-in-one where you do a migration for, you know, say some good sample of your services, and then once you're pretty confident with those, you could move to just enabling for everyone at once. That’s another good option. 

I’m trying to think about the drawbacks of that. Not sure there are any. I think that's probably the way to go unless you're really nervous about some of these compatibility problems or some of these like unknown unknowns; that might be the only time that that would be a little scary. 

The last one is the bridge. OpenTelemetry for some migrations actually provides a bridge where you can go from, you know, instrumentation A to instrumentation B without having to do any real code changes other than implementing the bridge. There are a few issues here. Well, already you can see the pros; I mean it’s pretty obvious, right? You don’t have to make many code changes. The problem is that you're going to have some worse performance in comparison to writing using the new method. 

It's going to be, you know, a fair, you know, there's a conversion cost to anything that you're doing in your application. If another option is you could just send from, you know, instrumentation A, and then convert it to instrumentation B with OTEL collector. So if you wanted to go from StatsD, the OTEL collector has a StatsD receiver and an OTEL exporter, so that's also fine. But both of these have the same drawback, which is that if you wanted to take advantage of some of the capabilities of the OTEL SDKs, you know, then this migration, you're not really doing a migration. 

It means that you can, you know, try and migrate stuff piece meal and like for dashboards and alerts, which is great, but it does have this drawback of like, you're not actually doing the migration; you're still, you know, just putting off the hard work later. And so this can also be confusing to your app developers, where someone says, "Well, my code says OpenTracing, but this trace says OTEL. Why can't I, you know, use X feature?" And that can be a little confusing. I think that's like not the end of the world though. It's, you know, as long as you're communicating well, that should be all right. 

But yeah, so this is really, I think using the collector is another really good path forward overall though. Doing the thing where you just send some traffic to the collector from one of your services, migrate your apps and dashboards, and then you have everything sent to the collector and then you can migrate your apps to OTEL with that all-in-one approach. 

And then your dashboards and alerts should just, you know, already be changed and you should be all set. Before I move to the next portion, do I have any questions? Any thoughts? I'm going to do a five count again.

**Speaker 2:** Jacob, I'm actually living through this right now. One thing that I'm running into quite a bit is like getting the various developer teams to shift their mindset from sort of our previous vendor that we were with into our newer one. And it's, you know, there's underlying things we're doing like the StatsD into like Prometheus style type thing with it, and yeah, that's led to a couple people who are like, "Oh, these things aren't apples to apples here anymore," and that's causing a bunch of stuff. Any tips and suggestions on like winning the hearts and minds here? 

**Jacob:** Yeah, so really like the thing to do is sell on features when you can. So if you could say, you know, this improves our—well, the easiest one is cost, right? If you're using StatsD, you're probably coming from Datadog, you're going to say, you know, "Our previous spend with Datadog was X thousands, maybe millions of dollars, and with Prometheus, it's like, you know, 100 a month," or something, right? That's the easiest one. 

But the more valuable one is, you know, you sell in the ecosystem where, for me, the real benefit of Prometheus is that the local development experience for metrics is actually much simpler. You don’t have to run, you know, a Datadog agent; you don’t have to do anything else; you can just hit your metrics endpoint and verify that your metrics are doing what you expect them to. And that’s a really good developer flow for testing that stuff. 

The other thing that is—I mean, people from Datadog; I used to work for Datadog, and their dashboard product is great. But Grafana, if you're using Grafana, also has a really strong dashboards product. Showing something like, you know, maybe you use Redis in your Kubernetes cluster, you install a Redis service monitor, and then you install the off-the-shelf Grafana dashboard for it, right? Like that’s a pretty great experience, and that’s all done at, you know, zero cost, which is pretty incredible. 

You know, Prometheus metric cost, which is very small in cents on the dollar. So the last thing you can do is like training. I have a storied history with training; I think that they never really achieve the thing that you want, which is for more people to be excited. Really what they do is can cause more confusion if you're not careful with like your language. 

I think maybe a strategy that I've always wanted to do is build some local tooling around the stuff to developers, whether it's, you know, writing maybe an end-to-end test or a little UI around their applications' metrics so they can do something with them locally to be like, "Oh yes, like this thing is working as expected." I mean, I think that the local dev for Prometheus is just pretty fantastic, and that’s probably what I would sell on. 

But again, you know, it's very company-specific in many ways, right? If people are really bought into the Datadog model of things, which is, you know, high cost, very low thinking, Prometheus and Grafana is not really that. It’s low cost but much more thinking, and ultimately, like you don't want your developers to have to think too much about their instrumentation. 

The thing that I tend to do is have a wrapper library before doing a migration, so that people are used to the same signatures; it’s just doing a different function under the hood. It also makes your migration easier. I didn't mention it here because most companies already have some type of wrapper because they have some needs that are like specific. It's usually like a thin wrapper, but doing as much as you can to not change their workflow and make it very simple is always going to be good. 

The thing with Prometheus you can do is check what metrics are available, and then you can auto-generate dashboards from those metrics. And you can add into your Helm charts, you know, these are the metrics I care about, and then you could just generate alerts automatically from that as well. So there's a lot of that quality of life stuff that again, like it's easy with the Datadog UI but is automatic with, you know, infrastructure as code. So that's like another trade-off I would say because not a lot of people, I don’t think, use Terraform at Datadog.

**Speaker 2:** Thanks for that, Jacob!

**Jacob:** Yeah, no problem! Any more questions on this part? 

### [00:22:45] Discussion on performance issues

**Speaker 1:** Cool, okay. So now I'll talk about what I did, what the team did for our migration from StatsD to OTEL for metrics. 

So, we began migrating using that slow tail path. The reasons were, I’d say, good motivating factors. As I mentioned earlier, the Metrics API and SDK weren’t declared stable at the time, which meant we would have had to deal with some signature changes, and that would have been potentially a lot of signatures. We did have a wrapper library, but still doing those types of changes can be pretty frustrating. 

We also wanted to use some new metrics features that StatsD didn’t support. This is like asynchronous instruments. The biggest one was exponential histograms, which OTEL sort of pioneered. And then the last one is that we would, you know, because we are the group that helps write these libraries, we wanted to understand the performance and quality of life features to make it as easy as possible and as quick a decision. You know, you don't have to worry about performance; you don't have to worry about all this other stuff. How do we make this simple? 

In migrating to OTEL for metrics, we did find a few performance issues. The first one was, you know, because we were using that wrapper library I mentioned, our implementation was working off of OpenTracing tags, which at the time was our tracing instrumentation, which we then would need to convert into OTEL tags anytime you wanted to make a metric. If you think about the amount of times that you call, you know, metric.record, that’s a lot of time, so that gets pretty expensive. 

And so while we waited for improved performance in the metric libraries, we just began our tracing migration because the OTEL team needed time to investigate some of the stuff that we brought to them. And it also meant that we could fix, you know, the very company-specific problem of this conversion. 

So then we began our tracing migration, going from OpenTracing and OpenCensus to OTEL. And so we went for this one with All or Nothing approach because OTEL for tracing had been stable for some time. We weren't really concerned about compatibility, and ultimately at the time there weren’t a lot of guides written, but the process was relatively straightforward and the compiler is really doing most of the work for you. The structure for it, we already supported OTEL traces in our product, so there actually, like, in theory, there were no real fun changes either for our own dashboards and alerts that we were going off of. 

So what I did for this was write the code for the migration and then push up, build, and push an image to our staging and production environments for a service that doesn't get much traffic. It does get some; it's important that it gets some. And then confirmed that each version in our product looked the same before and after. We have a view of sort of, you know, these red metrics per version, and it shows you the difference in those versions. And if any of those looked different, if the rate, for example, dropped off, then that’s a sign that we did something totally incorrect. 

Doing that for one service is good, but then the reason that the, you know, the All or Nothing approach is really useful is that you get that integration test. So when we migrated all of our services to this new method, it was really clear that service-to-service trace propagation wasn't working. You know, you just did a spot check of a few different traces, and it's really—that's like such an obvious thing. So that was a pretty easy fix. I actually had to just go into the OTEL community and implement something that had a TODO around it. 

Another issue that we had right before, you know, sort of in the last bit of this migration was that our sampler wasn't configured correctly. OTEL provides a lot of new features for sampling, and I was under-sampling in one environment and then over-sampling in another environment because I misconfigured it. So I had to roll that back really quickly, fix it, and then push it out a day or two later. 

But overall, this whole process took maybe a month of work to migrate, I think it's like a hundred or so services in all of our environments, which is, if you've ever led a migration, that’s a pretty good time. I was pretty happy with that one. It also meant that we could get back to our metrics migration because we had sort of achieved that goal with OpenTracing tags. 

We were able to use those, use the fact that we didn’t need to do this conversion to continue our metrics migration. It also—we came back to the OTEL team having shipped some real performance and quality of life improvements that really let us continue with this without the fear of performance problems. 

These changes also let us use this feature called metrics views, which let you create a new metric series to provide seamless compatibility with StatsD. StatsD emits their histogram as what's really called a Prometheus summary, but we wanted to emit exponential histograms, but that's the really the only change that was happening here. 

All of our other metrics were able to stay about the same, so we just needed to dual-write the old summary, which OTEL didn’t really support at the time and I think doesn't and shouldn't; it's a bad metric type. And we also wanted to dual-write the exponential histogram so that we could migrate all of our dashboards and alerts from this old approach to this new approach. 

And then finally, you know, as I said, we put this library behind a feature flag and then we could just flip that on and off whenever we wanted. And then we would roll it out pretty slowly over a two or three-month period to all of our environments. This also let us test the change really effectively as well. 

I think what's next? Let me go back. So any questions about this migration process? 

**Speaker 3:** Do a five count again.

**Jacob:** 

This is Rahul. I have one question around the OTLP protocol. So I'm guessing you must have used the OTLP receivers and exporters vastly during your metrics and trace migration. So what is the go-to protocol within OTLP? It supports gRPC and HTTP, but from a security and performance point of view, which is the go-to protocol for traces and metrics? 

**Jacob:** I'm not sure I follow. We actually just emitted OTLP directly from our SDKs to our SaaS, so we didn't go through a collector for this one. We could have gone through a collector, but we didn't want the operational overhead at the time. It was enough migrations to handle at once, but that didn't really answer your question. Can you restate your question? 

### [00:30:15] OpenTelemetry protocol discussion

**Speaker 3:** Yeah, I mean, I wanted to know what is the best protocol to use under OTLP? Is it gRPC or HTTP in terms of metrics and traces?

**Jacob:** Yeah, in terms of performance and security? Yeah, so I'm not the best with security recommendations. I would say we use gRPC for everything just because it's, I don't know, our sort of de facto internal standard. HTTP is more accepted for some, like depending on your company security requirements. I'm not sure of the real differences, like to compare and contrast them. I don't think I'd be the right person to speak to those. Maybe some of the other OTEL folks have more of an opinion or more information on that though. 

**Speaker 3:** Okay, cool. Were there any learnings around managing access tokens? And did you use multiple access tokens? Does it portal you know, if you're sending a lot of metrics or traces or a single access token? 

**Jacob:** Yeah, we just used the same access token approach that we did. I don't know if I can speak to that just because it's, you know, internal security stuff. 

**Speaker 3:** Yeah, no worries. 

**Jacob:** I’d say that, you know, the best thing you can do for security in like a cloud-native environment is use some sort of secrets provider. The Kubernetes external secrets operator is great. You can hook it up to something like GCP's KMS and decrypt your secrets to then load access tokens from, though. You can do the same thing with AWS or Vault or any of these other providers as well. If you're particularly like security inclined, it's also important to use things like mTLS as well. Something like Istio can help you with some of that. The OTEL operator actually provides some mTLS features in OpenShift. So, you know, there are a lot of security features out there to be used. Hopefully, that seeds some interesting investigation for you. 

**Speaker 3:** Yep, yep, thanks! 

**Jacob:** No problem! 

**Speaker 1:** So moving on, you might be wondering what's next? Are we going to do a logs migration? And right now we're under a different migration, which is changing our infrastructure metrics to use the OTEL first receivers, like the K8s cluster receiver, the C++ receiver, and so forth. Because we really want to start using some of these things the community is writing. 

After that, we should be able to begin migrating to use the new OTEL logging case, which should be looking pretty good in a few—they're looking good now, but I’m not sure what the state of it is for Go just yet, given that Go just released like a new standard logging library. 

Any more questions? 

**Speaker 4:** Hi, this is Jay speaking. I have one general question regarding OpenTelemetry. So I’m pretty new to this, and the reason why I was investigating into OpenTelemetry was for its ability to be backend agnostic for generating metric data. The question is, however, I was interested in exploring pull-based metrics exporters, but so far, I don’t know if I’m right or wrong, but the Prometheus exporter within the OpenTelemetry SDK is the only one that is supporting the pull-based metrics approach. Is that correct? 

**Jacob:** So, it sounds like you're going—so there are a few different types of exporters within OTEL. So there's an SDK exporter, which, yeah, there's a Prometheus exporter. I think there might even be a Datadog exporter. But one might be a better fit is to use the OTEL collector and use their exporters, which are numerous, and pretty much every backend has some type of exporter. 

So I think I would try and say you should—if you're using OTEL SDKs, you should export to OTEL collector and then export to, you know, your protocol of choice. 

**Speaker 4:** Yeah, but when using this OTEL exporter, this would rather be a push-based mechanism, right, to send data to the OTEL collector? 

**Jacob:** Yeah, that's correct. 

**Speaker 4:** Okay. 

**Jacob:** You can also, if your instrumentation is in Prometheus right now, you can have the OTEL collector scrape the Prometheus metrics and then export them as OTEL or export them as StatsD or, you know, whatever you want. 

**Speaker 4:** Good, thank you! 

**Speaker 1:** Any more questions? 

**Jacob:** Do you use any solution to manage or, you know, update the YAML file of the fleet of agent fleet on the go? And is there any built-in solution that you guys are using to manage the collectors? 

**Jacob:** Yeah, so I am a maintainer for the OTEL operator, and internally and externally, I recommend using the OTEL operator with the OTEL collector CRDs. They're pretty easy to use, easy to set up, easy to manage, and we're always developing and thinking about new features as well. And so that'll be the place to get those now and in the future, and I’d continue recommending that. 

**Speaker 4:** Okay, thanks! 

**Jacob:** Could I ask a question following on from the last question? Would you recommend using something like OpAmp in terms of managing your fleet of collectors, or would you say it's preferable to do it in an operator basis, you know, kind the CRD pattern? 

**Jacob:** Yeah, so if you're in Kubernetes, OpAmp—by the way, is still pretty early Alpha right now. Well, the protocol itself is stable, but the actual implementations are in Alpha. So I'm going to answer this question with the assumption that the implementations are done, if that's all right? 

**Speaker 4:** Sure! 

**Jacob:** So if you're in Kubernetes, I would recommend using the OpAmp bridge that we're developing. The bridge is a component that can connect to your vendor and will be able to manage pools of collectors rather than just having an extension that works on or a supervisor and an extension that works on a single collector pod. 

The reason for that is usually you are running a collector, and what you—you’re running a collector in a pool, not as a monolith. And so whereas OpAmp would be useful for, you know, a VM or just running it as a binary, doing it in Kubernetes, if you're running it as a pool, it's not a great pattern in Kubernetes to have a supervisor update a single pod's configuration to make it not uniform with the other pods in its replica set. 

So what that means is, you know, if you're going to run it in Kubernetes, if you're going to run a replica set of pods in Kubernetes, you want those pods to be the same configuration. And if you're only running a supervisor on each of those pods, but you're only making the change to a single one, that's an anti-pattern and can get you into some trouble. 

The bridge, however, can manage pools of collectors and is definitely what I would recommend to use again with this stuff being completed. That's what I would recommend for Kubernetes. 

**Speaker 4:** Okay, thank you! 

**Jacob:** No problem! 

### [00:40:20] Managing collectors in production

**Speaker 5:** I’ve got a few questions related to the work. So I work at a large U.S. bank, and I'm trying to bring in OTEL and essentially have that as our main strategy to try and move away from vendor-specific solutions. The question that we’re running into just now are the challenges of vanilla versus vendor. And what I mean by that is, do we just pull down, if I take the collector for example, do we just pull down the collector, configure the collector the way that we want it with receivers and exporters, or with the specific strategic vendors that we work with? 

Do we look to bring their vendor-wrapped collector and deploy that within our enterprise? There are obviously pros and cons in doing both. It doesn’t sound like, you know, from your side, Jacob, obviously you’re working from the vendor side. But I have spoken to other vendors who have given me an interesting range of opinions on what area or which of those to look at. It’d be quite interesting to see what your thoughts are.

**Jacob:** This is a great, really great question. It totally depends on your deployment model, I would say. So one option, especially, you know, if you're running thousands of pods and, you know, hundreds of clusters, the model that I would recommend you use is the gateway model, where, you know, let’s say you have a collector per group of apps, and then all of those collectors forward to a centralized pool of collectors that then forward to your vendor. 

This means that those pools for your applications are vendor-neutral because all they're really doing is gathering and forwarding stuff for your application teams. And then a centralized team would manage the gateway collectors. And then that's the place where you make the decision about, am I going to use my vendor-wrapped collector or am I going to use my, you know, vendor-neutral one? 

The choice becomes really easy to, you know, if there’s some feature that your vendor provides that’s only available in their vendor-specific collector, you could choose to use the wrapped collector there, and then in the future, if you wanted to change vendors, you still have OTEL data sending to that vendor collector, and so you can just change that one out very, very easily. 

The reason that this is good is because you wouldn’t have to go to your application teams or any of these other, you know, orgs and say, “Hey, you know, you have to reconfigure your whole setup because we’re changing our underlying vendor here,” whereas you, as the centralized team, could be the one to just change a single pool to make it all consistent. 

That’s probably what I—that’s like a pretty future-proofed approach. The configuration for what you’re going to do, no matter what, is going to be complicated, but that one is probably going to do you best if you really want to use the vendor collector. If not, still doing the gateway approach is a pretty good one. You can centralize things like officiation sampling rates or even like, you know, requirements for telemetry, like attribute requirements can be centralized before they can egress. 

It also means you can have, you know, set points of egress as well, which, you know, if you're in a pretty locked-down Kubernetes cluster, as you know, and is really important to have, you know, only a certain amount of applications that can egress from the cluster. 

**Speaker 5:** Okay, I think I definitely follow in terms of the deployment patterns and layouts and having the multiple levels of collectors. I think for me, being in the enterprise, what we are worried about is, again, moving stuff like this to production. So if we have a theoretical issue in a vanilla collector, you know, again, just a theoretical example, if we've got RTO and we've got to fix issues within a one or two hours, for example, that's probably going to be the tipping point for us on the vendor versus vanilla question. 

And because we would obviously be looking for some kind of support from a vendor, and typically we would have that, as most folks would have through a vendor. But then that certainly, in my mind, gives us another problem where instead of going from agent-proliferation where you are just now, it's almost like going to OTEL proliferation. You know, it's almost like you're solving one problem and creating another. 

So I think, like the others on the call, you know, we're relatively early in our journey, and we're just trying to go through the not necessarily the technical questions but the hardening questions. What would reality look like when we're in production with, you know, very high volumes of traffic coming through? 

**Jacob:** Yeah, I mean, that sounds like you're on the right path here overall as far as you're thinking going. 

There definitely is that worry of like collector proliferation. You can avoid that with, you know, multiple pools to gateways if you'd like. If you're going from—I mean, usually you're doing this if you're going for like a legacy protocol, something like StatsD, which doesn't like to go over the Internet because it's UDP or something like Prometheus where you have all of these targets and you don't want to worry about managing the scrapes for them, right? 

Doing this at scale is going to be really environment and volume dependent. I think if your vendor provides their own collector, they should be able to give you some support for, you know, the vanilla collectors that you run. I, you know, with the people that I work with, do give support for, you know, whatever collectors their customers run. 

And I mean, we don’t have a vendor-specific collector like our company just doesn’t give out a vendor-specific one. But I provide support for any collectors that are customers run. So if that's the fear, I would check with your vendor to see if they also will give you that type of support. 

I also found that like the steady state of these things is pretty—once you tune it with resource and auto-scaling, it's pretty hands-off, I found. I actually was just working yesterday in a cluster that I touch every six months or so to do some like Helm chart testing, and it's been running for six months without issue and with like a huge varying scale of traffic because of auto-scaling and sort of just because of how simple I keep those collectors. 

This is for both metrics and traces, too, for infrastructure and application. So, I mean, it's a much smaller example than what you're talking about for sure. But the point remains where it's like once you reach a good steady state, especially with like your balance cycle, if that's what you have, if you're auto-scaling the setup correctly and if your configuration is pretty, you know, nailed down, you should be—it should be pretty hands-off, knock on wood. But that’s definitely the hope. 

**Speaker 1:** Thank you! 

**Jacob:** Yeah, no problem! 

**Speaker 1:** So folks, we are coming up on time, so we've got about five more minutes in case anyone has any more burning questions for Jacob. 

**Speaker 6:** All righty, I will take that as a no. But thank you, Jacob, so much for joining today and sharing your migration story. I think this has resonated with a lot of folks, so we definitely appreciate you coming on and sharing this experience with everyone. 

### [00:48:35] Future plans for log migration

Like I said, this recording will be made available on the OTEL YouTube channel. Also, for anyone who has missed Jacob's OTEL Q&A session that we had last month, there is a video up on the OTEL channel, and Reyes, who works with Ren and me on the OTEL end-user working group, did a wonderful write-up of the Q&A in case video isn’t your jam. So definitely be sure to check that out. 

Okay, so if you go to this link here, you should be able to find our various Slack channels. We love—we encourage everyone to just ask questions, share use cases, we love hearing all that stuff. And also, if you or anyone you know has a really cool OTEL use case, you're just getting started, or you're a more advanced user, does not matter, we would love to hear from you. 

We're always looking for folks for OTEL in Practice, OTEL Q&A, and we also have monthly OTEL end-user discussions, which we run those for three different time zones. So we have them for EMEA, APAC, and Americas. So be sure to join any one of those because there are always really amazing and thoughtful discussions coming out of these. 

So, yeah, everyone, thank you very much! And once again, Jacob, thank you so much for taking the time to chat with us twice! 

**Jacob:** Thanks so much for having me! I appreciate it! 

**Speaker 1:** Thank you, bye! Have a good rest of your day!

## Raw YouTube Transcript

welcome everyone to otel in practice um really stoked for such a good turnout I think this is one of our best turnouts yay um and for anyone who uh could make it today there will be a recording so if you have friends who are like damn it I missed this you can let them know there that we will be posting the the recording um and prettying it up once it's available so uh Jacob presented um I want to say last month for otel Q&A we worked together at lightstep from service now um and uh I'll let's Jacob take it away yeah thank you um so yeah my name is Jacob arof I am a staff software engineer um I work on our Telemetry pipeline team uh our team is sort of in charge of our internal and external open source Telemetry efforts whether it's like OTL sdks and go or The Collector or the operator um sort of anything that you would be using to collect and send data to your vendor is what we do so today I'm going to be talking about a migration that we LED last year and a little bit year and a half ago uh for when we migrated from uh stasy to open Telemetry and also from open tracing to open Telemetry going to talk about the you know various migration paths that are out there some of the things that went well and went wrong or our migration uh but I also would like this to be you know feel free to ask as many questions as you have um my slides are pretty light so I'm really interested to here you know where people struggle currently uh what type of challenges you're running into what features you're interested in um so please you know raise a hand or you know comment in the chat uh and I'd be happy to answer any questions and go down any interesting avenues that you might have so with that I will share my screen uh can we all see my slides yes great thank you very much so yeah today I'll be talking about that oel migration story so first let's talk about a problem uh I've been in the industry for you know many years many years um and I've had to lead a few of these migrations where your handrolled metrics or tracing or logging Library just isn't cutting it anymore something where it's not performant enough maybe the maintenance is really expensive maybe um there's a feature that everybody wants that you just can't Implement um you know whatever it might be you want to do a migration um it can be very challenging and daunting depending on the amount of services that your company runs depending on what the organization's uh you know willingness to like undergo this migration might be uh so all of these are problems that I've faced um in various jobs not just where I am now but you know a few past roles as well uh it can be really hard to make this happen especially when you know you as maybe an SRE or a devops person or maybe just a you know engineer just want to make this work uh when you feel very passionate about making it happen and you just can't seem to get the buying that you need um so that is sort of the theme for today um and there is a solution right it's like migrating to this new thing um otels metric API I'm going be talking about metrics traces but you know this is really true about a lot of stuff um for otel the metric API the traces API and even the logging API is accepted by most major vendors out of the box now um the API is support for you know all these instrument types um in all of your favorite languages probably they're very few that don't um and the performance and compatibility is always top of mind for us um throughout the stack of otel um components everybody is always thinking about performance um so you know with all this in mind you're like yeah this sounds great uh I want to start using this um but there's a problem right like using these new tools migrating these new tools is hard how do you break up the work how do you know all of your Telemetry is working the same as before uh what are the risks of these different migration models what even are the different migration models um and probably the most important one here is how do you convinced leadership that this is worth doing um you can try and you know brute force your way into doing a migration but that to me is a recipe for Burnout um doing a migration in general where you know there's a new architecture that you think is really going to improve uh quality of life for your team for your company uh without getting organizational buyin is how you spend months in a project that may never see the light of day and that is really demoralizing and very difficult to work around so the first thing that you have to do when you want to migrate to a new tool is make the case um it's you know not even um you know show a proof of concept it's just can you convince people that this is worth doing sometimes a proof concept helps but that's not you know the key to success the key to success is really getting a group of people who agree with you that you're able to sell one-on-one who can help you really make it clear to the stakeholders uh you know in your leadership that this is worth doing um for me was pretty straightforward we're you know a main open tary contributor uh the oot metrics API and SDK wanted to go uh into their you know 1.0 stable um and they really wanted some quick feedback from someone who has a lot of traffic uh to test it out and see where the edges are so so in that case it was very easy to sell the migration in past jobs the selling point is usually you know you look at maintenance cost uh and performance overall um I worked with a handrolled uh solution uh in few jobs ago and when it came time to do a migration the thing that really sold people was just counting the amount of tickets and hours that we have to spend on you know new features and maintenance on our internal Library um as well as the amount of times where you know we've had an incident because something related to our instrumentation is just incorrect which you know does happen a lot with your own handrolled stuff so that is maybe a good basis to go off of uh does anyone have any questions sort of before I go into more specifics about this section and uh I'm going to do like a five count something to learned from a teacher of mine uh I'm just going to we're going to do 5 Seconds of Silence um until someone raises their hand or I'll just keep going so no cool oh is there something in the chat oh that's just me moving it a little cool no worries uh thank you um so let's talk about the migration paths um the first one is is what I call the All or Nothing um this method has you entirely rip out your existing instrumentation in favor of otel this would be you know a lot of people talk about you know replacing you know the engine of a of a running plane or running car that a lot of people will say that um this would be like you know just getting a new plane and have every and you have everyone hop to the new one while it's flying so it's difficult but you know maybe it has its benefits one of the pros is that you know once you've pushed your code and you've confirmed things are working and you have a good enough cicd system your work is really done right it just rolls out and everybody's happy this reduces the time for split brain split brain is what happens when you're on two systems that may not be compatible together you could imagine you know if you're on stats D or if you're on Prometheus even um if most of your metrics come from Prometheus they don't have periods in them they have you know pretty Str requirements about um their shape overall um you can't do things like um up down counters like that's not a type that they have um they used to not have a proper histogram they now do uh they now have an exponential histogram support which is experimental but you know there are things that otel has that um Prometheus or statsd just does not have and doesn't have the ability to do um so being in a split brain where some Services have it and some Services don't and they're emitting different metrics and different shapes different variables um that can get pretty unwieldy pretty fast and so if you're not very deliberate about planning how you're going to migrate your dashboards and alerts then you're going to be stuck with both of them for some time which if you're on call and if you've been on call during a migration for this type of stuff that gets really painful if you get paged at 3:00 a. you have to wake up and go oh you know which uh which dashboard should I check for this is the service on otel or is the service on statsd um that's really frustrating uh that that's the type of thing that you don't want to have an on call engineer uh think about so the other benefit of doing this All or Nothing approach is that the issues are incredibly visible if a dashboard breaks if an alert Pages or a service crashes um it's pretty obvious right hopefully you're looking at a dashboard or you know the person you take the pager when you're doing this might ation so that you experience that pain and hopefully you also page on no data it's very important um and then service crashes you should have an alert on that ideally um so all of these things though make it really clear that as soon as you do this All or Nothing thing um you know if all of your services are rolling out within an hour with this new change and everything is looking good that's a very good sign and that that gives a lot of confidence um this does though you know the Clauses here this requires a lot more thorough testing in a really good development environment if you're unable if you have a lot of uh environment drift where your staging environment is entirely different than your public environment this might be really challenging because this means you might have uh production bugs that you're just unable to catch in staging um and if you do if the blast radius is all of your services that can be really dangerous um also you know you do have those compatibility problems I mentioned and so you have to be very deliberate about um observing which dashboards and alerts break and fixing them proactively um or even in advance if you know what the metrics are going to be um the example I have here is is definitely a common one where you could imagine a metric type changes but a metric name doesn't which most vendors will just reject um or the dashboard that you're looking at will look very strange um this also does mean that there's more upfront effort to migrate your services it's going to take you know probably a group of people to help you monitor this depending on how many services you have to Mig if you're you know a company with maybe five to 10 Services that's not too bad to do with one person but if you're a company with hundreds of services you're going to have you're going to want a team of people to uh monitor this with you so on to the next one we have the slow tail um this method gives your application developers the ability to migrate themselves by usually flipping an environment variable on and off for whichever method they want um benefit here is that there's less upfront work uh you can confirm that it works for one service and push it out for that service only and you can do that in all of your environments so if you have that um you know if you don't have that confidence that I mentioned earlier about your saging environment versus your production environment um this would be really helpful because you could actually just push a single service without the fear of you know all these other services rolling out to verify that your change is worked as expected and finally this also allows you to have more time to develop dashboards and alerts to handle those compatibility issues problem is is that this can be very slow um if you have more than 50 services in you know at least two environments and it takes an hour to migrate a single service because you're trying to be extra careful then you're looking at a multiple weeks or months long process um if you leave a migration to app developers without a strong why also they'll never do it so it's usually going to fall on your team to make that happen um bugs Also may not reveal themselves if your services are not uniform imagine you have something like a q worker that works off of kofka uh which whose topology looks entirely different than something like you know a classic API server um if you're only you know instrumenting your API servers and then you go to instrument one of those kofka servers if there's something that's significantly different in how you did your instrumentation that might take some time to figure out uh what's going wrong um you know ideally otel um has figured out a lot of this but doing any of these migrations you always need to check um you know we just cannot and honestly should not know how you instrument your services um you know you don't want to have to explain your whole um observability backend to us that doesn't really make much sense so ultimately it's important that you do some work to check that the migration is working as you expected uh one thing that I did just think of is that you could do a combination of this slow tail all-in-one where you do a migration for you know say some good sample of your services and then once you're pretty confident with those you could move to just enabling for everyone at once um that's another good option uh it I'm trying to think about the drawbacks of that not sure there are any I think that's probably the way to go unless you're really nervous about um some of these compatibility problems or some of these like unknown unknowns that might be the only time that that would be a little uh scary so the last one is the brdge um open Telemetry for some migrations actually provides a bridge where you can go from you know instrumentation a to instrumentation B um without having to do any real code changes other than implementing the bridge there are a few issues here um well already you can see the pros I mean it's pretty obvious right you don't have to make many code changes problem is is that you're going to have some worse performance in comparison to writing using the new method um it's going to be you know a fair you know there's a conversion cost to anything that you're doing in your application um if another option is you could just send from you know instrumentation a and then convert it to instumentation B with otel collector so if you wanted to go from stats d The otel Collector has a stats D receiver and an otel exporter so that's also fine but both of these have the same drawback which is that if you wanted to take advantage of some of the capabilities of the otel sdks you know then this Migra you're not really doing a migration um it means that you can you know try and migrate stuff um Peace meal and like for dashboards and alerts which is great but it does have this drawback of like you're not actually doing the migration you're still you know just putting off the hard work later um and so this can also be confusing to your app developers where someone says well my code says open tracing but this trace says Hotel why can't I you know use x feature um and that that can be a little confusing I think that's like not the end of the world though it's it's you know as long as you're communicating well that should be all right um but yeah so this is really I think using the collector is another really good path forward overall though um doing the thing where you just send some traffic to The Collector from one of your services uh you migrate your apps and dashboards and then you have everything sent to the collector and then you can migrate your apps to otel with that allinone approach um and then your dashboards and alerts should just you know already be changed and you should be all set before I move to the next portion do I have any questions any thoughts I'm going do a five C one question Jacob I'm actually living through this right now um one thing that I'm running into quite a bit is like getting the various developer teams to shift their mindset from sort of our previous vendor that we were with into our newer one and it's you know there's underlying things we're doing like the statsd into like premier Prometheus style type thing with it and yeah that's led to a couple people who are like oh these things aren't apples to apples here anymore and that's causing a bunch of stuff um any tips and suggestions on like winning the hearts and Minds here yeah so really like the thing to do is sell on features when you can so if you could say you know this improves our well the easiest one is cost right if you're using staty you're probably coming from data dog you're going to say you know our previous spend with data dog was X thousands maybe millions of dollars and with Prometheus it's like you know 100 100 a month or something right that's the easiest one um but the more valuable one is you know you sell in the ecosystem where um for me the real benefit of Prometheus is that um the local development experience for metrics is actually much simpler um you don't have to run you know a data dog agent you don't have to do anything else you can just hit your metrics endpoint and verify that your metrics are doing what you expect them to um and that that's a really good developer flow um for testing that stuff um the other thing that is I I mean people from data dog I used to work for data dog um and they their dashboard product is great um but grafana if you're using grafana also has a really strong dashboards product showing something like you know maybe you use redus in your Cube cluster um you install a redus uh service Monitor and then you install the off-the-shelf grafana dashboard for it right like that's a pretty great experience and that's all done at you know zero which is pretty incredible you know you know Prometheus metric cost which is very small in cents on the dollar um so the last thing you can do is like trainings I I have a storied history with trainings I think that they never really achieve the thing that you want um which is for more people to be excited um really what they do is can they can cause more confusion if you're not careful uh with like your language um I think maybe a strategy that I've always wanted to do is like build some local tooling around the stuff to developers um whether it's you know writing maybe an endtoend test or a little UI around their applications uh metrics so they can do something with them locally to be like oh yes like this thing is working as expected um I mean I think that the local Dev for Prometheus is just pretty fantastic um and that's probably what I would sell on um but again you know it's so it's very company specific and in many ways right if people are really bought into the datadog model of things which is you know high cost very low thinking um Prometheus and grafana is not really that it's it's low cost but much more thinking and ultimately like you don't want your developers to have to think too much about their instrumentation um the thing that uh I tend to do is have a uh rapper library before doing a migration um so that people are used to the people are using the same signatures it's just doing a different function under the hood it also makes your migration easier um I didn't mention it here because most companies already have some type of wrapper because they have some needs that are like specific um it's usually like a thin wrapper but doing as much as you can to not change their workflow and make it very simple is always going to be good um the thing with Prometheus you can do is check what metrics are available and then you can autogenerate dashboards from those metrics and uh you can add into your um Helm charts you know these are the metrics I care about and then you could just generate alerts automatically from that as well um so there's a lot of that quality of life stuff that again like it's easy with the data dog UI but is automatic with um you know infrastructure as code so that's like another trade-off I would say because not a lot of people I don't think use terraform at data dog um so I don't know maybe they do and I just don't know that oh thanks for that check it yeah problem uh any more questions on this part cool okay so now I'll talk about what I did what the team did um to forour migration from statsd to otel or metrics obviously um so so we began migrating using that slow tail path um the reasons were uh I'd say good motivating factors as I mentioned earlier the metrix API andd weren't declared stable at the time which meant we would have had to deal with some signature changes um and that would have been potentially a lot of signatures uh we did have a wrapper library but still doing those types of changes can be pretty frustrating um we also wanted to use some new metrics features that statsd didn't support this is like asynchronous instruments um the biggest one was exponential histograms which otel sort of pioneered um and then the last one is that we would you know because we are the group that helps write these libraries uh we wanted to understand the performance and quality of life features to make it as easy as possible and as you know um as quick a decision it's just you know you don't have to worry about performance you don't have to worry about all this other stuff how do we make this simple um so in migrating to otel for metrics we did find a few performance issues um the first one was you know because we were using that rapper Library I mentioned um our implementation was working off of Open tracing tags which at the time was our tracing instrumentation which we then would need to convert into otel tags anytime you wanted to make a metric which if you think about the amount of times that you call you know metric. record um that's a lot of time so that gets pretty expensive um and so while we waited for improved performance in the metric libraries uh we just began our tracing migration because the otel team needed time to investigate some of the stuff that we brought to them um and it also meant that we could fix you know the very um our company specific problem of this uh conversion and so then we began our tracing migration um going from open tracing an open sensus to otel and so we went for this one with All or Nothing approach because otel for tracing had been stable for some time weren't really concerned about compatibility um and ultimately at the time there weren't a lot of guides written um but the process was relatively straightforward and the compiler is really doing most of the work for you um the structure for it we already supported oel traces in our product so there's actually like no in theory there were no real fun changes either uh for our own dashboards and alerts that we were going off of so what I did for this was write some write the code for the migration um and then push up build and push an image to our staging and production environments for a service that doesn't get much traffic does get some it's important that it gets some um and then it confirmed that each version in our product looked the same before and after we have a view uh of sort of you know these red metrics per version and it it shows you the difference in those versions and if any of those looked different if the rate for example uh dropped off um then that's a sign that we did something totally incorrect um doing that for one surface is good but then the reason that the you know the All or Nothing approach is really useful is that you get that integration test so when we when I migrated all of our services to this new method it was really clear that service to service Trace propagation wasn't working you know you just did a spot check of a few different um traces and it's really that's like such an obvious thing um so that was a pretty easy fix I actually had to just go into the oel community um and Implement something that uh had a Todo around it um another issue that we had right before you know sort of in the the last bit of this migration was that our sampler wasn't config configured correctly um otel provides a lot of new features for sampling and I was under sampling in one environment and then oversampling in another environment uh because I misconfigured it so I had to roll that back really quickly fix it and then push it out a day or two later but overall this whole process took maybe a month of work um to migrate I think it's like a hundred or so services in all of our environments um which is if you've ever led a migration that that's a pretty good time I was pretty happy with that one it also meant that we could get back to our metrics migration uh because we had sort of achieved that goal with open tracing tags um we were able to use those use the fact that we didn't need do this conversion to continue our metrix migration it also we came back to the otel team having shipped some real performance and quality of life improvements um that really let us continue with this without the fear of performance problems um these changes also let us use this feature called metrics views uh which let you create a new metric series to provide seamless compatibility with statsd statsd emits their histogram is what's really what's called a Prometheus summary um but we wanted to Emmit exponential histograms but that's the really the only change that was happening here um all of our other metrics were were able to say about the same so uh we just needed to dual write the old summary which otel didn't really support at the time and I think doesn't and shouldn't it's a bad metric type um and we also wanted to dualite the exponential histogram so that we could migrate all of our dashboards and alerts from this old approach to this new approach um and then finally you know as I said we put this Library behind a fature flag and then we could just flip that on and off whenever we wanted um and then we would roll it out pretty slowly over a two or three month period um to all of our environments this also let us like test the change really effectively as well um I think what's next let me go back so any questions about this migration process do five count again Jacob this is Rahul um hi I have one question around the OTP protocol so I'm guessing you must have used the OTP receivers and exporters vastly during your metrics and Trace migration so what is the go-to protocol within OTP it supports stdp and grpc but from security and performance point of view which is the go-to protocol for traces and metrics uh I'm not sure I follow we we actually just um emitted OTP directly from our sdks to our SAS so we didn't go through a collector for this one um we could have gone through a collector but uh we didn't want the operational overhead at the time it was enough migrations to to handle it once but that didn't really answer your question can you rest your question yeah I mean I wanted to know um what is the best protocol to use under OTP is it stdp or grpc um in terms of metrics and traces yeah in terms of performance and security yeah so I'm not the best with for security recommendations um I would say we use grpc or everything um just because it's I don't know our sort of deao internal standard um HTTP is more accepted for some like depending on your company security requirements um I'm not sure of the real differences like the like to compare and contrast them I don't think I'd be um the right person to speak to those maybe some of the other oel folks have more of an opinion or more information on that though yeah don't okay cool were there any learnings around managing access tokens um and did you use multiple access tokens uh does it portal you know um if you're sending a lot of metrics or traces or a single access token yeah we uh we just used um the same access to token approach that we did um where I I don't know if I can speak to that just because it's it's you know internal security stuff um yeah sorry about that yeah no way I'd say that you know the best thing you can do for security in like a cloud mative environment um is use some sort of Secrets provider um the kubernetes external Secrets operator um is great uh you can hook it up to something like gcps uh KMS um and um decrypts inversion your secrets um to then load access tokens from though um you can do the same thing with AWS or vault or any of these other providers as well um if you're particularly like security inclined it's also important to use things like mtls as well um something like ISO can help you with some of that um the otel operator actually provides some mtls features in open shift um so you know there there are a lot of security features out there to be used um hopefully that that seeds some interesting um investigation for you yep yep thanks no problem um so moving on uh you might be wondering what's next are we going to do a logs migration um and right now we're under a different migration which is changing our infrastructure metrics to use the otel first receivers like the CP cluster receiver the CET receiver um and so forth um because we really want to start using some of these things the community is writing um after that we should be able to begin migrating to use the new oo loging case which um should be looking pretty good in a few they're looking good now but um I'm not sure what the state of it is uh for go just yet given that go just released like a new um standard logging Library um yeah any more questions hi this is uh Jay speaking um I have one general question um uh regarding open telary so um I I'm pretty new to this new to open tet Tre and uh the the reason why I was investigating into open Telemetry was uh for its ability to be backend agnostic for generating uh metric uh data uh the question is however uh I was interested in exploring po based uh metrics uh uh exporters uh but uh uh so far uh I don't if I'm right or wrong but the promethus export exporter within the open Telemetry SDK is the only one that is supporting uh the pull based metrics approach is that correct so um it sounds like you're going so there are a few different types of exporters within um uh within Hotel so there's an SDK exporter um which yeah there's a prus exporter I think there might even be a D exporter um but one what might be a better fit is to use the otel collector and use their exporters which are numerous and and pretty much every backend um has some type of exporter um so I think I would try and and say you should if you're GNA use otel sdks you should export an OTP to an Noel collector and then export to you know your protocol of choice um yeah but when using this uhel exporter this would rather be a push based mechanism right uh to send data to The oel Collector yeah that's correct okay okay uh you can also if your instrumentation is in Prometheus right now though um you can have the otel collector scrape the Prometheus metrics and then export them as OTP or export them as statd or you know whatever you want good thank you any more question do you use any solution to manage or you know update the AML file um of the fleet of agent Fleet um on the go and is there any built-in solution that you guys are using to manage the collectors yeah so um the I am a maintainer for the oel operator um and internally and externally I recommend using the oel operator with the otel collector crds um they're pretty uh easy to use easy to set up easy to manage um and we're always developing and thinking about new features as well um and so that that'll be the place to get those uh now and in the future and I'd continue recommending that okay thanks Jacob could ask a question following on from the last question um would you recommend using something like opamp in terms of managing your Fleet of collectors or would you say it's preferable to do it in an operator basis you know kind the C pattern yeah so if you're in kubernetes um OPM by the way is still pretty early Alpha right now well the protocol itself stable but the actual implementations are in Alpha um so I'm going to answer this question with the assumption that the implementations are done if that's all right sure yeah um so if you're in kubernetes I would recommend using the opamp bridge that we're developing um the bridge is a component that can connect to your vendor um and will be able to manage uh pools of collectors rather than just having an extension that works on or a supervisor and an extension that works on a single collector pod um the reason for that is usually you are running a collector um and what you you're running a collector in a pool not as a monolith and so whereas opamp would be opamp on a using a supervisor would be very useful for um you know a a VM or just running it as a binary um doing it in kubernetes if you're running it as a pool it's not a great pattern in kubernetes to have a supervisor update a single pods configuration to make it not uniform with the other pods in its replica set so what that means is you know if you're going to run it in kubernetes if you're going to run a replica set of pods in kubernetes you want those pods to be the same configuration and if you're only running a supervisor um if you're running a supervisor on each of those pods but you're only making the change to a single one um that's an anti-pattern and can get you into some trouble um so the bridge however can manage pools of col ctors um and is definitely what I would recommend to use again with this stuff being completed that's what I would recommend for kubernetes okay thank you no problem I've got a few questions um related to the work so I work at a large US Bank and I'm trying to bring in hotel um and essentially have that as our main strategy um to try and move away from B on vendors specifically um the question that we running into just now are the challenges like vanilla versus vendor and what I mean by that is um do we just pull down if I take the collector for example do we just pull down the collector configure The Collector the way that we want it with receivers and exporters um or with the specific the Strategic vendors that we work with uh do we look to bring their vendor wrapped collector and deploy that within our Enterprise there's obviously pros and cons in doing both it doesn't sound like you know from your side Jacob obviously you're working from the vendor side but I have spoken to other vendors who have given me an interesting range of opinions on what what area or which of those to look at um it'd be quite interesting to see what your thoughts are yeah this this is a great uh really great question um it totally depends on your deployment model I would say so one option especially you know if you're running thousands of PODS and you know hundreds of clusters um the model that I would recommend you use is the Gateway model where you know let's say you have a collector per group of apps and then all of those collectors forward to a centralized pool of collectors that then forward to your vendor um this means that your uh those pools for your applications are vendor neutral um because all they're really doing is gathering in forwarding stuff um for your application teams and then a centralized team would manage the Gateway uh the Gateway collectors and then that's the place where you make the decision about am I going to use my vendor wrapped collector or am I going to use my uh you know vendor neutral one the Choice becomes really easy to you know if for some there's some feature that your vendor provides um that's only available in their WS collector um you could choose to use the rapid collector there and then in the future if you wanted to change vendors you still have OTL data sending to that vender collector and so you can just change that one out very very easy the reason that this is good is because you wouldn't have to go to your application teams um or any of these other you know Orcs and say hey you know you have to reconfigure your whole um setup because we're we're changing our underlying vendor here whereas you as the centralized team could be the one to just change a single pool to make it all um consistent um that's probably the what I that that's like a pretty uh future proofed approach um the conf you know the configuration for what what you're going to do no matter what is going to be complicated um but that one is probably going to do you best if you really want to use the vendor collector if not still doing the Gateway approach is a pretty good one um you can centralize things like officiation sampling rates um or even like you know requirements for Telemetry things like attribute requirements um can be centralized before they can egress um it also means you can have you know set points of egress as well um um which you know if you're in a pretty lock down kubernetes cluster as you know and is is really important to have um you know only a certain amount of applications that can erress from the cluster okay I think um I think I definitely follow in terms of the the deployment patterns and layouts and having the multiple level of collector I think I think that's where we are thinking and going I think for me being in the Enterprise what what we are worried about is again moving stuff like this to production so if we have a theoretical issue in in a uh H sorry a vanilla collector you know again just theoretical example if we've got RTO and we've got to fix issues within a one or two hours for example um that's probably going to be the Tipping Point for us on the vendor versus vanilla question and because we we we would obviously be looking for some kind of support uh from a vender and typically we we would have that as as most folks would have through a vendor but then that then it certainly in my mind gives us another problem where instead of going from agent Poli proliferation where you are just now it's almost like going to Hotel proliferation you know it's almost like you're solving one problem and creating another so um I think like the others on the call you know we we're relatively early in in our journey and we're just trying to to go through the not necessarily the technical questions but the hardening questions what what what would reality look like when we're in production with with you know very high volumes of traffic coming through yeah I mean that that's it sounds like you're on the right path here overall um as far as you're thinking going um there definitely is that worry of like collector proliferation um you can avoid that you know multiple pools to gateways if you'd like um if you're going from I mean usually you're doing this if you're going for like a legacy protocol something like statsd which doesn't like to go over the Internet because it's UDP um or something like preus where you have all of these targets and you don't want to worry about managing the scrapes for them right um doing this at scale is going to be really environment and volume dependent um I think if your vendor provides their own collector they should be able to give you some support for um you know the vanilla collectors that you run I you know with the people that I work with do give support for you know whatever collectors they run and I mean we don't have a vendor specific collector like like like uh our company just doesn't give out a vendor specific one um but I you know I provide support for for any collectors that are customer runs um so if that's the fear I would check with your vendor to see they also will give you that type of support um I also found that like the steady state of these things is pretty once you tune it with uh resourcing and autoscaling um it's pretty hands-off I found um I actually was just working yesterday in a cluster that I touch every six months or so to do um some like Helm chart testing and it's been running for six months without issue um and with like a huge varying scale of traffic um because of autoscaling and and sort of just because of how simple we keep I I keep those collectors um this is for both metrics and traces too um for infrastructure and and application so I mean it's a much smaller example than than what you're talking about for sure um but the point remains where it's like once you reach a good steady state um especially with like your the Bal cycle um if that's what you have um if you're Autos scaling the setup correctly and if your configuration is is pretty uh you know nailed down you should be it should be pretty hands off knock on wood but that's definitely the hope yeah yeah okay thanks Jacob appreciate that yeah no problem thank you so folks we are coming up on time so we've got about five more minutes in case anyone has any more burning questions for Jacob Al righty I will take that as a no but uh thank you Jacob so much for um for joining today and sharing your migration story I think uh I think this has resonated with a lot of folks so we definitely appreciate you um coming on and and and sharing this experience with everyone um like I said this recording will be made available on the otel um YouTube channel also for anyone who has missed um Jacob's um otel Q&A uh session that we had last month there is a video up on the otel channel and reys um who works with Ren and me on the otel and user working group did a wonderful write up of the uh the Q&A in case video isn't your jam so definitely be sure to check that out okay so if you go to this link here you should be able to find our um our our various slack channels and we love um we encourage everyone to uh to just ask questions share use cases we love hearing all that stuff and also if you or anyone you know has a really cool um Hotel use case you're just getting started or you're a more advanced user does not matter we would love love to hear from you we're always looking for folks for otel in practice otel Q&A and we also have monthly uh otel end user discussions which we run those for three different time zones so we have them for emia APAC and Americas so be sure to join any one of those because always there's always uh really amazing and thoughtful discussions coming out of of these so uh yeah everyone thank you very much and once again Jacob thank you so much for taking the time to uh to chat with us twice yeah thanks so much for having me I appreciate thank you by have a good rest of your day bye yeah

