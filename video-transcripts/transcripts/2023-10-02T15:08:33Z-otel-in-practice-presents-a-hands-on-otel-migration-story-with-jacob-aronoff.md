# OTEL in Practice Presents: A Hands-On OTel Migration Story with Jacob Aronoff

Published on 2023-10-02T15:08:33Z

## Description

What happens when you're an Observability vendor migrating to OpenTelemetry? Jacob Aronoff knows exactly what that's like, ...

URL: https://www.youtube.com/watch?v=pHHINe9D94w

## Summary

In this YouTube video, Jacob Arof, a staff software engineer at Lightstep, discusses the migration from StatsD and OpenTracing to OpenTelemetry, focusing on the challenges and strategies involved in the process. Jacob outlines various migration paths, such as the "All or Nothing" approach, the "Slow Tail" method, and using a bridge for gradual migration. He emphasizes the importance of gaining organizational buy-in, the need for thorough testing, and the benefits of OpenTelemetry's API support across multiple programming languages. Jacob also shares personal insights from their migration journey, including performance issues encountered and solutions implemented. The session encourages audience participation, with attendees asking questions about practical challenges and seeking advice on transitioning to OpenTelemetry effectively. The video concludes by inviting viewers to engage with the OpenTelemetry community and participate in future discussions.

## Chapters

Here's a summary of the key moments from the livestream:

00:00:00 Welcome and Introductions  
00:01:30 Jacob's Background and Role  
00:02:45 Overview of the Migration from StatsD to OpenTelemetry  
00:05:00 Challenges in Migration and Gaining Organizational Buy-in  
00:08:15 Different Migration Paths Explained  
00:12:30 Discussion on the All or Nothing Migration Approach  
00:16:45 Exploring the Slow Tail Migration Method  
00:20:00 Advantages and Disadvantages of the Bridge Approach  
00:25:00 Audience Questions on Migration Experiences  
00:30:00 Jacob Shares His Team's Migration Process and Insights  
00:35:00 Discussion on OpenTelemetry Protocols and Collectors  
00:40:00 Managing Access Tokens and Security in OpenTelemetry  
00:45:00 Future Plans for Logging Migration and Infrastructure Updates  
00:50:00 Closing Remarks and Encouragement for Further Discussion  

Feel free to ask if you need more details or have other questions!

# OpenTelemetry in Practice

Welcome everyone to OpenTelemetry in Practice! I'm really excited about such a great turnout; I think this is one of our best turnouts yet. For anyone who couldn't make it today, there will be a recording available. If you have friends who missed this, please let them know we will be posting the recording soon and polishing it up once it's ready.

Jacob presented last month for the OpenTelemetry Q&A. We worked together at Lightstep, which is now part of ServiceNow. Without further ado, I'll let Jacob take it away.

---

## Presentation by Jacob Arof

Thank you! My name is Jacob Arof, and I am a Staff Software Engineer working on our Telemetry Pipeline team. Our team is responsible for our internal and external open-source telemetry efforts, including OpenTelemetry SDKs in Go, the Collector, and the Operator. Essentially, we handle everything you would use to collect and send data to your vendor.

Today, I’m going to talk about a migration we led last year, transitioning from StatsD to OpenTelemetry and from OpenTracing to OpenTelemetry. I’ll cover various migration paths, what went well, what went wrong during our migration, and I'm open to questions throughout, so please feel free to ask. My slides are pretty light, and I'm interested to hear about the challenges you're facing and the features you're interested in.

Let's dive into the migration story.

### The Problem

I’ve been in the industry for many years and have led several migrations where hand-rolled metrics, tracing, or logging libraries just aren’t cutting it anymore. Some common issues include performance not being sufficient, high maintenance costs, or features that everyone wants but can't be implemented. Depending on the number of services your company runs and the organization's willingness to undergo migration, these can be daunting challenges.

This theme resonates with many engineers, especially SREs or DevOps folks who are passionate about making things work but struggle to get the necessary buy-in. The solution, of course, is to migrate to something new, like OpenTelemetry's metric API, trace API, or logging API, which are accepted by most major vendors out of the box. 

### Migration Challenges

When considering migration, you might face several questions:

- How do you break up the work?
- How do you ensure all your telemetry works the same as before?
- What are the risks of different migration models?
- How do you convince leadership that this is worth doing?

Trying to brute-force your way into a migration can lead to burnout. Hence, the first step is making a convincing case for migration. Sometimes, a proof of concept helps, but the key is getting a group of people who agree with you and can help communicate the value to stakeholders.

In my case, we were a main OpenTelemetry contributor. The OpenTelemetry metrics API and SDK were looking for quick feedback from a team with high traffic, making it easy to sell the migration. In past roles, the selling point often revolved around maintenance costs and performance improvements, especially when we counted the number of tickets and hours spent on features and maintenance.

### Migration Paths

Let's discuss the migration paths we considered:

1. **All or Nothing**: 
   - This method involves completely ripping out existing instrumentation in favor of OpenTelemetry. 
   - Pros: You reduce the time for split-brain scenarios, where some services are on one system, and others on another. It makes issues very visible since any problems will be apparent immediately.
   - Cons: This requires thorough testing in a robust development environment. If staging and production environments differ significantly, you may face production bugs that you couldn't catch in staging.

2. **Slow Tail**:
   - This method allows application developers to migrate themselves by toggling an environment variable. 
   - Pros: Less upfront work and the ability to confirm functionality for one service at a time. It also gives you more time to develop dashboards and alerts.
   - Cons: This can be slow, especially in larger environments. Bugs may not reveal themselves uniformly if services are not instrumented the same way.

3. **Bridge**:
   - OpenTelemetry provides a bridge for some migrations, allowing you to transition from one instrumentation to another without significant code changes.
   - Pros: Minimal code changes required.
   - Cons: You may experience performance drawbacks compared to using the new method directly.

### Transition to OpenTelemetry

In our case, we began migrating from StatsD to OpenTelemetry metrics using the Slow Tail approach. We had several motivating factors, including wanting to utilize new metrics features that StatsD didn’t support, like asynchronous instruments and exponential histograms. 

While we started our tracing migration, we faced some performance issues due to the wrapper library we were using, which required conversion to OpenTelemetry tags. This led us to begin our tracing migration using the All or Nothing approach, as tracing had been stable for some time.

We wrote the code for the migration and pushed an image to our staging and production environments, confirming that each version looked the same before and after the migration. This was a crucial step, as it allowed us to catch integration issues early on.

### Next Steps

Currently, we are focusing on migrating our infrastructure metrics to use OpenTelemetry's first receivers and plan to migrate to the new logging cases soon. 

---

### Questions and Discussion

Do we have any questions?

**Question from Rahul**: I'm currently experiencing challenges in shifting mindsets from our previous vendor to OpenTelemetry. Any tips on winning over various developer teams?

**Jacob's Answer**: It's crucial to sell on features. For instance, highlight cost savings—if you’re moving from a service like DataDog to Prometheus, the cost difference can be significant. Additionally, emphasize the local development experience for metrics and the ease of using Grafana for dashboards. Building local tooling around metrics can also help developers see the benefits firsthand.

**Question from Jay**: What is the best protocol to use within OpenTelemetry for performance and security?

**Jacob's Answer**: We typically use gRPC for our setups, but it can depend on your company's security requirements. I recommend using the OpenTelemetry Collector where possible, as it offers numerous options for exporting data to your desired backend.

---

As we wrap up, I want to thank everyone for their participation today. If you have any further questions or would like to share your OpenTelemetry use cases, feel free to reach out! 

Thank you, Jacob, for sharing your insights and experiences with us. We appreciate your time and expertise. Have a great day, everyone!

## Raw YouTube Transcript

welcome everyone to otel in practice um really stoked for such a good turnout I think this is one of our best turnouts yay um and for anyone who uh could make it today there will be a recording so if you have friends who are like damn it I missed this you can let them know there that we will be posting the the recording um and prettying it up once it's available so uh Jacob presented um I want to say last month for otel Q&A we worked together at lightstep from service now um and uh I'll let's Jacob take it away yeah thank you um so yeah my name is Jacob arof I am a staff software engineer um I work on our Telemetry pipeline team uh our team is sort of in charge of our internal and external open source Telemetry efforts whether it's like OTL sdks and go or The Collector or the operator um sort of anything that you would be using to collect and send data to your vendor is what we do so today I'm going to be talking about a migration that we LED last year and a little bit year and a half ago uh for when we migrated from uh stasy to open Telemetry and also from open tracing to open Telemetry going to talk about the you know various migration paths that are out there some of the things that went well and went wrong or our migration uh but I also would like this to be you know feel free to ask as many questions as you have um my slides are pretty light so I'm really interested to here you know where people struggle currently uh what type of challenges you're running into what features you're interested in um so please you know raise a hand or you know comment in the chat uh and I'd be happy to answer any questions and go down any interesting avenues that you might have so with that I will share my screen uh can we all see my slides yes great thank you very much so yeah today I'll be talking about that oel migration story so first let's talk about a problem uh I've been in the industry for you know many years many years um and I've had to lead a few of these migrations where your handrolled metrics or tracing or logging Library just isn't cutting it anymore something where it's not performant enough maybe the maintenance is really expensive maybe um there's a feature that everybody wants that you just can't Implement um you know whatever it might be you want to do a migration um it can be very challenging and daunting depending on the amount of services that your company runs depending on what the organization's uh you know willingness to like undergo this migration might be uh so all of these are problems that I've faced um in various jobs not just where I am now but you know a few past roles as well uh it can be really hard to make this happen especially when you know you as maybe an SRE or a devops person or maybe just a you know engineer just want to make this work uh when you feel very passionate about making it happen and you just can't seem to get the buying that you need um so that is sort of the theme for today um and there is a solution right it's like migrating to this new thing um otels metric API I'm going be talking about metrics traces but you know this is really true about a lot of stuff um for otel the metric API the traces API and even the logging API is accepted by most major vendors out of the box now um the API is support for you know all these instrument types um in all of your favorite languages probably they're very few that don't um and the performance and compatibility is always top of mind for us um throughout the stack of otel um components everybody is always thinking about performance um so you know with all this in mind you're like yeah this sounds great uh I want to start using this um but there's a problem right like using these new tools migrating these new tools is hard how do you break up the work how do you know all of your Telemetry is working the same as before uh what are the risks of these different migration models what even are the different migration models um and probably the most important one here is how do you convinced leadership that this is worth doing um you can try and you know brute force your way into doing a migration but that to me is a recipe for Burnout um doing a migration in general where you know there's a new architecture that you think is really going to improve uh quality of life for your team for your company uh without getting organizational buyin is how you spend months in a project that may never see the light of day and that is really demoralizing and very difficult to work around so the first thing that you have to do when you want to migrate to a new tool is make the case um it's you know not even um you know show a proof of concept it's just can you convince people that this is worth doing sometimes a proof concept helps but that's not you know the key to success the key to success is really getting a group of people who agree with you that you're able to sell one-on-one who can help you really make it clear to the stakeholders uh you know in your leadership that this is worth doing um for me was pretty straightforward we're you know a main open tary contributor uh the oot metrics API and SDK wanted to go uh into their you know 1.0 stable um and they really wanted some quick feedback from someone who has a lot of traffic uh to test it out and see where the edges are so so in that case it was very easy to sell the migration in past jobs the selling point is usually you know you look at maintenance cost uh and performance overall um I worked with a handrolled uh solution uh in few jobs ago and when it came time to do a migration the thing that really sold people was just counting the amount of tickets and hours that we have to spend on you know new features and maintenance on our internal Library um as well as the amount of times where you know we've had an incident because something related to our instrumentation is just incorrect which you know does happen a lot with your own handrolled stuff so that is maybe a good basis to go off of uh does anyone have any questions sort of before I go into more specifics about this section and uh I'm going to do like a five count something to learned from a teacher of mine uh I'm just going to we're going to do 5 Seconds of Silence um until someone raises their hand or I'll just keep going so no cool oh is there something in the chat oh that's just me moving it a little cool no worries uh thank you um so let's talk about the migration paths um the first one is is what I call the All or Nothing um this method has you entirely rip out your existing instrumentation in favor of otel this would be you know a lot of people talk about you know replacing you know the engine of a of a running plane or running car that a lot of people will say that um this would be like you know just getting a new plane and have every and you have everyone hop to the new one while it's flying so it's difficult but you know maybe it has its benefits one of the pros is that you know once you've pushed your code and you've confirmed things are working and you have a good enough cicd system your work is really done right it just rolls out and everybody's happy this reduces the time for split brain split brain is what happens when you're on two systems that may not be compatible together you could imagine you know if you're on stats D or if you're on Prometheus even um if most of your metrics come from Prometheus they don't have periods in them they have you know pretty Str requirements about um their shape overall um you can't do things like um up down counters like that's not a type that they have um they used to not have a proper histogram they now do uh they now have an exponential histogram support which is experimental but you know there are things that otel has that um Prometheus or statsd just does not have and doesn't have the ability to do um so being in a split brain where some Services have it and some Services don't and they're emitting different metrics and different shapes different variables um that can get pretty unwieldy pretty fast and so if you're not very deliberate about planning how you're going to migrate your dashboards and alerts then you're going to be stuck with both of them for some time which if you're on call and if you've been on call during a migration for this type of stuff that gets really painful if you get paged at 3:00 a. you have to wake up and go oh you know which uh which dashboard should I check for this is the service on otel or is the service on statsd um that's really frustrating uh that that's the type of thing that you don't want to have an on call engineer uh think about so the other benefit of doing this All or Nothing approach is that the issues are incredibly visible if a dashboard breaks if an alert Pages or a service crashes um it's pretty obvious right hopefully you're looking at a dashboard or you know the person you take the pager when you're doing this might ation so that you experience that pain and hopefully you also page on no data it's very important um and then service crashes you should have an alert on that ideally um so all of these things though make it really clear that as soon as you do this All or Nothing thing um you know if all of your services are rolling out within an hour with this new change and everything is looking good that's a very good sign and that that gives a lot of confidence um this does though you know the Clauses here this requires a lot more thorough testing in a really good development environment if you're unable if you have a lot of uh environment drift where your staging environment is entirely different than your public environment this might be really challenging because this means you might have uh production bugs that you're just unable to catch in staging um and if you do if the blast radius is all of your services that can be really dangerous um also you know you do have those compatibility problems I mentioned and so you have to be very deliberate about um observing which dashboards and alerts break and fixing them proactively um or even in advance if you know what the metrics are going to be um the example I have here is is definitely a common one where you could imagine a metric type changes but a metric name doesn't which most vendors will just reject um or the dashboard that you're looking at will look very strange um this also does mean that there's more upfront effort to migrate your services it's going to take you know probably a group of people to help you monitor this depending on how many services you have to Mig if you're you know a company with maybe five to 10 Services that's not too bad to do with one person but if you're a company with hundreds of services you're going to have you're going to want a team of people to uh monitor this with you so on to the next one we have the slow tail um this method gives your application developers the ability to migrate themselves by usually flipping an environment variable on and off for whichever method they want um benefit here is that there's less upfront work uh you can confirm that it works for one service and push it out for that service only and you can do that in all of your environments so if you have that um you know if you don't have that confidence that I mentioned earlier about your saging environment versus your production environment um this would be really helpful because you could actually just push a single service without the fear of you know all these other services rolling out to verify that your change is worked as expected and finally this also allows you to have more time to develop dashboards and alerts to handle those compatibility issues problem is is that this can be very slow um if you have more than 50 services in you know at least two environments and it takes an hour to migrate a single service because you're trying to be extra careful then you're looking at a multiple weeks or months long process um if you leave a migration to app developers without a strong why also they'll never do it so it's usually going to fall on your team to make that happen um bugs Also may not reveal themselves if your services are not uniform imagine you have something like a q worker that works off of kofka uh which whose topology looks entirely different than something like you know a classic API server um if you're only you know instrumenting your API servers and then you go to instrument one of those kofka servers if there's something that's significantly different in how you did your instrumentation that might take some time to figure out uh what's going wrong um you know ideally otel um has figured out a lot of this but doing any of these migrations you always need to check um you know we just cannot and honestly should not know how you instrument your services um you know you don't want to have to explain your whole um observability backend to us that doesn't really make much sense so ultimately it's important that you do some work to check that the migration is working as you expected uh one thing that I did just think of is that you could do a combination of this slow tail all-in-one where you do a migration for you know say some good sample of your services and then once you're pretty confident with those you could move to just enabling for everyone at once um that's another good option uh it I'm trying to think about the drawbacks of that not sure there are any I think that's probably the way to go unless you're really nervous about um some of these compatibility problems or some of these like unknown unknowns that might be the only time that that would be a little uh scary so the last one is the brdge um open Telemetry for some migrations actually provides a bridge where you can go from you know instrumentation a to instrumentation B um without having to do any real code changes other than implementing the bridge there are a few issues here um well already you can see the pros I mean it's pretty obvious right you don't have to make many code changes problem is is that you're going to have some worse performance in comparison to writing using the new method um it's going to be you know a fair you know there's a conversion cost to anything that you're doing in your application um if another option is you could just send from you know instrumentation a and then convert it to instumentation B with otel collector so if you wanted to go from stats d The otel Collector has a stats D receiver and an otel exporter so that's also fine but both of these have the same drawback which is that if you wanted to take advantage of some of the capabilities of the otel sdks you know then this Migra you're not really doing a migration um it means that you can you know try and migrate stuff um Peace meal and like for dashboards and alerts which is great but it does have this drawback of like you're not actually doing the migration you're still you know just putting off the hard work later um and so this can also be confusing to your app developers where someone says well my code says open tracing but this trace says Hotel why can't I you know use x feature um and that that can be a little confusing I think that's like not the end of the world though it's it's you know as long as you're communicating well that should be all right um but yeah so this is really I think using the collector is another really good path forward overall though um doing the thing where you just send some traffic to The Collector from one of your services uh you migrate your apps and dashboards and then you have everything sent to the collector and then you can migrate your apps to otel with that allinone approach um and then your dashboards and alerts should just you know already be changed and you should be all set before I move to the next portion do I have any questions any thoughts I'm going do a five C one question Jacob I'm actually living through this right now um one thing that I'm running into quite a bit is like getting the various developer teams to shift their mindset from sort of our previous vendor that we were with into our newer one and it's you know there's underlying things we're doing like the statsd into like premier Prometheus style type thing with it and yeah that's led to a couple people who are like oh these things aren't apples to apples here anymore and that's causing a bunch of stuff um any tips and suggestions on like winning the hearts and Minds here yeah so really like the thing to do is sell on features when you can so if you could say you know this improves our well the easiest one is cost right if you're using staty you're probably coming from data dog you're going to say you know our previous spend with data dog was X thousands maybe millions of dollars and with Prometheus it's like you know 100 100 a month or something right that's the easiest one um but the more valuable one is you know you sell in the ecosystem where um for me the real benefit of Prometheus is that um the local development experience for metrics is actually much simpler um you don't have to run you know a data dog agent you don't have to do anything else you can just hit your metrics endpoint and verify that your metrics are doing what you expect them to um and that that's a really good developer flow um for testing that stuff um the other thing that is I I mean people from data dog I used to work for data dog um and they their dashboard product is great um but grafana if you're using grafana also has a really strong dashboards product showing something like you know maybe you use redus in your Cube cluster um you install a redus uh service Monitor and then you install the off-the-shelf grafana dashboard for it right like that's a pretty great experience and that's all done at you know zero which is pretty incredible you know you know Prometheus metric cost which is very small in cents on the dollar um so the last thing you can do is like trainings I I have a storied history with trainings I think that they never really achieve the thing that you want um which is for more people to be excited um really what they do is can they can cause more confusion if you're not careful uh with like your language um I think maybe a strategy that I've always wanted to do is like build some local tooling around the stuff to developers um whether it's you know writing maybe an endtoend test or a little UI around their applications uh metrics so they can do something with them locally to be like oh yes like this thing is working as expected um I mean I think that the local Dev for Prometheus is just pretty fantastic um and that's probably what I would sell on um but again you know it's so it's very company specific and in many ways right if people are really bought into the datadog model of things which is you know high cost very low thinking um Prometheus and grafana is not really that it's it's low cost but much more thinking and ultimately like you don't want your developers to have to think too much about their instrumentation um the thing that uh I tend to do is have a uh rapper library before doing a migration um so that people are used to the people are using the same signatures it's just doing a different function under the hood it also makes your migration easier um I didn't mention it here because most companies already have some type of wrapper because they have some needs that are like specific um it's usually like a thin wrapper but doing as much as you can to not change their workflow and make it very simple is always going to be good um the thing with Prometheus you can do is check what metrics are available and then you can autogenerate dashboards from those metrics and uh you can add into your um Helm charts you know these are the metrics I care about and then you could just generate alerts automatically from that as well um so there's a lot of that quality of life stuff that again like it's easy with the data dog UI but is automatic with um you know infrastructure as code so that's like another trade-off I would say because not a lot of people I don't think use terraform at data dog um so I don't know maybe they do and I just don't know that oh thanks for that check it yeah problem uh any more questions on this part cool okay so now I'll talk about what I did what the team did um to forour migration from statsd to otel or metrics obviously um so so we began migrating using that slow tail path um the reasons were uh I'd say good motivating factors as I mentioned earlier the metrix API andd weren't declared stable at the time which meant we would have had to deal with some signature changes um and that would have been potentially a lot of signatures uh we did have a wrapper library but still doing those types of changes can be pretty frustrating um we also wanted to use some new metrics features that statsd didn't support this is like asynchronous instruments um the biggest one was exponential histograms which otel sort of pioneered um and then the last one is that we would you know because we are the group that helps write these libraries uh we wanted to understand the performance and quality of life features to make it as easy as possible and as you know um as quick a decision it's just you know you don't have to worry about performance you don't have to worry about all this other stuff how do we make this simple um so in migrating to otel for metrics we did find a few performance issues um the first one was you know because we were using that rapper Library I mentioned um our implementation was working off of Open tracing tags which at the time was our tracing instrumentation which we then would need to convert into otel tags anytime you wanted to make a metric which if you think about the amount of times that you call you know metric. record um that's a lot of time so that gets pretty expensive um and so while we waited for improved performance in the metric libraries uh we just began our tracing migration because the otel team needed time to investigate some of the stuff that we brought to them um and it also meant that we could fix you know the very um our company specific problem of this uh conversion and so then we began our tracing migration um going from open tracing an open sensus to otel and so we went for this one with All or Nothing approach because otel for tracing had been stable for some time weren't really concerned about compatibility um and ultimately at the time there weren't a lot of guides written um but the process was relatively straightforward and the compiler is really doing most of the work for you um the structure for it we already supported oel traces in our product so there's actually like no in theory there were no real fun changes either uh for our own dashboards and alerts that we were going off of so what I did for this was write some write the code for the migration um and then push up build and push an image to our staging and production environments for a service that doesn't get much traffic does get some it's important that it gets some um and then it confirmed that each version in our product looked the same before and after we have a view uh of sort of you know these red metrics per version and it it shows you the difference in those versions and if any of those looked different if the rate for example uh dropped off um then that's a sign that we did something totally incorrect um doing that for one surface is good but then the reason that the you know the All or Nothing approach is really useful is that you get that integration test so when we when I migrated all of our services to this new method it was really clear that service to service Trace propagation wasn't working you know you just did a spot check of a few different um traces and it's really that's like such an obvious thing um so that was a pretty easy fix I actually had to just go into the oel community um and Implement something that uh had a Todo around it um another issue that we had right before you know sort of in the the last bit of this migration was that our sampler wasn't config configured correctly um otel provides a lot of new features for sampling and I was under sampling in one environment and then oversampling in another environment uh because I misconfigured it so I had to roll that back really quickly fix it and then push it out a day or two later but overall this whole process took maybe a month of work um to migrate I think it's like a hundred or so services in all of our environments um which is if you've ever led a migration that that's a pretty good time I was pretty happy with that one it also meant that we could get back to our metrics migration uh because we had sort of achieved that goal with open tracing tags um we were able to use those use the fact that we didn't need do this conversion to continue our metrix migration it also we came back to the otel team having shipped some real performance and quality of life improvements um that really let us continue with this without the fear of performance problems um these changes also let us use this feature called metrics views uh which let you create a new metric series to provide seamless compatibility with statsd statsd emits their histogram is what's really what's called a Prometheus summary um but we wanted to Emmit exponential histograms but that's the really the only change that was happening here um all of our other metrics were were able to say about the same so uh we just needed to dual write the old summary which otel didn't really support at the time and I think doesn't and shouldn't it's a bad metric type um and we also wanted to dualite the exponential histogram so that we could migrate all of our dashboards and alerts from this old approach to this new approach um and then finally you know as I said we put this Library behind a fature flag and then we could just flip that on and off whenever we wanted um and then we would roll it out pretty slowly over a two or three month period um to all of our environments this also let us like test the change really effectively as well um I think what's next let me go back so any questions about this migration process do five count again Jacob this is Rahul um hi I have one question around the OTP protocol so I'm guessing you must have used the OTP receivers and exporters vastly during your metrics and Trace migration so what is the go-to protocol within OTP it supports stdp and grpc but from security and performance point of view which is the go-to protocol for traces and metrics uh I'm not sure I follow we we actually just um emitted OTP directly from our sdks to our SAS so we didn't go through a collector for this one um we could have gone through a collector but uh we didn't want the operational overhead at the time it was enough migrations to to handle it once but that didn't really answer your question can you rest your question yeah I mean I wanted to know um what is the best protocol to use under OTP is it stdp or grpc um in terms of metrics and traces yeah in terms of performance and security yeah so I'm not the best with for security recommendations um I would say we use grpc or everything um just because it's I don't know our sort of deao internal standard um HTTP is more accepted for some like depending on your company security requirements um I'm not sure of the real differences like the like to compare and contrast them I don't think I'd be um the right person to speak to those maybe some of the other oel folks have more of an opinion or more information on that though yeah don't okay cool were there any learnings around managing access tokens um and did you use multiple access tokens uh does it portal you know um if you're sending a lot of metrics or traces or a single access token yeah we uh we just used um the same access to token approach that we did um where I I don't know if I can speak to that just because it's it's you know internal security stuff um yeah sorry about that yeah no way I'd say that you know the best thing you can do for security in like a cloud mative environment um is use some sort of Secrets provider um the kubernetes external Secrets operator um is great uh you can hook it up to something like gcps uh KMS um and um decrypts inversion your secrets um to then load access tokens from though um you can do the same thing with AWS or vault or any of these other providers as well um if you're particularly like security inclined it's also important to use things like mtls as well um something like ISO can help you with some of that um the otel operator actually provides some mtls features in open shift um so you know there there are a lot of security features out there to be used um hopefully that that seeds some interesting um investigation for you yep yep thanks no problem um so moving on uh you might be wondering what's next are we going to do a logs migration um and right now we're under a different migration which is changing our infrastructure metrics to use the otel first receivers like the CP cluster receiver the CET receiver um and so forth um because we really want to start using some of these things the community is writing um after that we should be able to begin migrating to use the new oo loging case which um should be looking pretty good in a few they're looking good now but um I'm not sure what the state of it is uh for go just yet given that go just released like a new um standard logging Library um yeah any more questions hi this is uh Jay speaking um I have one general question um uh regarding open telary so um I I'm pretty new to this new to open tet Tre and uh the the reason why I was investigating into open Telemetry was uh for its ability to be backend agnostic for generating uh metric uh data uh the question is however uh I was interested in exploring po based uh metrics uh uh exporters uh but uh uh so far uh I don't if I'm right or wrong but the promethus export exporter within the open Telemetry SDK is the only one that is supporting uh the pull based metrics approach is that correct so um it sounds like you're going so there are a few different types of exporters within um uh within Hotel so there's an SDK exporter um which yeah there's a prus exporter I think there might even be a D exporter um but one what might be a better fit is to use the otel collector and use their exporters which are numerous and and pretty much every backend um has some type of exporter um so I think I would try and and say you should if you're GNA use otel sdks you should export an OTP to an Noel collector and then export to you know your protocol of choice um yeah but when using this uhel exporter this would rather be a push based mechanism right uh to send data to The oel Collector yeah that's correct okay okay uh you can also if your instrumentation is in Prometheus right now though um you can have the otel collector scrape the Prometheus metrics and then export them as OTP or export them as statd or you know whatever you want good thank you any more question do you use any solution to manage or you know update the AML file um of the fleet of agent Fleet um on the go and is there any built-in solution that you guys are using to manage the collectors yeah so um the I am a maintainer for the oel operator um and internally and externally I recommend using the oel operator with the otel collector crds um they're pretty uh easy to use easy to set up easy to manage um and we're always developing and thinking about new features as well um and so that that'll be the place to get those uh now and in the future and I'd continue recommending that okay thanks Jacob could ask a question following on from the last question um would you recommend using something like opamp in terms of managing your Fleet of collectors or would you say it's preferable to do it in an operator basis you know kind the C pattern yeah so if you're in kubernetes um OPM by the way is still pretty early Alpha right now well the protocol itself stable but the actual implementations are in Alpha um so I'm going to answer this question with the assumption that the implementations are done if that's all right sure yeah um so if you're in kubernetes I would recommend using the opamp bridge that we're developing um the bridge is a component that can connect to your vendor um and will be able to manage uh pools of collectors rather than just having an extension that works on or a supervisor and an extension that works on a single collector pod um the reason for that is usually you are running a collector um and what you you're running a collector in a pool not as a monolith and so whereas opamp would be opamp on a using a supervisor would be very useful for um you know a a VM or just running it as a binary um doing it in kubernetes if you're running it as a pool it's not a great pattern in kubernetes to have a supervisor update a single pods configuration to make it not uniform with the other pods in its replica set so what that means is you know if you're going to run it in kubernetes if you're going to run a replica set of pods in kubernetes you want those pods to be the same configuration and if you're only running a supervisor um if you're running a supervisor on each of those pods but you're only making the change to a single one um that's an anti-pattern and can get you into some trouble um so the bridge however can manage pools of col ctors um and is definitely what I would recommend to use again with this stuff being completed that's what I would recommend for kubernetes okay thank you no problem I've got a few questions um related to the work so I work at a large US Bank and I'm trying to bring in hotel um and essentially have that as our main strategy um to try and move away from B on vendors specifically um the question that we running into just now are the challenges like vanilla versus vendor and what I mean by that is um do we just pull down if I take the collector for example do we just pull down the collector configure The Collector the way that we want it with receivers and exporters um or with the specific the Strategic vendors that we work with uh do we look to bring their vendor wrapped collector and deploy that within our Enterprise there's obviously pros and cons in doing both it doesn't sound like you know from your side Jacob obviously you're working from the vendor side but I have spoken to other vendors who have given me an interesting range of opinions on what what area or which of those to look at um it'd be quite interesting to see what your thoughts are yeah this this is a great uh really great question um it totally depends on your deployment model I would say so one option especially you know if you're running thousands of PODS and you know hundreds of clusters um the model that I would recommend you use is the Gateway model where you know let's say you have a collector per group of apps and then all of those collectors forward to a centralized pool of collectors that then forward to your vendor um this means that your uh those pools for your applications are vendor neutral um because all they're really doing is gathering in forwarding stuff um for your application teams and then a centralized team would manage the Gateway uh the Gateway collectors and then that's the place where you make the decision about am I going to use my vendor wrapped collector or am I going to use my uh you know vendor neutral one the Choice becomes really easy to you know if for some there's some feature that your vendor provides um that's only available in their WS collector um you could choose to use the rapid collector there and then in the future if you wanted to change vendors you still have OTL data sending to that vender collector and so you can just change that one out very very easy the reason that this is good is because you wouldn't have to go to your application teams um or any of these other you know Orcs and say hey you know you have to reconfigure your whole um setup because we're we're changing our underlying vendor here whereas you as the centralized team could be the one to just change a single pool to make it all um consistent um that's probably the what I that that's like a pretty uh future proofed approach um the conf you know the configuration for what what you're going to do no matter what is going to be complicated um but that one is probably going to do you best if you really want to use the vendor collector if not still doing the Gateway approach is a pretty good one um you can centralize things like officiation sampling rates um or even like you know requirements for Telemetry things like attribute requirements um can be centralized before they can egress um it also means you can have you know set points of egress as well um um which you know if you're in a pretty lock down kubernetes cluster as you know and is is really important to have um you know only a certain amount of applications that can erress from the cluster okay I think um I think I definitely follow in terms of the the deployment patterns and layouts and having the multiple level of collector I think I think that's where we are thinking and going I think for me being in the Enterprise what what we are worried about is again moving stuff like this to production so if we have a theoretical issue in in a uh H sorry a vanilla collector you know again just theoretical example if we've got RTO and we've got to fix issues within a one or two hours for example um that's probably going to be the Tipping Point for us on the vendor versus vanilla question and because we we we would obviously be looking for some kind of support uh from a vender and typically we we would have that as as most folks would have through a vendor but then that then it certainly in my mind gives us another problem where instead of going from agent Poli proliferation where you are just now it's almost like going to Hotel proliferation you know it's almost like you're solving one problem and creating another so um I think like the others on the call you know we we're relatively early in in our journey and we're just trying to to go through the not necessarily the technical questions but the hardening questions what what what would reality look like when we're in production with with you know very high volumes of traffic coming through yeah I mean that that's it sounds like you're on the right path here overall um as far as you're thinking going um there definitely is that worry of like collector proliferation um you can avoid that you know multiple pools to gateways if you'd like um if you're going from I mean usually you're doing this if you're going for like a legacy protocol something like statsd which doesn't like to go over the Internet because it's UDP um or something like preus where you have all of these targets and you don't want to worry about managing the scrapes for them right um doing this at scale is going to be really environment and volume dependent um I think if your vendor provides their own collector they should be able to give you some support for um you know the vanilla collectors that you run I you know with the people that I work with do give support for you know whatever collectors they run and I mean we don't have a vendor specific collector like like like uh our company just doesn't give out a vendor specific one um but I you know I provide support for for any collectors that are customer runs um so if that's the fear I would check with your vendor to see they also will give you that type of support um I also found that like the steady state of these things is pretty once you tune it with uh resourcing and autoscaling um it's pretty hands-off I found um I actually was just working yesterday in a cluster that I touch every six months or so to do um some like Helm chart testing and it's been running for six months without issue um and with like a huge varying scale of traffic um because of autoscaling and and sort of just because of how simple we keep I I keep those collectors um this is for both metrics and traces too um for infrastructure and and application so I mean it's a much smaller example than than what you're talking about for sure um but the point remains where it's like once you reach a good steady state um especially with like your the Bal cycle um if that's what you have um if you're Autos scaling the setup correctly and if your configuration is is pretty uh you know nailed down you should be it should be pretty hands off knock on wood but that's definitely the hope yeah yeah okay thanks Jacob appreciate that yeah no problem thank you so folks we are coming up on time so we've got about five more minutes in case anyone has any more burning questions for Jacob Al righty I will take that as a no but uh thank you Jacob so much for um for joining today and sharing your migration story I think uh I think this has resonated with a lot of folks so we definitely appreciate you um coming on and and and sharing this experience with everyone um like I said this recording will be made available on the otel um YouTube channel also for anyone who has missed um Jacob's um otel Q&A uh session that we had last month there is a video up on the otel channel and reys um who works with Ren and me on the otel and user working group did a wonderful write up of the uh the Q&A in case video isn't your jam so definitely be sure to check that out okay so if you go to this link here you should be able to find our um our our various slack channels and we love um we encourage everyone to uh to just ask questions share use cases we love hearing all that stuff and also if you or anyone you know has a really cool um Hotel use case you're just getting started or you're a more advanced user does not matter we would love love to hear from you we're always looking for folks for otel in practice otel Q&A and we also have monthly uh otel end user discussions which we run those for three different time zones so we have them for emia APAC and Americas so be sure to join any one of those because always there's always uh really amazing and thoughtful discussions coming out of of these so uh yeah everyone thank you very much and once again Jacob thank you so much for taking the time to uh to chat with us twice yeah thanks so much for having me I appreciate thank you by have a good rest of your day bye yeah

