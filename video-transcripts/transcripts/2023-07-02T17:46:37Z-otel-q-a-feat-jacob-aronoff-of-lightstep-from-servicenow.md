# OTel Q&amp;A Feat. Jacob Aronoff of Lightstep (from ServiceNow)

Published on 2023-07-02T17:46:37Z

## Description

Jacob Aronoff, one of the OTel Operator maintainers, joins the #OTel End-User Working Group's Q&A series to talk about how he ...

URL: https://www.youtube.com/watch?v=dpXhgZL9tzU

## Summary

In this YouTube video, a Q&A session features Jacob Arnoff, a staff engineer at Lightstep, discussing his experiences with migrating to OpenTelemetry (otel) within his organization. The conversation is led by an unnamed host, who highlights the significance of an observability vendor using otel for their own products. Jacob shares insights about the migration process from OpenTracing to OpenTelemetry, detailing the challenges he faced, such as performance issues and the importance of maintaining alerting capabilities during the transition. Key topics include migration strategies, the use of attributes and metrics, the differences between sidecars, deployments, and daemon sets in Kubernetes, and the implementation of the target allocator for efficient data collection. The discussion wraps up with a focus on the evolving nature of telemetry setups and a call for Jacob to potentially give a talk on his experiences.

## Chapters

00:00:00 Introductions
00:01:18 Guest introduction: Jacob Arnoff
00:05:54 Migration from OpenTracing to OpenTelemetry
00:10:49 Migration strategies discussion
00:16:23 Performance issues during migration
00:22:37 Discussion on metrics aggregation
00:31:28 Target allocator explanation
00:39:40 Collector setup discussion
00:47:12 Use of stateful sets vs deployments
00:52:07 Wrap-up and parting thoughts

## Transcript

### [00:00:00] Introductions

**Host:** Thank you, welcome everyone to Hotel Q&A. Thanks for joining. Today we are super lucky to have Jacob Arnoff from Lightstep from ServiceNow join us. It's a cool treat because I tapped Jacob because he was telling me that he had submitted a CFP to one of the, I think it was KubeCon, right Jacob? 

**Jacob:** Yeah, it was a CFP on migrating to Hotel from our organization migrating at Hotel, right? If I understand correctly, which I thought that's a pretty freaking cool story to tell. 

### [00:01:18] Guest introduction: Jacob Arnoff

**Host:** So I asked him to join and have this Q&A and hear the story because I think it makes for a compelling narrative when an observability vendor talks about using OTEL themselves on their own products. So here we are. Welcome, Jacob. Do you want to do a quick little intro?

**Jacob:** Sure. Yeah, hi, my name is Jacob Arnoff. I'm a staff engineer at Lightstep on the Telemetry pipeline team. I've been with Lightstep for almost two years, and the first year of that journey was solely focused on Hotel migrations, doing them internally and making them easier for customers, sort of that whole process. 

**Host:** That's interesting. Do we want to do this very Q&A style or should I just go right into the story and talk about it?

**Jacob:** We can let it evolve organically.

**Host:** Cool. Yeah, I like that. Why don't we start? I mean, I think you're rearing to go, so maybe why don't you describe, like, set the scene for us.

**Jacob:** When I joined Lightstep, we were still on OpenTracing for tracing and then a mix of OpenCensus and some hand-rolled StatsD stuff for metrics. This meant that we had to run a proxy on every single pod that we ran in Kubernetes. A proxy, since it's a sidecar on every pod, means that every single time you run it, one of your applications, you have to run another little application that's going to read from StatsD and then forward those metrics off. 

For us, as we were building out a metric solution, that destination was Lightstep. I came in, and this was sort of at the beginning of metrics Alpha, I think. I was like, hey, it would be great for us to, we know that Hotel for metrics is going to be a huge effort for us in the next year or two. We wanted to reach stability. The Hotel team at Lightstep had been working on it a lot and really wanted some immediate feedback on how to improve it. So I took on that migration for us. 

We also had the theory that doing so would save us a good chunk of money because we would no longer need to run these relatively expensive StatsD sidecars. I planned it initially to be sort of as safe as possible. I'd done some migrations like this in the past, and there are a few different ways that you can do migrations like this. You can do the all-in-one go, which for us would have been possible since we're in a mono repo, but it's much more dangerous because you worry about, am I going to push a bug that's going to take everything down? 

Obviously, this is application data. It's data about your applications, which we use for alerting. We used to understand how our workloads are functioning in all of our environments, and so it's important that we don't take that down because that would be disastrous for us. But obviously for an end user, it's going to be the same story. They want the comfort that if they migrate to Hotel, they're not going to lose all of their alerting capabilities immediately. They want a safe and easy migration. 

So our initial approach was to do a sort of feature flag-based part of the configuration that you run in Kubernetes. It would disable this sidecar and enable some code that would then swap to OTEL for metrics and then forward it off to where it's supposed to go. That was sort of the path there. 

### [00:05:54] Migration from OpenTracing to OpenTelemetry

Midway through this journey of doing these migrations, I had tested it all out, and staging looked pretty good. I tested the container in our meta environment, so we use it to monitor our public environment. I noticed some pretty large performance issues in those 2021, and I had reached out to the Hotel team, and we had worked together to sort of alleviate some of those concerns. One of the ones that we found that was a big blocker was we heavily use attributes on metrics right now, and it was incredibly tedious to go in and figure out why, you know, figure out which metrics are using all these attributes and getting rid of them. 

I had a theory that like really this one code path was the problem where we're doing the conversion from our internal tagging implementation through Hotel tags, which had a lot of other logic with it that was pretty expensive to do on pretty much every call. So I was like, you know what? There's no better time than now to begin another migration from OpenTracing to OTEL, basically. While we wait on the Hotel folks on the metric side to push out more performant code, more performant implementations for us, we can also test out this theory that if we migrate to Hotel entirely, we're actually going to see even more performance benefits. 

At that point, I said, okay, I'm going to put a pause in the metrics work while we wait for Hotel, and I'm going to begin on this tracing migration. However, I decided to try a different approach, which is the all-or-nothing approach. The OpenTracing to OpenTelemetry path was a bit more known. There were a few really small docs and examples, and they are backwards compatible. You’re able to use them in conjunction with each other. 

So first step: set up propagators. Second step: make sure that all of our plugins worked, which at the time they weren't open-sourced; now they are open-sourced, so people can just use them. I just did it all in one swoop. Maybe I had to revert like three times from our staging environment, nothing really major. There was one bug that I missed where we were previously doing a lot of in-app sampling because we had a really noisy function call. So I had to implement the custom sampler, which is actually like ten times easier with OTEL than it was with OpenTracing. I was able to get rid of a good like 1000 lines of code or so and some really dangerous hacks. That was a really good thing. 

Yeah, so then that went out. Very happy. Also, stopping at any time if we want to get back to like more Q&A stuff, but I have so many questions from this already.

**Host:** Do you mind taking a quick pause?

**Jacob:** Sure.

**Host:** A couple of comments. One thing that came to my mind as you're saying this: this is actually a really freaking cool story because, like, I think for there we tend to see like two different types of organizations. We see the ones where there's like zero code instrumented; this is like their first foray into instrumenting their code. Then we see the organizations that I think have either that have dabbled in OpenTracing. So I think it's a really cool story because this is like a real-life migration story where you can actually provide advice on this is what you can do if you find yourself in this situation, which is really cool. 

I want to call that out because I think that's a really important thing, especially if you're starting to get into OpenTelemetry. The other thing that I wanted to ask you about because you said that it's a monorepo, so in that case, did you find it, and especially since you did the all-or-nothing approach, did you find having a monorepo more challenging than if you'd been dealing with microservices instead?

**Jacob:** Yeah, well, so we do use microservices; it's just that we have like a repo of microservices. Sorry, my bad about you guys. 

### [00:10:49] Migration strategies discussion

**Host:** In that case, how do you know, because, I mean, yes, you're going for like a big bang approach, but you got to start somewhere. So then where do you start?

**Jacob:** I started with, and it was the same with how I started the metrics migration. I started with really small services that my team owned that were really low traffic but enough for it to be constant. The reason that you have to pick a service like that is if it's too low traffic, if you're only getting like one request every minute, one request every like ten minutes, you have to worry about sample rates. You might not have a lot of data to compare against. Really, that's the big thing that you need to have is some data to compare against. 

I wrote a script early on for the metrics migration that just queried different build tags that are on all of our metrics. You would say, you know, query all of the metrics for service X, grouped by release tag, and if you see that the standard deviation for the newer build tag is like, you know, greater than one, right? So if it's one or more standard deviations away from the previous release, then there’s probably something going wrong in your instrumentation library. If you assume that your metrics are relatively stable, then if they're not, it's important to know. 

The other thing I had to check for was that all of the attributes were still present before and after migration, which is another thing that matters. Sometimes they weren't because something might be something that StatsD just adds automatically that we don't really care about, and so those were acceptable. I just hand-waved and said those are fine; we don't care. 

For tracing, it’s sort of the same deal where I picked a service that had both internal-only traces that stayed within a single service and then traces that span multiple services with different types of instrumentation. Coming from Envoy to Hotel to OpenTracing, what you want to see is that the trace before has the same structure as the trace after. So I made another script that checked that those structures were relatively similar and that all of them had the same attributes as well. 

**Host:** Right, right. Because tracing attributes again, I was doing an attribute migration. That was really the point of doing the tracing one, so what matters is that all the attributes stayed the same.

**Jacob:** Yeah, it's interesting too because you're starting from the point where you were migrating from an existing thing. You have that frame of reference, which I guess is kind of a double-edged sword, right? Because on the one hand, it's like you know you pretty much know that you've instrumented the things. Hopefully, maybe you'll discover as you go along that there's more stuff to instrument, but at least you have a baseline to start from. But then I guess on the other hand, if something's missing, you're like, oh damn, why is that missing?

**Jacob:** Yeah, and those, you know, "why is this missing" stories are the really complicated ones. Because of course, sometimes it's easy to just like, you know, oh, I forgot to add this thing in this place, and that's usually pretty simple. But sometimes it's like, oh, there's an upstream library that doesn't emit the thing or Hotel, and now I need to, like, again, this is like early stuff. Most of these have all been upgraded and are fine now. 

But there is an example with like our gRPC. Actually, this is like an interesting one. I had done a gRPC migration on a gRPC util package, which I think is now in like OpenTelemetry. There was an issue with propagation. I was trying to understand, you know, what's going wrong here? And when I looked at the code, it just tells you how early in this story I was doing this migration, where there was supposed to be a propagator. There was just a TODO. Oh no. 

It was from someone on it; it was from Alex Book, which I'll call him out. I sent it to Alex, and I was like, hey, this is a funny TODO because I just, you know, took down an entire service's traces in staging. I spent some time to like fix that TODO. It wasn't that difficult; it was just that they were waiting on another thing. I mean, that's how it goes; you're waiting on someone else, which, you know, another person, it's just endless cycles of that type of thing. 

But then I got it working, so that was like one of the main blockers for us. Nice, nice. I was able to upstream it as well; it wasn't just a fix for ourselves; it was a fix for the community. 

### [00:16:23] Performance issues during migration

**Host:** And that was, yeah, there were a few things like that. A lot of the metrics work actually resulted in big performance boosts for Hotel metrics, like Hotel Go metrics. It also has given the specs folks some ideas about how descriptive the API should be or various features. 

**Jacob:** Things like views and the use of views is something that we did heavily in that early migration because we were worried about, can you just, yeah, definitely just tell folks what you mean by that.

**Jacob:** So a metrics view is something that's run inside of your metrics provider in OTEL. Your media provider in Hotel. What it's doing is it's saying you can configure it to do kind of a lot. It could just be dropping this attribute whenever you see it. If you're a centralized SRE and you don't want anybody to instrument code with any user ID attributes because that's a super high cardinality thing, it's going to explode your metrics cost, right? 

So you could just make a view that gets added to your instrumentation that says just don't let this attribute from being recorded, just deny it. So that's like probably the most common use case. 

There are other ones though, more advanced use cases for dynamically changing things like the temporality or the aggregation of your metrics. Temporality being cumulative or delta for like a counter. Am I recording zero, one, three? I'm trying to, too crazy. Zero, one, three, or am I recording one and two? 

Aggregation is going to be about how do you, I always struggle to explain all of them, and I'm trying to come back to what I had done in that moment. 

**Host:** Talk about temporality.

**Jacob:** Oh, aggregation is like, oh man, being on the spot is so much harder because it's like I want to look it up, but feel free to look it up. That's totally cool. 

Aggregation is like how you send off these metrics, basically. We had an aggregation that instead of doing, well, for histograms, this is like most useful when you record a histogram. There are a few different types of histograms. Datadog's histograms, StatsD's histograms are not a true histogram because what they're recording is like aggregation samples. 

They give you a min, max, sum, count, average. And so I actually don't even think they give you sum. I looked at this last night; they don't give you sum. They give you like min, max, count, average and like P95 or something. 

And so the problem with that is in distributed computing, if you had multiple applications that are reporting P95, there's no way that you could get a true P95 from that observation with that aggregation. The reason for that is that in order to get P95, you can't, if you have five P95 observations, there's not an aggregation to say give me the overall P95 from that, right? 

You need to have something about the original data to actually recalculate it. You could get the average of the P95s, but that's not a great metric; that's not really telling you much. It's not really accurate. And if you're going to alert on something, if you're going to page someone at night, you should be paging on accurate measurements. 

Initially though, we did have a few people who relied on this min, max, sum, count metric. So we used Hotel views in the metrics SDK to configure custom aggregation for our histograms to emit what some would call a distribution, what OTEL calls an exponential histogram, or the min, max and the min, max and count. 

So we were dual emitting this. This works because they're different metric names that we were emitting, so there was no overlap between them. So what we did was we migrated after we did the metrics migration. We were able to then go back and say any dashboard, any alert, anything that was using a min, max, sum, count metric, just change it to be a distribution instead. 

Because we had enough data in the past, like, you know, a few weeks, months of running Hotel metrics in our public environment, that was possible to do. So that was like one of the key features because we had it, it was ten times easier. And we were able to do it from the application. We didn't have to introduce any other components, which is pretty neat.

**Host:** Right, right. Cool. Another question that I had for you. So like when you were doing this migration, traces and metrics were in existence; logs, I believe would not have been, like, the specification would not have been ready, possibly not even in the works.

**Jacob:** No, it was still really early for that, so, but I guess in lieu of logs, there's span events that you could use. So it's not something like that was leveraged as well?

### [00:22:37] Discussion on metrics aggregation

**Jacob:** Definitely. We've heard a long time have you used span events analogs, or a lot of things internally. I'm a big fan of them. I am not a huge fan of logging; I find it to be really cumbersome and really expensive. IOPS for like tracing and trace logs whenever possible, I find it easier for myself to reason about. 

There are other people who are logging first, and that's great, but that's just not who I am. I like logging for local development and tracing for distributed elements; that makes sense. But we use this heavily. That was one of the first things that I checked worked. 

It's actually an interesting bug where we had some custom code or OpenTracing that allowed us to serialize JSON blobs in the span events, and that stopped working because we didn't emit them in the same way. It's like a little hazy, but I had to like rewrite a processor to make that work and then update some downstream code like in Lightstep as a platform to Facebook.

**Host:** Cool. Now, how about keeping that in mind? Now that logs are more mature, are there any plans to do any conversions? And please correct me if I'm wrong, but my understanding too is that like with the log specification maturing more and more, that like span events are going to be replaced by logs in some form. Like, it's going to be the log specification for span events. Have you heard anything around that?

**Jacob:** No, this is a bit outside of where my recent focus has been, so I'm not positive. I think right now the way that we do, I think the thing that we would change is how we collect those logs potentially. Right now we use, how do we do this right now? It changed recently; I don't want to say something incorrect. 

We previously did it by just using like Google's logging agent, where they basically are running like Fluent Bit on every node in a GKE cluster, and then they send it off to GCP, and they just like tail it there. I think this changed though, and I'm not sure what we do now.

**Host:** Okay, cool, cool. Speaking of GKE, I have many questions on GKE specifically. Do you, because I believe there's like a feature now in newer versions of Kubernetes where there's like some, I think there's some telemetry collection; do you know if that's been enabled in any of the clusters?

**Jacob:** Yeah, so I think that Kubernetes now has the ability to emit like the Hotel traces natively.

**Host:** Yeah, yeah.

**Jacob:** I'm not sure if we're collecting those yet. I don't know what version; that's more of a question for the SREs. I don't, Kubernetes came out like I think even last year, starting whatever, like last fall kind of thing. That's a really good question that I want to look into because I want to see if really what I would like to do is see if we can collect the traces that we get from those are enough to use the span metrics processor to generate better Kubernetes metrics from those traces. 

I'm very focused on like infrastructure metrics, like Kubernetes infrastructure metrics, and I find them to be very painful in their current form. It would be really cool. Right now, I prefer to use the Prometheus APIs for them currently. It's just a bit more ubiquitous in the observability community to use Prometheus to do that.

**Host:** Right, right. Go ahead.

**Jacob:** No, go ahead. I'll let you complete the thought; maybe it answers my questions. 

**Jacob:** And so that's what we do right now, and I use the target allocator, which is a nutshell component that I work on, to distribute those targets, which is a pretty efficient way of getting all that data. We also use like daemon sets as well that we run in our clusters to get that data in addition to that. So that works pretty effectively. 

The thing that's frustrating is just Prometheus. Prometheus script failures can be a super common problem, and it gets really annoying when you have to worry about metrics cardinality, as well, because it can explode. 

I actually found a bug in GKE maybe six to eight months ago, six, seven months ago, but they've since fixed where they weren't deleting or reconciling certificate signing requests in their Kubernetes cluster, which meant that for Kube State Metrics, which reports on cluster state, it was omitting because there were so many certificate signing requests left from these abandoned nodes. 

Something on the magnitude of like six hundred thousand for like a single metric, which is huge. And so then Prometheus, the Prometheus that I was running fell over because of that. And that's like a thing that happens constantly in this Prometheus realm, which is just like someone emits a high cardinality metric, Prometheus goes to scrape it, and then it just crashes.

**Host:** Oh wow.

**Jacob:** Which isn't fun.

**Host:** Yeah, that does not sound fun. I want to just take a step back because you mentioned the target allocator. I was wondering if you could expand a little bit on that because I know like we actually had one of our previous Q&A folks also mention the target allocator. That was the first time I had heard of it, so I think it'd be super helpful to just get a little overview.

**Jacob:** Sure. Yeah, so the target allocator is a component, part of the Kubernetes operator in Hotel that does something that Prometheus can't do, which is dynamically shard targets amongst a pool of scrapers. Prometheus has some experimental functionality for sharding, but you still have a problem for querying because Prometheus is a database, not just a scraper. 

If you shard your targets, you don't necessarily—you have to do some amount of coordination within those Prometheus instances, which gets expensive. It's like a very experimental feature, or you could scale Prometheus with something like Thanos or Cortex, which is Grafana's Prometheus scaling solution, I think, right? 

Which works, but you just then have to run like six more components that you then need to monitor, and then if those go down, how do you monitor? It's all these other problems. 

In Hotel, we just basically tack on this Prometheus receiver to get all this data, but because we want to be more efficient than Prometheus, because we don't need to store the data, we tell—we have this component, the target allocator, which goes to do the service discovery from Prometheus. So it says, give me all of the targets that I need to scrape. 

Then the target allocator says, with those targets, distribute them evenly amongst the set of collectors that are running.

**Host:** Oh, okay.

**Jacob:** So that's the main thing that it is doing. It does some more stuff around job discovery now, or if you're using Prometheus service monitors, which is part of the Prometheus operator, which is a very popular way of running Prometheus in your cluster. It's what a lot of vendors use as well. 

### [00:31:28] Target allocator explanation

So if you're on GAE or OpenShift, I think both of those natively used service monitors and pod monitors. The target allocator can also pull those service monitors and pod monitors and update the collectors' scrape configs to do that.

**Host:** Oh, cool. That's awesome. And so related to the Prometheus thread, are you running like Prometheus itself or are you just scraping the Prometheus metrics and pumping them through to the collector, then?

**Jacob:** Exactly right. Just no Prometheus instances, just the collector running Prometheus receiver and then sending them off to Lightstep.

**Host:** Oh, living the dream. That was always like that was always my dream.

**Jacob:** That's awesome.

**Host:** That's nice. Do you use, like, because I remember Lightstep has like a Prometheus operator that helps facilitate that, so we used to have this thing.

**Jacob:** Yeah, we used to have this thing called the Prometheus sidecar, which you might run as part of your Prometheus installation, which would then sit on the same pod as your Prometheus instance and read the write-ahead log that Prometheus has for persistence and batching and all these other things. 

So we would read the write-ahead log and then forward those metrics, but if your Prometheus is very noisy, as many customers have very noisy Prometheus statistics, it's not really efficient. It can get really noisy, and it's not that—what's the word? It's not the best thing to run. The collector is like the best way to run.

**Host:** Okay, so and it sounds like this thing still requires Prometheus to be installed?

**Jacob:** Yeah, you would still need to be running a whole computer system.

**Host:** Oh, okay. I thought it was—I was under the impression it was like a replacement for Prometheus and that it was—maybe I'm thinking of something else. 

**Jacob:** There was a thing that I knew of that was like a replacement for needing Prometheus, and it was like vendor-neutral, so it wasn't like, oh, you have to use Lightstep to use this thing. I think I might just be a Hotel operator collector target allocator trio.

**Host:** Oh, okay, okay. 

**Jacob:** But maybe there's another thing out there.

**Host:** Oh, unless maybe that got integrated into like the target allocator as part of—anyway, it is a mystery.

**Jacob:** Yeah.

**Host:** Okay, cool, cool. Oh, that's awesome. So then, okay, since we're talking collectors now, I, for me, the two million dollar question, the one that I'm always curious about is collector setup. So what is the collector setup that y'all have chosen? What works for now?

**Jacob:** It's hard to say because we run a lot of different types of collectors. Yeah, at Lightstep, we run like metrics things, tracing things, internal ones, external ones. There are a lot of different collectors that are running at all times. 

You have like a separate one that just collects metrics and one that just collects traces. Right now, we don't—it's all varying flux. Right now, we're changing this a lot to run experiments and stuff. Basically, like the best way for us to be able to make features for customers and end users is by running them ourselves and then using them internally, making sure that they work, and then sending them for the open-source realm, and so that's what we're trying to do even more of. 

We're kind of reaching a point where we dogfood everything, which gets really confusing because you have to like—

**Host:** Yeah, I can imagine.

**Jacob:** —we're running like in a single path. There could be like, I think, two different collectors in two environments that could be running two different images in two different versions. It gets really meta and very confusing to talk about.

**Host:** Yeah, I can imagine. And then, you know, if you're sending from collector A across an environment to collector B, collector B also emits telemetry about itself, which is then collected by collector C. 

**Jacob:** Yeah, it's just chains like you basically ensure that you have to make sure that the collectors actually work. 

**Host:** You have to be sure that everything along this path—yeah, you just have to know which thing has the data.

**Jacob:** Right. Well, you shouldn't have to; we do that for you, but like, right. 

**Host:** Yeah, and we make like dashboards to help with that, but that's like the problem. When it's like we're debugging this stuff, when there's a problem, you have to think about like where's the problem actually? Is it in how we collect the data? Is it in how we emit the data? Is it in, you know, the source of how the data was generated? It's, you know, one of like a bunch of things.

**Jacob:** Yeah.

**Host:** Now, like you need to work on the Hotel operator, and I've been reading up on the operator recently. There's like, I think, four different deployment modes, right? There's sidecar deployment, daemon set, and what's the other one?

**Jacob:** Stateful set.

**Host:** Yes, yes. My question is, which mode, or is it like all of the above depending on the thing that you need to do?

**Jacob:** Yeah, it's all the above depending on what you need to do and your general needs and how you like to run applications for like reliability and stuff. 

So sidecar is the one that we use the least and is probably used the least. If I were to make just like a bet, sidecars are really useful across the industry. You would think?

**Host:** Yeah, across the industry, I'd be willing to bet that those are the least popular—those are the least popular method. Sidecars are just expensive, and if you're not using them, if you don't really need them, then you shouldn't use them. You'll really only need them like something that's run as a sidecar, it's like Istio, which makes a lot of sense to run as a sidecar because it's doing like traffic proxy hooks into your container network to change how that all does its thing. 

You get a performance hit if you sidecar your collectors, right? For all your services, you just get like a cost; it would just cost you a lot more, and you also wouldn't be able to do as much with like, if you're making like Kubernetes API calls for attribute enrichment, that's like the thing that would get exponentially more expensive if you're running it as a sidecar. 

But as like a stateful set of, you know, five pods, that's not that expensive, but if you have a sidecar on like 10,000 pods, then that's 10,000 API calls made to the Kubernetes API. 

### [00:39:40] Collector setup discussion

**Host:** Right, right. What would be the advantage of running your collector as a stateful set versus a deployment? I guess what's the state that you would want to persist?

**Jacob:** This is an important thing to know for not just like how the collector runs as an application, but how like your applications can run. Stateful sets have consistent IDs. If you have a stateful set with 10 replicas, they're all going to be the stateful set name dash counter number, so okay, that's a really valuable thing when you want consistent IDs, right? 

As opposed to like with deployments, like when you like your pods are all like random, random crap, right? So the pod IDs are done where it's deployment name dash replica set ID dash pod ID, right? And so with stateful sets, because we have this consistent IDs, we can actually do some extra work for the target allocator, which is why we require that. 

The other thing that stateful sets guarantee is what's called an in-place deployment, which is what daemon sets do as well, where you take the replica, you take the pod down before you create a new one. The reason that this is important is that in the deployment, you normally do a one up, one down, right? 

Or what's called a rolling deployment, a rolling update. If we were to do this for the target allocator, we would probably get much more unreliable scrapes because you would—and someone actually just asked this question in the operator channel. I'm going to give them this exact response. When a new replica comes up, you have to redistribute all the targets because your hash ring that you place these on is changed. 

If you're doing a rolling deployment, if you're doing one up, one down, that's a really expensive operation because then you have to recalculate all these hashes that you assign. So if you were to do a one down, one up, you would still have to redo this whole thing because you would lose a pod, which means it's taken out of the ring, redistribute, you would gain a new ID, and then you'd have to redistribute again, right? 

Whereas stateful sets, because it's a consistent ID range, you don't have to do that at all. This means that when we do a one down, one up, it keeps the same targets each time.

**Host:** Right, right. So it's almost like a placeholder for it, like you don't have to recalculate the ring basically because—

**Jacob:** Yeah, it's just sort of like a little, what's it called? 

**Host:** Yeah, yeah, yeah. I can't think of the word. 

**Jacob:** Cool, that's really neat. I didn't know that. 

**Host:** It was funny because I was reading about like pods being deployed as stateful sets. I'm like, straight or sorry, not collectors. I'm like, I straight up do not understand what the use case would be, but this makes a lot of sense. So that's really cool, and so it's not as useful, or this is really only useful for like a tracing use case, I would say, or sorry, metrics use case where you're doing complete test scores. 

We would probably run it as a deployment for anything else because a deployment gives you everything that you need pretty much. The collectors are stateless; they don't need to hold on to anything. 

Deployments are much more lean as a result. 

**Host:** Yeah, they can just run and roll out and everybody's happy, and that's how we run most of our collectors, deployment. And then at what point would a daemon set be useful?

**Jacob:** Yeah, so daemon sets are really good for things like node scraping, which we do a lot of. So this allows you to scrape like the kubelet that's run on every node. It allows you to scrape the node exporter that's also run on every node, which is another Prometheus daemon set that most people run. 

Yes, the daemon sets guarantee that you've got odds running on every node, right? Exactly. Every node that matches its selector, right?

**Host:** Right, right.

**Jacob:** And so that's really useful for like scaling out. If you have a cluster of like 800 plus nodes, it's more reliable to run like a bunch of little collectors that get those tiny metrics rather than a few bigger stateful set pods. Because your blast radius is much lower, so if like one pod goes down, you lose like just a tiny bit of data. 

But remember, like with all this cardinality stuff, that's a lot of memory. If you're doing like a stateful set scraping all these nodes, that's a lot of targets, that's a lot of memory; it can go down much more easily, and you lose more data. 

Luckily, the collector isn't like Prometheus, where we don't care about that state. So if a collector goes down, it comes back up super fast, so usually the blip is low. But it does mean that the blip is more flappy, right? Where like it could go up and down pretty quickly if you're past the point of saturation. 

That's why it's good to have like a HPA (Horizontal Pod Autoscaler) on that stuff, but still, daemon set is a bit more reliable.

**Host:** Right, and it sounds like it would be useful again from a metrics standpoint.

**Jacob:** Yeah, yeah. Tracing, you could do it for tracing and just send it on like a node port. But tracing workloads, again, because it's all push-based, they are much easier to scale on, and you can distribute targets; you can load balance. There are all these other benefits that we get from push-based workloads. 

Pull-based is like the reason that Prometheus is so ubiquitous in my opinion is just because it makes local development really easy, where you just can scrape your local endpoint, and that's what most back-end development is anyway. 

So you could hit endpoint A and then hit your metrics endpoint and then hit endpoint A again for the metric standpoint. You can just check that; it's like a very easy developer loop. 

### [00:47:12] Use of stateful sets vs deployments

It also means that you don't have to reach out outside of the network, so if you're a really strict proxy requirements to send data, local dev is much easier for that. 

That's why like Hotel now has like a really good Prometheus exporter, so you could do both.

**Host:** Right, right. And then I'm assuming there's a centralized gateway somewhere or—

**Jacob:** This is part of the collector chain that I was talking about. Again, we're running a lot of experiments. I can like half talk about it.

**Host:** Okay.

**Jacob:** I can be vague. A big effort within Hotel right now is around Arrow, which you might have been hearing about some. There's been some work done by Lightstep and F5 to improve the processing speed and egress and ingress costs of OTEL data by using Apache Arrow, which is a project for columnar-based data representations. 

We're just like doing some proof of concepts or like proof of implementation work to see what the actual performance of this stuff looks like, right? 

And also, you know, check that everything works as expected, yeah, as well, which it is, but you always have to check.

**Host:** Yes, absolutely. Well, I'd say the main takeaway from this whole story on collectors is like it sounds like it's always going to be an evolving game, which is not a terrible thing to do.

**Jacob:** No, it's important that you keep your telemetry up to date. I think that like library authors and maintainers are like constantly working on new performance features and new ease of use, like quality of life stuff as well.

**Host:** Yeah, yeah. Especially with OTEL, like we talk about quality of life a lot. 

**Jacob:** Yeah, and so that is definitely a focus, and that's why it's important to keep up to date. It makes migrations easier as well. Trying to migrate from like an ancient version of something to the latest version, you're probably missing a lot of breaking changes potentially, and you have to be careful of that.

**Host:** And on that vein, then how do you ensure that everyone's keeping up to date with the latest versions of Hotel across the org?

**Jacob:** I think like stuff like Dependabot is pretty good. We use it internally, or not internally, we use it in Hotel. We're keeping up to date with Hotel stuff, so I find it to be really helpful. It's very frustrating sometimes because it's like Hotel packages all update in lockstep pretty much; that means you have to update a fair amount of packages at once, but it does do it for you, which is pretty nice.

**Host:** That's nice. That's nice. But you should be doing this not just for Hotel but like any dependency. Right? Like CVEs happen in the industry constantly, and if you're not staying up to date with vulnerability fixes, then you're opening yourself up to security attacks, which you don't want.

**Jacob:** Yeah, yeah, do something about it is my recommendation.

**Host:** That's fair, that's fair. I know we've got like four minutes left. Do you want to give us any like parting thoughts as we wrap up? 

I'm interested to hear if there are any questions from the group that we've missed in our discussion here.

**Jacob:** Any takers with burning questions? Now's the time. If not, I'm going to call on Rhys. But this was great. 

I feel like I was like rapidly trying to take notes. I probably will have more as I try to go back and tidy up some of my notes. But yeah, honestly, I was just trying to keep up with people; there was so much information. 

### [00:52:07] Wrap-up and parting thoughts

I feel like because I've been reading up on the operator the last week, and so like more questions than answers, and I feel like I have some answers now. 

I've definitely learned a lot. I hope folks on this call have learned a lot as well. I think this is like really great information. I think it gives folks an idea of what's involved in a migration, things to consider, like when you're setting up your collectors, things to avoid doing, keeping up with those latest versions of Hotel—y'all always a good thing.

**Jacob:** Yeah, that's the big one is that like new fixes really often. 

**Host:** Yeah, yeah, like new versions every two weeks for the collector, I'd say it's like a pretty frequent cadence. There are some Prometheus libraries that like don't update like ever—like Thanos hasn't released since March, right? Which is absurd to me because there's like a bug in compatibility between Prometheus and Thanos right now.

**Jacob:** Oh really? How cheap.

**Host:** Yeah, so you can use them together if you're like coding, so not fun.

**Jacob:** Yeah, damn. Alright, last chance for burning questions y'all. 

**Erica:** Thanks for the info and your time, Jacob.

**Jacob:** Thank you, Erica, for hopping on. 

**Host:** Yeah, this was really awesome. Thank you so much because like for real, this is a dope topic. 

**Jacob:** We'll, you know, reach out to your friendly neighborhood CFP approvers.

**Host:** No, thank you so much. I feel like there was so much more we could have chatted about as well, so we might—

**Jacob:** Yeah, yeah. Maybe I can, if I get accepted for the talk, then I can give a sneak peek at least. 

**Host:** I can get some feedback on it. Actually, you know what? Like if you're interested, because we have Hotel in Practice, which is kind of like giving a little talk. So if you're interested in doing that even like before you find out whether or not you get accepted, we are happy to have you.

**Jacob:** Yeah, it sounds great. Cool, I'm in.

**Host:** Cool, cool. We'll figure out the behind the scenes on like when to schedule you and—

**Jacob:** Sounds good.

**Host:** Yay, cool. Alright, and Daniel said thanks, Jacob, as well. And yeah, we're at the top of the hour. Thanks for joining us.

**Jacob:** Thank you all.

**Host:** Thank you, everyone. Bye.

## Raw YouTube Transcript

thank you welcome everyone to Hotel q a thanks for joining um today we are super lucky to have Jacob aaronoff from lightstep from service now join us um it's it's a cool treat because um like I tapped Jacob because he was telling me that he had submitted a cfp to one of the I think it was cubecon right Jacob yeah it was a cfp on like um migrating to Hotel um from like our or like our organization migrating at hotel right if I if I understand correctly which I thought that's pretty freaking cool story to tell so um so I asked them to to join and and have this q a and um and hear the story because I think it's it's I think it makes for a compelling narrative like when an observability vendor talks about using otel themselves on their own products so here we are um so welcome Jacob um do you want to do like a quick little intro sure um yeah hi my name is Jacob arnoff I'm a staff engineer at lightstep um on the Telemetry pipeline team um I've been to lead stuff for almost two years uh and the first like year of that Journey was solely focused on like Hotel migrations um making doing them internally and making them easier for customers um sort of that whole process um so yeah I could get into this anyway that is interesting how do you think do we want to do this very q a style or should I just go right into the story talk about um we can we'll we'll let it evil organically about um cool yeah I like that why don't we start like um I mean I think you're you're rearing to go so maybe why don't you describe like set the scene for us so um when I joined lightstep uh we were still on open tracing for uh tracing um and then a mix of open census and some hand rolled stats key stuff or metrics so this meant that we had to run a proxy on every single pod that we ran in kubernetes and a proxy since it's a sidecar on every pod which means that every single time you run it one of your applications you have to run another little application that's going to read from statsd and then forward those metrics off um and you know for us as we're building out a metric solution that destination was light step so I came in and this was sort of at the beginning of like metrics Alpha I think um and I was like hey it would be great for us to we know that um open uh we know that like hotels hotel for metrics is going to be a huge effort for us in the next like year or two we wanted to reach stability um the hotel team had been we have like an Hotel team internal light step they've been working on it a lot and really wanted some um immediate feedback on how to improve it so I took on that migration for us we also had the theory that doing so would save us you know good chunk of money because we would no longer need to run these relatively expensive stats key um uh stats the sidecars so I planned it initially um to be sort of as safe as possible I'd done some migrations like this in the past um and there are a few different ways that you can do migrations like this you can do the all-in-one go uh which for us would have been possible we're in a mono rebuild um but it's much more dangerous because you you worry about you know am I going to push a bug that's going to take everything down right um obviously this is application data that it's data about your applications which we use for alerting we used to understand you know how our workloads are functioning um in all of our environments and so it's important that we don't take that down because that would be disastrous for us but obviously for you know an end user is going to be the same story they don't they want the comfort that if they migrate to Hotel they're not going to lose all of their alerting capabilities immediately they're not like you want a safe and easy migration so that's the only one with our initial approach for doing a sort of like feature flag based um just like part of that can part of the configuration that you run in kubernetes um it would disable this sidecar enable some code that would then swap to otel for metrics and then forward it off you know where it's supposed to go um so that was sort of the path there um Midway through this journey of doing these migrations I had tested it all out and staging looks pretty good I tested the container in our meta environment so we use to monitor our public environment um I noticed some pretty large performance issues um in those 20 2021 and I had reached out to the hotel team and we had worked together to sort of alleviate some of those concerns one of the ones that we found that was a big blocker was we heavily use attributes on metrics right now and it was incredibly tedious to go in and figure out why you know figure out which metrics are using all these attributes and getting rid of them so um well I had a theory that like really this one code path was the problem where we're doing the conversion from our internal tagging implementation through Hotel tags which had a lot of other logic with it that was pretty expensive to do on pretty much every call so I was like you know what this there's no better time than now to begin another migration for her open tracing to otel basically while we wait on the hotel folks on the metric side to push out um more performant code more performant implementations for us we can also test out this theory that if we migrate the hotel entirely we're actually going to see even more performance benefits so at that point I said okay I'm going to put a pause in the metrics work while we wait for hotel and I'm going to begin on this tracing migration nutrition migration however um I decided to try a different approach which is the All or Nothing approach basically um the open tracing to open Telemetry path was a bit more known there were a few really like small docs and examples and they are backwards compatible like you're able to use them in conjunction with each other so one thing admitting Hotel one thing emitting um open tracing is not the end of the world so you can mix those as long as you have the propagators set up correctly so first step so propagators Second Step um make sure that all of our plugins worked which at the time they weren't open sourced uh now they are open source so people can just use them um and I just did it all in one swoop maybe I had to revert like three times from our staging environment nothing really major um and then there was one bug that I missed where um we were previously doing a lot of in-app sampling because we had a really noisy function call um so I had to implement the custom sampler which is actually like 10 times easier with otel than it was with open tracing I was able to get rid of a good like 1000 lines of code or so and some really dangerous hacks so that was a really good thing um and yeah so then that went out very happy um also stopping at any time if this if we want to get back to uh like more q a stuff but I I have so many questions from this already but do you mind do you mind taking like a quick pause because I I'm like sure um okay so a couple comments so one thing that that came to my mind as you're saying this I'm like this is actually really freaking cool story because like I think for there we we tend to see like two different types of organizations right we see the ones where there's like zero code instrumented like they are this is like their first foray into instrumenting their code and then we see the the organizations that I think have either that have like dabbled in open tracing so I think it's a really cool story because this is like a real life migration story where you can actually provide advice on this is what you can do if you find yourself in this situation which is really cool um so I wanna I wanna call that out because I I think um I think that's a really important thing um especially if you're like starting to get into open Telemetry um the other thing that I wanted to ask you about because uh you said that it's um that it's a monorepo um so in that case did you find it and especially since you did the All or Nothing approach um did you find having a mono repo more challenging than if you'd been dealing with microservices instead um yeah well so we we do use microservices it's just that we have like um repo of microservices sorry my my bad about you guys um in that case yeah then um how do you um how do you know like because I mean yes you're going for like a big bang approach but you got to start somewhere so then like where where do you uh where do you start so I started with um and it was the same with how I started the metrics migration um I started with really small services that um my team owned that were really low traffic but enough for it to be constant um so the reason that you have to that you want to pick a service like that is if it's too low traffic if you're only getting like one request every minute one request every like 10 minutes um you have to worry about sample rates um you might not have a lot of data to compare against really that's like the big um thing that you need to have is some data to compare against I wrote a script early on for the metrics migration that um just queried uh different build tags um that are on all of our metrics so you would say you know query all of the metrics for service X um grouped by release tag and if you see that the standard deviation for the newer build tag is like you know greater than one right so if it's one or more standard deviations away from the previous release then there's probably something going wrong in your instrumentation Library right if you assume that your metrics are relatively stable then if they're not it's important to know um the other thing I had to check for was that all of the attributes were still present before and after migration which is another thing that matters sometimes they weren't because something might be something that stats T just adds automatically that we don't really care about and so those were acceptable I just like hand waved instead those are fine we don't care um for tracing is sort of the same deal where I picked a service that had um both internal only traces traces that stayed within a single service and then traces that um span multiple services with different types of instrumentation so coming from like Envoy to hotel to open tracing and what you want to see is that the trace before has the same structure as the trace Factor so I made another script that checked that those structures were relatively similar and that all of them had the same attributes as well right right um because tracing attributes again I was doing an attribute migration that was really the point of doing the tracing one so what matters that all the attributes um stayed the same right yeah it's interesting too because like because you're you're starting from the point where you were migrating from an existing thing like you have that frame of reference which I guess is kind of a double-edged sword right because on the one hand it's like you know you pretty much know that you've instrumented the things hopefully maybe maybe you'll discover as you go along that there's like more stuff to instrument but at least like you have a baseline to start from but then I guess on the other hand if something's missing you're like oh damn why is that missing right yeah and those you know why is this missing stories or the really complicated ones because of course sometimes it's easy to just like you know oh I forgot to add this thing in this place and that's usually pretty simple but sometimes it's like oh there's an upstream library that doesn't admit the thing or hotel and now I need to like again this is like early stuff most of these have all been upgraded and are fine now um but there is an example with like our grpc um actually this is like an interesting one um I had done a grpc I migrated on a grpc util package which I think is now in like token trip um the like Hotel goken trip um and there was an issue with propagation I was trying to understand you know what's what's going wrong here and when I looked at the code and it just tells you how early in this story I was doing this migration um where there was supposed to be a propagator there was just a to do oh no um so and the city was from someone on it was from Alex book which I'll call him out um and so I I sent it to Alex and I was like hey this is a funny to do because I just you know took down um an entire Services traces and staging um so I spent some time to like fix that to do so it wasn't that difficult it was just that they were waiting on another thing I mean that's how it goes you're waiting on someone else which you know another person it's just endless cycles of that type of thing yeah um but then I got it working so that was like one of the main uh blockers for us nice nice and was able to Upstream it as well it wasn't just a fixed for ourselves it was a fix for the community and so that that was yeah there were a few things like that um a lot of the metrics work um actually resulted in big performance boosts for um Hotel metrics like Hotel go metrics and um it also has given the specs folks like some ideas about how descriptive the API should be or various features so things like um uh views and the use of views is something that we did heavily in that early migration because we were worried about can you just yeah definitely just tell folks what you mean by that yeah so um of metrics View is something that's run inside of the your metrics provider in otel your media provider in hotel so what it's doing is it's saying you can configure it to do kind of a lot it could just be um drop this attribute whenever you see it if some if you're a centralized SRE and you don't want anybody to instrument code with any user ID attributes because that's a super high card analogy thing it's going to explode your metrics cost right so you could just make a view that gets added to your instrumentation that says just don't let this attribute from being recorded just deny it um so that's like it's probably most common use case um there are other ones though more advanced use cases for dynamically changing things like the temporality or the aggregation of your Matrix so temporality being cumulative or Delta for like a counter um I you know am I recording um zero one three I'm trying to two crazy zero one three or am I recording one and two right right um and then your uh aggregation is going to be about how do you um I these things are so I always struggle to explain all of them and I'm trying to like come back to uh what I had done in that moment um talk about temporality oh aggregation is like um oh man being on the spot is so much harder because it's like I want to look it up but feel free to look it up that's totally cool well aggregation is like how you um like send off these metrics basically we had an aggregation that instead of doing um well for histograms this is like most useful when you record a histogram there are a few different types of histograms um datadog's histograms stats of these histogram is not a true histogram because what they're recording is um uh like aggregation samples so they give you a min max sum count average um and so I actually don't even think they give you some I looked at this last night they don't give you some they give you like min max count to average and like P95 or something um and so the problem with that is in distributed computing if you had multiple applications that um are reporting of P95 the there's no way that you could get a true P95 from that observation with that um aggregation um the reason for that is that in order to get P95 like you you can't if you have five p95s oh five P95 observations there's not an aggregation to say give me the overall P95 from that right you need to have something about the original data to actually recalculate it you could get the average of the p95s but that's not a great um metric that's not really like it doesn't really tell you much it's not really accurate um and if you're going to alert on something if you're going to page someone at night you should be paging on accurate measurements yeah um so initially though we did have a few people who relied on this min max some counter instrument so we used um Hotel views in the metrics SDK to configure custom aggregation for our histograms to do emit what some would call a distribution um what Oto calls an exponential histogram um or the min max and the Min Maxim count so we were dual emitting this works because they're different metric names that we were emitting so there was no overlap between them so what we did was we migrated after we did the metrics migration we were able to then go back and say any dashboard any alert anything that had was using a min max some count metric um just change it to be a distribution instead and because we had enough data in the past like you know a few weeks months of running Hotel metrics in our public environment that was possible to do um so that that was like one of the key features that because we had it it was 10 times easier um and we were able to do it from the application uh we didn't have to introduce any other components which is pretty neat right right cool um um another question that I had for you um so like when you were doing this migration it was traces and metrics were in existence logs I believe would not have been like the specification would not have been ready possibly not even in the works no it was still really early for that so um but I guess in lieu of logs there's span events that you could use so it's not something like that was leveraged as well definitely um we've heard a long time have you used span events analogs um or a lot of things um internally um I'm a big fan of them I am not a huge fan of vlogging I find it to be really cumbersome and really expensive um and iops for like tracing and Trace logs whenever possible I find it like easier for myself to reason about there are other people who are like logging first and that's great but um that's just not who I am um I like I like logging for local development and tracing for distributed elements that makes sense um but we use this heavily that was one of the first things that I checked worked um It's actually an interesting bug where we had some custom code or open tracing that allowed us to serialize like Json blobs in the span events um and that stopped working because we didn't emit them in the same way uh it's like a little hazy but um I had to like rewrite a processor to make that work and then update some Downstream code like in lightstep as platform to Facebook cool so now how about um keeping that in mind like now that logs are more mature is there are there any plans to do any conversions like and and please correct me if I'm wrong but my understanding too is that like with the log specification like maturing more and more that um like span events are going to be replaced by logs in some form like it's going to be the log specification for span events have you heard anything around that like no this is a bit outside of um where my recent Focus has been so I'm not positive um I think right now the way that we do I think the thing that we would change is how we collect those logs potentially um right now we use um uh how do we do this right now it changed recently I don't want to say something incorrects but um we previously did it by just using like Google's logging agent where they basically are running like fluent bit on every um node in a in the gke cluster yeah and then they send it off to like gcp and they just like tail it there um I think this changed though and I'm not sure what we do now okay cool cool um speaking of kcp of many questions on gke specifically like so um do you um because I I believe there's like a feature now in like newer versions of of kubernetes where there's like some um I think there's there's like some Telemetry collection do you know if that's been enabled in any of the uh in any of the Clusters yeah so I think that kubernetes now has the ability to emit like the hotel traces natively yeah yeah yeah um I'm not sure if we're collecting those yet um I don't know what version that's more of a like a question for this sres I I don't um kubernetes I came out like I think even last year starting whatever like last fall kind of thing yeah that's a really good question um that I want to look into because I want to see if uh really what I would like to do is see if we can collect the if the traces that we get from those are enough to use the spend metrics processor to generate like better kubernetes metrics from those traces um I'm very focused on like infrastructure metrics like kubernetes infrastructure metrics um and I find them to be very painful in their current form um and it would be really cool right now um I prefer to use the Prometheus apis for them currently um it's just a bit more ubiquitous in the like observability Community to use Prometheus to do that um just because like that's that's what kubernetes like natively emits right right go ahead oh uh no go ahead I'll let you complete the thought maybe it answers my questions um and so that's what we do right now and I use the target allocator which is you know nutshell component that I work on um to distribute those targets which is you know pretty efficient way of getting all that data um we also use like demon sets as well that we run in our clusters to get that data um in addition to that so that works pretty effectively the thing that's frustrating is just Prometheus um Prometheus script failures can be um a super common problem and it gets really annoying when you have to like worry about metrics cardinality um as well because it can explode yeah I actually found a bug in gke maybe six to eight months ago six seven months ago but they've since fixed where they weren't deleting uh they weren't reconciling certificate signing requests in their kubernetes cluster which meant that for Kube State metrics which reports on cluster State um it was oming because there were so many um certificate signing requests left from these abandoned nodes and so something on the magnitude of like six hundred thousand for like a single metric which is huge and so then Prometheus the Prometheus that I was running fell over because of that um and that's like a thing that happens constantly in this Prometheus realm which is just like someone admits a high cardinality metric Prometheus goes to scrape it and then it just like crashes oh wow um um which isn't Fun yeah that does not sound fun um I want to just take a step back because you you mentioned the target allocator I was wondering if you could expand a little bit on that because I know like we we actually had one of our previous q a folks also mentioned the target allocator um that was the first time I had heard of it so I think it'd be like super helpful to just get a little overview sure yeah so apparent allocator is component um part of the kubernetes operator um in hotel that does something that Prometheus can't do um which is uh dynamically Shard targets amongst a pool of scrapers so previous has some experimental functionality for sharding but you still have a problem for um querying because Prometheus is a um database not just a scraper yeah um if you Shard your targets you don't necessarily you have to do some amount of coordination within those Prometheus instances which gets expensive it's like a very experimental feature um or you could scale Prometheus with something like Thanos or cortex which is um grafana's Prometheus scaling solution I think right yeah which works but you just then have to run like six more components that you then need to Monitor and then if those go down how do you met a monitor it's all these other problems right right um an hotel we just basically like uh tack on this Prometheus receiver to get all this data but um because we want to be more efficient than Prometheus because we don't need to store the data we tell we have this component the target allocator which goes to do the service discovery from Prometheus so it says give me all of the targets that I need to scrape and then the target allocator says um with those targets distribute them evenly amongst the set of collectors that's running oh okay um so that that's the main thing um that it is doing it does some more stuff around um job Discovery now or if you're using Prometheus service monitors which is part of the Prometheus operator which is a very popular way of like running Prometheus in your cluster um it's what a lot of like vendors use as well so if you're on GAE or openshift I think both of those natively used service monitors and pod monitors oh so the target allocator can also pull those service monitors and pod monitors and uh update the collectors scrape configs to do that oh cool it's awesome um and so one related to the Prometheus thread um are you running like Prometheus itself or are you just scraping the Prometheus metrics and pumping them through to the collector then exactly right just um no Prometheus instances just um The Collector running Prometheus receiver and then sending them off to light step oh living the dream that that was always like that was always my dream that's awesome that's nice um do you use like because I I remember like lights up has like a Prometheus um like a Prometheus operator that uh helps facilitate that so we used to have this thing yeah we used to have this thing called the Prometheus sidecar which you might run yeah you would run it as part of your Prometheus installation which would then sit on the um on the same pod as your Prometheus instance and read the uh write ahead log that Prometheus has for like um persistence and uh batching and all these other things yeah and so we would read the right ahead log and then forward those metrics but if your Prometheus is very noisy as many customers have very noisy Prometheus statistics yeah um it's not really efficient it can get really noisy and it not that uh what's the word it's not the best thing to run the collector is like the fact the best way to run okay so and and it sounds like this thing still requires um to have Prometheus installed and yeah you would still need to be running a whole computer system oh okay I I thought it was I was under the impression it was like a replacement for Prometheus and that it was um maybe I'm thinking of something else his there was a thing that I knew it was like a replacement for for like needing Prometheus um and it was like vendor neutral so it wasn't like oh you have to use lightstep to use this thing um I think I might just be a hotel operator collector Target allocator Trio oh oh okay okay but maybe there's another thing out there oh unless unless maybe that got integrated into like the target allocator as part of anyway it is a mystery yeah okay cool cool oh that's awesome um so then okay since we're we're talking collectors now um I for me like the two million dollar question the one that I'm always curious about is collector setup so what what is the collector setup that you have that y'all have chosen has like what works for for now yeah it's hard to say because we run a lot of different types of collectors yeah headlight stuff we run like metrics things tracing things internal ones external ones there are a lot of different collectors that are running at all times you have like a separate one that just collects metrics and one that just collects traces and um right now we don't um it's all varying flux okay right now we're changing this a lot um to run like experiments and stuff um basically like the the best way for us to be able to make features for customers and end users is by running them ourselves and then using them internally making sure that they work and then sending them for the open source realm and so that's what we're trying to do even more of like we're kind of reaching a point where uh we dog food everything which gets really um confusing because you have to like yeah I can imagine yeah we're running like in a single path there could be like I think two different two collectors in two environments that could be running two different images in two different versions like it's it gets really meta and very confusing to talk about yeah yeah I can imagine and then you know if you're sending from collector eight across an environment to collector B um collector B also emits Telemetry about itself which is then collected by collector C yeah and so it like it's just chains like you basically ensure that that you have to like make sure that the collectors actually working yeah you have to be sure that everything along this path yeah you just have to know which thing has the data right well you shouldn't have to we do that for you but like right um yeah and we make like dashboards to help with that but um that's like the problem when it's like we're debugging this stuff is when there's a problem you have to think about like where's the problem actually is it in how we collect the data is it and how we emit the data is it in um you know the source of the how the data was generated it's it's you know one of like a bunch of things yeah yeah um now like you need to work on the hotel operator so and and I I've been reading up on the operator recently and there's like I think four different deployment modes right there's sidecar deployment demon set and um what's the other one um yeah nice yeah um yeah um so my question is um which mode um or is it like all of the above depending on the thing that you need to do yeah it's all the above depending on what you need to do um and you're neat like your general needs and uh like how you like to run applications for like reliability and stuff yeah so um sidecar is the one that we use the least and is probably used the least if I were to make just like a bet um sidecars are really useful like across the industry you would think yeah across the industry I'd be willing to bet that those are the least popular that's the least popular method sidecares are just expensive and if you're not using them if you don't really need them then you shouldn't use them and you'll really only need them like something that's run as a sidecar it's like istio which yeah yeah makes a lot of sense to run as a sidecar because it's doing like traffic proxy like hooks into your um you know container Network to change how that all does its thing yeah and you get a performance hit uh if you sidecar your collectors right for all your services you just get like a cost it would just cost you a lot more um and you also wouldn't be able to do as much with like um if you're making like kubernetes API calls for attribute enrichment that's like the thing that would get exponentially more expensive if you're running it as a sidecar right but as like a staple set of like you know five pods that's not that expensive but if you have a sidecar on like 10 000 pods then that's 10 000 API calls made to the kubernetes API right yeah yeah that's exclusive what um what would be the advantage of running um your collector as a full set versus a deployment like where I guess what's what's the state that you would want to persist yes this is um like I don't know the right word is an unknown stateful sets aren't only used for their ability to mount volumes um there are a few other things that are inherent to how staple sets run that are really valuable in distributed computing this is an important thing to know for not just like how The Collector runs as an application but how like your applications can run right um stateful sets have um consistent IDs so if you have a staple set with 10 replicas they're all going to be um the staple set name Dash um um counter number so it goes from like zero to n so okay that's a really valuable thing when you want consistent IDs right as opposed to like with with deployments like when you like your your pods are all like random random crap right yeah so the Pod IDs are done where you it's um deployment name Dash replica set ID Dash pod ID right and so with staple sets because we have this consistent IDs we can actually do some extra work with uh for the Target allocator which is why we require that and so the other thing that stateful sets guarantee is what's called a um In-Place deployment which is what demon sets do as well where you have you take the replica you take the Pod down before you create a new one um right so the reason that this is important is that in the deployment you normally do a one up one down right um or what's called a rolling deployment a rolling update and so if we were to do this for um uh if we were to do this for the uh with the target allocator um we would probably get much more unreliable scrapes because you would and someone actually just asked this question in the operator Channel I mean I'm going to give them this exact response um when a new replica comes up um you have to redistribute all the targets because your hash ring that you place these on is changed so if you're doing a rolling deployment if you're doing one up one down that's a really expensive operation um because then you have to recalculate all these hashes that you assign um so if you were to do a one down one up you would still have to redo this whole thing because um you would lose a pod which means it's taken out of the ring redistribute you would gain a new ID and then you'd have to redistribute again right um whereas stateful sets because it's a consistent ID range you don't have to do that at all and so this means that when we do a one down one up it keeps the same targets each time right right so it's like it's almost like a placeholder for it like you don't have to recalculate the ring basically because yeah it's just sort of like a little um uh what's it called uh yeah yeah yeah yeah I can't think of the word cool that's really neat I didn't know that yeah it was funny because I was reading about like pause being deployed a stateful sets I'm like straight or sorry not uh collectors I'm like I straight up do not understand what the use case would be but this makes a lot of sense so that's uh that's really cool and so it's not as useful um or this is really only useful for like a tracing use case I would say or sorry metrics use case where you're like doing complete test scores and we would probably run it as a deployment for anything else um because a deployment gives you everything that you need pretty much um because the collectors are stateless they don't need to hold on to anything yeah um deployments are much more lean as a result yeah yeah um they can just run and roll out and everybody's happy and that's how we run most of our collectors deployment and then at what point would a demon set be useful yeah so demon sets are really good for um things like her node scraping um which we do a lot of so um this allows you to scrape like the kublet that's run on every node um it allows you to scrape the uh node exporter that's also run on every node which is another Prometheus demon set that most people run right right yes the demon sets guarantee that you've got odds running on every node right exactly every node that matches its selector right right right yeah um and so that's really useful for like scaling out so if you have like a cluster of like 800 plus nodes um it's more reliable to run like um a bunch of little collectors that get those tiny metrics rather than a few bigger staple set pods right because your blast radius is much lower so if like one pod goes down you lose like just a tiny bit of data but remember like with all this cardinality stuff that's a lot of memory so um if you're doing like a staple set scraping all these nodes that's a lot of targets that's a lot of memory it can go down much more easily and you lose more data luckily the collector isn't like Prometheus where we don't care about that state so if a collector goes down it comes back up super fast so usually the blip is low but it does mean that the blip is more flappy right where like it could go up and down pretty quickly if you if you're past the point of saturation that's why it's good to have like a HPA horizontal Auto scaler right right on that stuff but still demon said is a bit more reliable right and it sounds like it would be useful again like from a metric standpoint yeah yeah tracing um you could do it for tracing um and just send it on like a node port but tracing workloads again because it's all push based they are much easier to scale on um and you can distribute targets you can load balance like there are all these other benefits that we get from push-based workloads pull based is like the reason that Prometheus is so ubiquitous in my opinion is just because it makes local development really easy um where you just can scrape your local endpoint and that's what most back-end development is anyway so you could like hit end point a and then hit your metric set point and then hit end point a again metric standpoint you can just like check that it's like a very easy like developer Loop no it also means that you don't have to reach out side of the network so if you're a really strict like proxy requirements to send data local Dev is much easier for that right just why like hotel now has like a really good Prometheus exporter so you could do both right right right if if you have that hankering for for running Prometheus yeah um and and then um I'm assuming there's a centralized Gateway somewhere or um or on flights um this is part of the like collector chain that I was talking about um again we're running a lot of experiments cool um I can like half talk about it okay um I can be vague um a big effort within hotel right now is um around Arrow which you might have been hearing about some um the there's been some work done by lightstep and F5 to improve the um processing speed and egress and Ingress costs of otel data um by using Apache Arrow which is a project for columnar based data representations um and so we're just like doing some group of Concepts or like proof of implementation work to to um see what the actual performance of this stuff looks like right um and also like you know check that everything works as expected yeah yeah as well which it is but that's you always have to check yes absolutely well I'd say the main takeaway from from this whole story on collectors is like it sounds like it's always going to be an evolving game which is not a terrible thing to do no it's important that you keep your telemetry up to date yeah um I think that like Library authors and maintainers are like constantly working on new performance features and new ease of use like quality of life stuff as well yeah yeah um especially with an Hotel like we talk about quality of life a lot yeah um yeah and so that is definitely a focus and that's why it's important to keep up to date it makes migrations easier as well trying to migrate from like an ancient version of something to a latest version probably missing a lot of breaking changes potentially yeah and you have to be careful of that and and on that vein then how do you ensure that everyone's keeping up to date with the latest versions of hotel across the org um I think like stuff like depend about is pretty good um we use it internally or not internally we use it um in a hotel we're keeping up to date with Hotel stuff so I find it to be really helpful it's very frustrating sometimes because it's like hotel packages all update and like lockstep pretty much that means you have to update update fair amount of packages at once but it does do it for you which is pretty nice that's nice that's nice um but you should be doing this not just for hotel but like any dependency yeah right like CVS happen in the industry constantly and if you're not staying up to date with um vulnerability fixes then you're opening yourself up to security attacks which you don't want so um yeah yeah do something about it is is my recommendation that's fair that's fair um I know we've got like four minutes left um do you want to give us any like parting thoughts as we as we wrap up um I'm interested to hear if there are any questions from the group that we've we've missed in our discussion here any takers with burning questions Now's the Time if not I'm going to call on uh Rhys but I this was great um I feel like uh I was like rapidly trying to take notes um I probably will have more as I try to like go back and tidy up some of my notes um but yeah honestly I was just trying to like keep up with people with so much information I feel like because I've been reading up on the on on the operator like the last week and so like more questions than answers and I feel like I have some answers now um I I've definitely learned a lot um I hope folks on this call have learned a lot as well I think this is like really great information I think it gives it gives folks like an idea of like what's involved in a migration things to consider like when you're setting up your your collectors um things to avoid doing keeping up with those latest versions of Hotel y'all always a good thing yeah that's the big one is that like new fixes um really often yeah yeah like new versions every two weeks for The Collector I'd say it's like a pretty frequent Cadence there are some Prometheus libraries that like don't update like ever like Thanos hasn't released since March right which is absurd to me because there's like a bug in compatibility between Prometheus and Thanos right now oh really how cheap yeah so you can use them together if you're like coding so not fun yeah damn all right last chance for for burning questions y'all Erica said thanks for the info on your time Jacob thank you Erica for hopping on um yeah this was really awesome thank you so much um because like for real this is a dope dope topic so yeah we'll you know reach out to your uh friendly neighborhood uh cfp approvers no thank you so much I feel like there was so much more we could have chatted about as well so we might um yeah yeah maybe I can um if I get accepted for the talk then I can give a sneak peek at least I can get some feedback on it actually you know what like if you're if you're interested because we have hotel in practice which is kind of like giving like a little talk so if you're interested in doing that even like before you find out whether or not you get accepted like we are happy to have you yeah it sounds great cool I'm in cool cool we'll uh we'll figure out the behind the scenes on like when to schedule you and um sounds good yay cool all right and Daniel said thanks Jacob as well and um yeah we're at the top of the hour thanks for joining us thank you all thank you yeah thank you everyone bye

