# OTel in Practice: How We Scaled KafkaLog Ingestion for OTel by 150%

Published on 2025-09-25T05:32:25Z

## Description

Join us for what's sure to be an insightful session, as Dakota Paasman from Bindplane shares how their team solved a customer ...

URL: https://www.youtube.com/watch?v=GrQyUPeCljo

## Summary

In this edition of "Hotel in Practice," host Henrik welcomed Dakota Passman from Bindplane to discuss scaling open telemetry logs with Kafka. Dakota, a software engineer at Bindplane, shared insights from a project where they helped a customer overcome performance bottlenecks while pulling events from Kafka. The customer initially faced issues with only 12,000 events per second per partition, which led to a growing backlog. Dakota highlighted key strategies that improved performance, including using a new Kafka client, optimizing log encoding, reconfiguring the data processing pipeline to prioritize batching, and switching the exporter protocol. After implementing these changes, the throughput increased significantly, demonstrating how careful configuration can enhance system performance. The session concluded with a Q&A segment where Dakota addressed audience questions related to performance tuning and configuration best practices. Viewers were encouraged to access the recording on YouTube and LinkedIn for further learning.

## Chapters

Here are the key moments from the livestream with timestamps:

00:00:00 Introductions and welcome to the stream  
00:01:45 Introduction of speaker Dakota Passman  
00:03:10 Overview of the topic: Scaling OpenTelemetry logs with Kafka  
00:05:00 Discussing performance issues with the customer's Kafka integration  
00:07:30 Key takeaways for improving Kafka receiver performance   
00:09:20 Explanation of the customer's original configuration and bottleneck  
00:13:00 Live demo setup and initial performance metrics  
00:15:30 Implementation of changes to improve performance  
00:20:00 Analysis of the changes made and their impact on throughput  
00:25:00 Q&A session discussing exporters and batching strategies  
00:30:00 Closing remarks and information about future events and community engagement

# Hotel in Practice: Scaling OpenTelemetry Logs with Kafka

Hello everyone, and welcome to our latest edition of Hotel in Practice! We are so excited to have you join us today. Remember to tell your friends that if they weren't able to make it, the recordings will be available afterward. You can check out our recordings on our YouTube channel at **hotel-official** and also on the Hotel LinkedIn page.

Today, we have a very special guest, Dakota Passman from Bindplane, who will talk about scaling OpenTelemetry logs with Kafka. Without further ado, let’s bring Dakota on!

---

### Introduction

**Dakota:** Hi! Thanks for having me. I'm Dakota, and I've been a software engineer at Bindplane for a little over two and a half years. I work on our platform, focusing a lot on the hotel project and making various contributions, especially regarding the supervisor. I'm happy to be here.

**Host:** Excellent! If you're ready, let’s get started. For anyone interested, we will have time for questions after Dakota's presentation, so please feel free to post them in the chat.

---

### Presentation Overview

**Dakota:** Today, I want to discuss a situation we handled with a customer using the Kafka receiver at scale. They were experiencing performance issues and encountered a bottleneck while trying to pull events from Kafka. Initially, they were pulling only 12,000 events per second per partition, which was not enough to keep up. We're going to cover that bottleneck and how we helped them reach a target of 30,000 events per second (EPS). We’ll also demo those changes live in a replica environment.

**Key Takeaways:**
1. **Don't Trust Defaults:** The Kafka receiver uses a default Kafka client that performs differently. A new client implementation added shortly before we encountered issues provided a significant improvement.
  
2. **Configuration Matters:** Proper configuration of the collector can make a huge difference. This might seem obvious, but it’s easy to overlook if you’re unsure of the configuration options.
  
3. **Pipeline Configuration:** The configuration of the pipeline is also crucial. It can greatly affect performance.
  
4. **Understand Your Observability Goals:** This understanding helps tune the environment effectively to ensure your Kafka receiver performs well and meets your observability goals.

---

### The Customer's Situation

The customer was pulling 192,000 events per second across all their partitions, with a single topic of 16 partitions. They needed to reach 480,000 events per second to keep up with the data going into Kafka and start cutting down their backlog. They were using the OpenTelemetry collector and had many default options set, leading to rapid backlog growth. Kafka was ingesting 30,000 events per second per partition, resulting in a backlog increasing by 288,000 events per second.

---

### Changes Made

Here are some changes we implemented:

1. **Client Upgrade:** We switched to the new FronGo client, which improved throughput by about 30%.
  
2. **Log Encoding:** The receiver was initially set to use OTLP JSON, which was inefficient for their use case. Switching it to raw JSON provided a significant performance boost.

3. **Batching:** We moved the batch processor to the start of the pipeline, allowing the receiver to push out large quantities of events consistently.

4. **Protocol Change:** Changing the protocol from gRPC to HTTP for the exporter also improved performance significantly in their specific case.

---

### Demo

In the demo, we will use a single collector and a single partition for simplicity. Initially, we were pulling around 12,000 events per second. After applying the changes, we expect to see significant improvements.

(At this point in the presentation, Dakota demonstrates the changes made to the configuration and shows the performance metrics.)

---

### Results

We witnessed a jump from 12,000 events per second to around 25,000-26,000 events per second after implementing the changes. This improvement allowed the customer to catch up with Kafka's ingestion rate and start reducing their backlog.

---

### Conclusion

In summary, we identified several key factors that contributed to the bottleneck:
- Back pressure from upstream components affecting the receiver.
- The performance differences between the FronGo client and the default client.
- Avoiding unnecessary encoding conversions based on telemetry goals.

Thank you for attending this session! We have some links to share, including a blog post about this topic and a link to the CNCF Slack.

---

### Q&A Session

**Host:** Now, let’s move on to questions from the audience.

1. **How do different exporters impact performance?**
   - The impact will depend on the exporters used. For this customer, switching from gRPC to HTTP for the SecOps exporter resulted in better throughput.

2. **What was the initial reason to put batching at the end of the pipeline?**
   - This approach may have worked better in other environments, but it depends on the processors in your pipeline.

3. **Would using a memory limiter bring extra performance?**
   - I'm not familiar enough with that processor to give a definitive answer, but I’m open to looking into it.

4. **Is the FronGo feature flag only useful for the Kafka receiver?**
   - No, it's a general implementation not specific to the receiver. Other Kafka components might utilize this client as well.

5. **Did you size the batching to improve performance?**
   - Yes, we experimented with the size of batching to find what worked best for our environment.

6. **What about resource usage? Does it have any consequences?**
   - The configuration changes didn’t significantly affect resource consumption, which remained reasonable.

---

### Closing Remarks

If there are no more questions, that wraps up our session. Thank you all for attending! Remember, the recording will be available on our LinkedIn page and YouTube channel. Also, for those interested in sharing their stories with the Hotel End User SIG, please reach out to us on CNCF Slack.

We would love to hear from you, especially if you have experiences with OpenTelemetry to share. Thank you so much, and we hope to see you at CubeCon North America!

## Raw YouTube Transcript

Hello everyone and welcome to our latest edition of Hotel in Practice. Um we are so excited to uh have you join here today. And remember um tell your friends for anyone who wasn't able to make it today that we will have our recordings available after the fact. Um you can go to our YouTube channel at hotel-official and you can also check out the hotel LinkedIn page. We we should have the recording available on there too. So we have a very special um hotel in practice today. We have Dakota Passman from Bindplane talking about um doing some gnarly scaling of open telemetry logs with Kafka. So without further ado, let's bring Dakota on. >> Hello. >> Welcome Dakota. >> Hi. Thanks for having me >> and to be here. >> Oh, super excited to have you here. Um, so, uh, would you like to do a, uh, a brief intro of yourself before starting? >> Yeah. Yeah. Um, I'm Dakota. I've been a software engineer at Biome Plan for a little over two and a half years now. Uh, working on our platform. Uh, focusing a lot on the hotel project. Uh, making some different contributions. Uh, focusing a lot these days on the supervisor. Um, it's a pretty cool project going on there. Um but yeah, happy to be here. >> Excellent. So if if you're ready to go, then uh let's get started. And um by the way, for anyone who is interested, um we will have some time after Dakota's presentation for questions. So please feel free to post them in the chat and we will address them after the the presentation. >> Yeah. Cool. All right. Um, so yeah, today I want to talk about a situation um, we handled with a customer of ours using the Kafka receiver at scale. Um, so yeah, um, we had to scale their environment. Uh, they were having performance issues. They're running into a bottleneck uh, trying to pull events from Kafka. um you know they were doing when we started talking to them they were pulling only 20 or 12,000 events per second um per partition in their topic um which was just not enough to keep up. Um so we're going to talk about that bottleneck, how we got them to get up to a target EPS of 30,000. Um and then we're going to demo those changes uh live in a replica environment. Um, so to start, we're going to go over some of the takeaways just so that we can keep these in mind as we're going through this, um, and see where they're coming up. Um, for starters, uh, don't trust defaults. Um, the Kafka receiver by default uses a certain Kafka client. Um, and these clients behave differently. They perform differently. Um, it's a default. However, uh shortly before we had this issue with the customer, there was a new client implementation added um that we were able to use um and using that client over the default client was a big improvement. Um secondly, uh configuration of the collector makes a huge difference. Uh, and that might seem kind of obvious, but it's sometimes easy to overlook that, especially if you're not entirely sure of what configuration options you have and what they might exactly do. Um, and something else that I think gets overlooked sometimes is configuration of the pipeline matters a lot too, especially in this situation. Um, and then finally, it's important to understand your observability goals. Um, so you can tune the environment to get it right. um and make sure that your Kafka receiver or really any receiver that you might be using is performing well um in your environment and hitting the observability goals that you're looking for. Um so yeah, to kind of set the stage a bit, so this customer of ours um they were pulling 192,000 events per second across all of their partitions. Um they had just a single topic with 16 partitions. Uh again doing roughly 12,000 events per second per partition as a sum that's 192,000 events per second. Um however, you know, they needed to really they need to be at 480,000 um events per second uh in order to keep up with the amount of data going into Kafka. Um and at that point then be able to start cutting down this backlog that they had going. Um so yeah they're using the open telemetry collector uh using the cafka receiver a lot of default options set um you know one topic 16 partitions each one's only doing 12,000 events per second. Um the backlog was growing rapidly. you know, Kafka was ingesting 30,000 events per second per partition roughly and so their backlog is growing by 288,000 events per second across all the partitions. Um, so they're quickly falling behind. Um, in this situation, it was security telemetry that they were collecting from Kafka and trying to send to a seam back end. Um, and so already they're dealing with delayed telemetry. Um, and at this uh performance, you know, this poor performance, they're at risk of starting to drop some of this telemetry, too. So, some of the changes we made. So, this is kind of like a illustration of the configuration and the pipeline that they had going. And you can see some of these changes as we moved through uh the pipeline. So first off the Kafka receiver uh using the new FronGo client which was the Kafka client I mentioned before using that implementation uh we saw a big jump you know we started we jumped up to about 30% um just using that client instead of the default um another thing the log encoding that we were pulling from Kafka that made a big um they were the receiver was configured to use OTLP JSON um which didn't really make sense in their situation um and so there's a lot of unnecessary work being done to handle OTLP JSON uh so instead we switched it to just raw JSON or normal JSON um not OTLP specifically and that was another big performance boost there. Um, one other thing, batching. Uh, we moved our batch processor to the start of the processor pipeline, uh, right after the receiver. Uh, the reason we did this, it allowed the receiver to pump out large quantities of events rather than being restricted by the data flow of the process user in front of it. So it was reliably able to push out, you know, thousands of events in a batch at once rather than maybe, you know, a couple hundred and then jump up to a couple thousand back to a couple hundred. You know, it's much more consistent performance batching right after the receiver. That's what we saw. And as a result, we're able to boost our throughput doing that. Um and then one final change we made uh this is again specific to their environment and their pipeline. Um they were sending the Zack Ops seam back seam back end and we saw that changing the protocol used from gRPC to HTTP um actually increased the performance. Um so in their specific case uh this made a big impact. We were able to get uh increased throughput doing this as well. Um the idea here is similar to the batch processor we discussed here. Um you know the exporter is just able to handle throughput better at HTTP or using HTTP instead of gRPC. Um, and this affected the receiver upstream from the exporter, allowing it to push data out a lot faster. So in this demo, um, you know, the customer, they had one topic, 16 partitions. Um, they had a single collector per partition. Um, in this demo, we're just going to be using a single collector, a single partition. Um, demoing with 16 partitions and managing collectors at that scale is difficult to do manually. Um, so we're just going to keep it simple here, just a single collector. Um, going to start the environment should be about 12,000 events per second that we're pulling. Um and then we're going to apply the changes to the environment and then we'll see um you know we'll discuss those changes more in depth and see how it's performing afterwards. Um yeah. So if I come over here and check this out. So I started this up shortly before we started here. So this green line up here, this is going to be the events per second that the Kafka topic in partition is pulling in. And we're stable at around about 26 27,000 events per second. And then down here is our single receiver, single copy receiver. Um this is using the default client. Um it's pulling in these events as OTLPJSON. Um, and batching is being done afterwards. Um, and here you can see that we're stable at 12,000 events. Now, I'm going to just quickly stop this collector and then I'm going to start it up using the new config. And then also crucially here, I'm going to start it up using this feature gate to enable the Fronco appliance. So, I'm going to start that. I'm going to let that run in the background for a little bit, let these changes trickle through. Um, and I'm going to talk about the changes we made more in depth. So on the left here, this is the first configuration that the collector was using. Um, and this is the configuration for the Kafka receiver. This is a pretty default config. Uh the key difference here again is this OTLP JSON encoding versus on the right here in the new config, we're going to be using text. Um using text is just going to be able to pull the data faster. It's not going to have to worry about um transforming or handling that data as much. Um and so it can just push the events through faster. It's not going to get bogged down trying to transform it. Um I've got some collecting the uh metrics from this receiver. Um here I've got some processors. These are define the same across the two here. This is to generate or to simulate um how moving the badge processor around in the pipeline affects the actual throughput. Um so these processes are are all the same, you know, just pretty basic adding some fields. Um, next exporters. We've got no oporter. We're not going to show um the effects of the stock ops exporter in this demo just because that's can it's a little much. Um, so we're going to just keep it simple here using the OP exporter. Um, you know, eliminate that uh lever from the situation. Um, and then we're sending our internal telemetry. And then the other important part here. So yeah, this is the initial configuration. We can see here the order we define our processors. You know, we've got our two transform processors and then the batch. Again, the improvement we saw was adding the batch processor at the start of this processors uh definition list. Um again this is so that the receiver is sending directly to the batch processor. Um it can get consistent performance consistently pushing out a high number of events um rather than being bottlenecked by the process the other processors in front of it. Um and then the other crucial thing again I pointed out when I restarted the collector I used the new feature gate um to use the frongo client instead of the default So, we'll come back here. We'll give this a refresh. And this is going to keep going. But at the moment, we can see this new collector here. This is the old one turning off. And this is the new one coming back on. And we briefly spike up. And I expect if we let this go for a little while, we'll see this um stabilize and mirror the Kafka uh Kafka line. Um let this go for a little bit. Um, but I guess yeah, just to kind of go back here and reiterate some of these points again, use the Fronco feature feature flag to use a Fronco client just performs a lot better than the default client in the Kafka receiver. uh your log encoding. Again, if you're uh handling the data unnecessarily, in this case, we're pulling data from Kafka that's not necessarily OTLP JSON, but because we have a receiver set up for that encoding, um there's unnecessary work being done to transform it into that. Um and we can just pull it in as normal JSON and handle it afterwards. Um that was a big source of improvement. Um, and then again batching early, the receiver is able to consistently push out a consistent number of of events rather than having seen that fluctuate based on what's happening upstream of it. Um, and again in a similar vein um not shown shown in the demo but for sec ops or for this exporter um tuning and configuring that affects the receiver upstream of it. Um, yeah. So, those were again some of the changes we made. We'll see if we've got some better data here. So, it's still early, but you can see how this collector is starting to stabilize kind of right around the throughput that the Kafka uh topic is getting. Um, I expect this to get, you know, closer as it comes online and starts to stabilize. But again, you can see we jumped from doing 12,000 events per second and then with all those changes we made, now we're closing in around 25 26,000 events per second. Um, drastically better improvement. And this is how we were able to help this customer, scale up their throughput so that they're able to keep up with Kafka. Um, and in this in their case, we were able to get past what Kafka was ingesting so that we're able to also start cutting down the backlog and catch up. Um, yeah. So, got refresh this a bit. Cool. So, yeah, you can see this is stabilizing. So again, why it worked? Back pressure. Back pressure is the the term for this idea I've been talking about where what's happening upstream from or downstream of the receiver is affecting the receiver. You know, it's just not able to push events through as fast because it's got to wait on the components ahead of it to process their events. Um you don't know you don't notice it until in this case the receiver starts falling behind and is affected by it. Um, again very similar to the early batching. Um, the FronGo client just performs a lot better than the default client um, in terms of pulling events from Kafka. Again, use that feature gate to enable it. Um, and then yeah, finally, avoid doing unnecessary encoding conversions. Again, this is understanding your telemetry goals in your environment. um understanding the what the data you're sending looks like so that you can uh make sure that your receivers and other components are configured properly to handle it. Um and yeah, I think that's all I have for a demo. Um we can keep checking out that graph to see how it's looking as it's coming online. Um, got a couple links here. One to the blog post that we made about this. Um, and then also the link out to the CNCF Slack. And I think that is it for my slides. Um, yeah. Cool. Sorry. mute muted. Mike, looks like we have a question uh coming in from LinkedIn. Um so, how do different exporters impact performance? >> Yeah. Um I think that's definitely going to be very dependent on the exporters being used. Um you know, it's going to be something you have to investigate per case and and again, you know, fine-tune the configuration of them. Again in this case for the customers a sec ops exporter uh using gRPC just wasn't able to keep up with the throughput we had um and in this situation HTTP was able to perform a lot better. Um so yeah it's just a matter of trial and error see what's hap what works best for your environment based on your telemetry and what your your goals are. >> Awesome. Um, looks like we have another question. Um, batch has been recommended to be the first processor of the pipeline. What was the initial reason to put it at the end of the first pipeline? >> Yeah, so the reason for putting it at the end, um, you know, I'm not entirely sure of the decision to do that or, you know, what went into that decision. Um, I think it's a perfectly valid approach. I know maybe in other environments that works better. I think it depends on the processors in your pipeline. Um you know I think some processors it be better to batch afterwards um rather than before. Um yeah again it's very dependent on you know what your pipeline looks like, what your goals are um and what you're trying to achieve. >> Awesome. Another follow-up question or another question. Uh, you did not use the memory, sorry, you did not use the memory limiter. Would that bring extra performance uh from using it if you if you use the memory limiter? >> That's a great question. Um, I can't say I'm too familiar with that processor, so I can't I don't know if I have a good answer for that. um certainly would be willing to look into it and understand it and see if that could have applied here. >> Fair enough. Um another one that we have is uh the France Go feature flag is it only useful for the Kafka receiver? >> It's a good question too. Yeah. So no. Um the Fran Go client implementation is just a general implementation not specific to the receiver. So I think some of the other Kafka components in the project um you know it's a matter of whether or not they utilize this client as well um and looking into seeing if there's a way to enable them to use this client or not. Um if not I imagine that's something that'll happen shortly is the ability to use the other Kafka components with that Fronco client. Um so yeah. >> All right. Awesome. Um, another question we have. Did you size the batching to get improvements and did you export and sorry, did you adjust the exporter batch as well? >> Yeah, that's another good question. Um, yeah, we definitely did play around with the size of the batching. Um, you know, I think in this demo we've got it at Yeah, we've got it configured like this. Um, yeah, that's definitely something that you can play around with. you know, toggle that, fine-tune it, see what works best for your environment and your throughput. Um, and yeah, what's able to get you through or get your telemetry through? Um, yeah. >> And then final question that we have here, uh, what about the resource usage? Does it have any consequences? >> Yeah, that's a great question, too. Um, yeah, I mean, definitely something you want to be aware of. um with our customer, you know, making these configuration changes. Um they didn't affect the resource consumption and usage like that, you know, like the the amount of resources the collector was using. It was still pretty um not negligible, but it didn't affect the system. It wasn't like all of a sudden we jumped from using, you know, 20% memory to 80% memory or CPU, you know, it was very reasonable. Um, so it wasn't necessarily a concern. >> Excellent. Um, do we have any other questions from the audience? Um, let me share these links in the chat as well. >> Oh, perfect. Yeah. Stay tuned. >> We need like a little drum roll. >> Yeah. >> Okay. Awesome. Henrik has shared the links. >> Perfect. >> Chat. Amazing. Um well I suppose then if we don't have any other questions I guess that is a wrap. So um short and sweet informative that's a lot that's a lot to cover in a short time and also like lots of great lots of great questions lots of great uh considerations from a from a performance side um as well. So definitely appreciate the uh the questions that we've gotten and again um tell your friends for those who are on the stream that this recording will be available after the fact both on our LinkedIn page the open telemetry LinkedIn page and on the open telemetry YouTube channel which is hotel-official. Um and then a couple of housekeeping notes um for anyone who is interested. We the CFPs are still open for uh for CubeCon and I think in the last week the CFPs have also opened for the CubeCon colloccated events. That's the CubeCon in um in Europe in Amsterdam. Not not the upcoming one. The upcoming one that's that's done. Those CFPs are closed. But if you are interested in applying to uh CubeCon EU and Amsterdam um and or to the colllocated events um these are the links and remember folks that the submission limit for CubeCon is now three um three proposals per person and that includes whether you're a uh the main submitter or a uh a co-speaker. Um and then the submission limit for the colllocated events is 10 across all coll-located events which is very cool. So uh yeah there's that going on. And then for anyone who is interested in sharing their stories with the hotel end user SIG, we love hearing from the community. So please reach out to us. You can find us in uh CNCF Slack. We have a lovely um Slack channel which is uh I believe SIG hotel end user and Henrik has been nice enough to put the uh the link to our Slack channel on there as well. Oh uh yeah, sorry I had it backwards is hotel-end user. Um and yeah, we would love to hear your stories whether it is through hotel in practice. So this type style of presentation and you know if you're if you have a presentation that you want to test out this is a great proving ground or if it's you know a talk you've given somewhere and you just want to share it with more folks this is also another great way to uh to get the the topic out there. Um Dakota you presented this at um at open source summit right the collo the observability day or the the I forget what the hotel day was that is that correct day >> um I didn't I didn't uh present anything but one of our co-workers at bonf did present a topic about the cafner receiver >> ah okay >> his presentation was yeah a deep dive into his understanding of it and some of the unique aspects of it he figured out so >> oh Nice. Very nice. That's awesome. Yeah. So, we we would love to hear topics like that. We also love to hear um so anything where you've like forot and practice, anything that you've learned cool stuff about open telemetry, we would love to hear from you. So, hit us up on our um on our Slack channel. Uh we also have hotel Q&A. So, for anyone who wants to talk about their hotel journey, um this one's an interview style um uh format. Both of these are live streams. So we would love to hear from you and we have uh recently had I think our previous hotel and practice was with folks from Alibaba in the APAC region. So we are looking for more folks in the APAC region as well who would love to share their story because you know what open telemetry CNCF it's a global undertaking. We have uh lots of love from folks all all across the globe. So um we are definitely if you're in the AP pack region and you have a story to share um we would schedule an AP pack friendly hotel and practice hotel Q&A um to to cater to the uh to the time zone. So um that is it for us. Um and hope to uh if you're going to CubeCon North America uh you'll probably see some of our crew at um at CubeCon North America. So excited to uh excited to have folks connect in person on on open telemetry. Thank you so much everyone for attending. >> Thanks

