# OTel in Practice Presents: Observability is a Team Sport with Iris Dyrmishi [FULL VIDEO]

Published on 2023-06-12T23:45:52Z

## Description

Observability ins't the responsibility of one single team. Join Iris Dyrmishi of Farfetch as she talks about how the collaboration ...

URL: https://www.youtube.com/watch?v=U1yLXnMONkc

## Summary

In this YouTube video, Iris, a platform engineer specializing in observability at Farfetch, leads an open discussion about the importance of observability as a collaborative effort within engineering teams. She emphasizes that observability is not the sole responsibility of a central team but rather a shared duty among all engineers, regardless of their specific roles. Iris outlines what she considers a perfect observability system, highlighting the necessity for centralized access to metrics, optimized data costs, reliable alerting, and team involvement in incident response and monitoring. Throughout the session, she encourages audience participation, discussing challenges such as excessive data, the need for structured telemetry, and the integration of observability into team processes and culture. The conversation also touches on practical tools and frameworks used in observability, such as OpenTelemetry, Prometheus, and Grafana, and the significance of creating a culture that values observability across the organization. The discussion concludes with an invitation for further community engagement and sharing of best practices in observability.

## Chapters

00:00:00 Introductions  
00:01:30 Overview of Observability Culture  
00:05:00 Discussion on Responsibility of Observability  
00:10:00 Defining a Perfect Observability System  
00:15:30 Importance of Team Involvement in Observability  
00:20:00 Role of Observability Engineers  
00:25:00 Challenges in Observability and Incident Response  
00:30:00 Custom Solutions and Tools for Observability  
00:35:00 Addressing Anomaly Detection in Data  
00:40:00 Centralizing Observability Tools and Data

# Observability Culture Discussion

**Iris**: Thank you for joining us again! We had her for a Q&A last month, and she did a fantastic job, so we asked her to come back. I’ll let her take it away.

---

**Iris**: Hello everyone! Thank you so much for joining today. My name is Iris, and I’m a platform engineer focused on observability at Farfetch. I’m passionate about observability and love to share its culture, not just within my company but across the board.

Today, I want this to be an open discussion. I would love to hear about the observability culture in your companies as well. I believe that observability is a team sport; it shouldn’t be confined to a single central team, regardless of the company size. Everyone should contribute and be involved in observability.

Observability has existed for a long time, but it’s becoming a significant focus now, and we have this amazing culture spreading. However, we still have a long way to go. Conversations like this are essential for us to learn from each other and improve our practices.

### Introduction

Please feel free to interrupt me if you have questions or opinions about what I'm saying. A spoiler alert: these are mostly insights I've gained from my job, and I welcome any differing opinions for discussion.

The first question I ask all engineers interviewing for my team is: *Whose responsibility is observability?* Let’s be honest, observability is implemented differently across various companies. I've seen many approaches, and the consensus is that observability is everyone’s responsibility. If an engineer answers that question affirmatively, I know we are on the right path.

### Defining a Perfect Observability System

To understand why I believe everyone should be responsible for observability, let me outline what I think a perfect observability system looks like. Of course, this is subjective and may not apply universally, but here are the key characteristics I envision:

1. **Centralized View**: Engineers should have easy access to traces, metrics, logs, dashboards, and alerts without memorizing multiple URLs.
   
2. **Optimized Data**: Sending everything doesn’t mean better observability. It’s crucial to optimize data to control costs while ensuring it remains useful for troubleshooting.

3. **Reliable Alerting**: Alerts must be dependable and customizable.

4. **Trace Correlation**: There should be correlation between different telemetry signals.

My job as an observability engineer at Farfetch is to provide these systems for engineers, but this is a collaborative effort. Many characteristics of a perfect observability system require teamwork from various individuals who understand the nuances of their products.

I would love to hear if anyone has different thoughts or additional ideas about what makes a perfect observability system.

### The Importance of Team Involvement

Observability is crucial in a company, especially as it grows. Many organizations still don’t grasp its importance until they face issues. Here’s why I believe teams should be involved:

- **Incident Response**: In our company, incident response is vital. With a large user base relying on our product, it’s crucial for all teams to be involved in incident management. If engineers are not engaged in monitoring their systems, it could take hours to respond to incidents instead of minutes.

- **Custom Alerts and Dashboards**: Custom alerts created by the engineering teams will be more effective than generic ones set by someone outside the team. This ensures that alerts are relevant and actionable.

- **Auto-scaling**: Effective auto-scaling relies on telemetry data. Engineers who know their systems can set appropriate scaling rules based on their unique needs.

To summarize, observability isn’t just about having a system; it’s about ensuring everyone involved in building and maintaining the product takes ownership of observability.

### Challenges and Solutions

Observability can get messy, especially in large organizations. It’s common for engineers to add custom metrics and logs, leading to a cluttered environment. Here are some challenges I’ve encountered:

- **Too Many Metrics/Logs**: Engineers may flood the system with unnecessary data, making it hard to sift through and find valuable information.

- **Lack of Structure**: Unstructured logs can be particularly frustrating. If engineers can’t easily query and analyze data, it hampers their ability to troubleshoot effectively.

- **Cost Concerns**: Observability can be expensive, and it’s essential to strike a balance between collecting sufficient data and managing costs.

### Creating a Culture of Observability

For a successful observability culture, we need to:

- **Educate and Empower**: Provide engineers with the tools and knowledge to take ownership of observability. It’s crucial to avoid judgment and create a supportive environment where engineers can learn and adapt.

- **Be Patient**: Transitioning to a robust observability culture takes time, especially for new team members who may have different experiences from previous roles.

- **Foster Collaboration**: Encourage a collaborative atmosphere where observability is a shared responsibility, and everyone feels empowered to contribute.

### Conclusion

In summary, we all need to work together to cultivate a great observability culture and system, making our lives significantly easier. If anyone has questions or would like to share their experiences, please feel free to speak up!

---

**Derek**: One comment related to what you said about observability is the ability to form rich queries against data once it's in your analytics backend. If you can’t query effectively, it leads to frustration. 

**Iris**: Absolutely, I completely agree. 

**Derek**: What was the observability culture like when you joined your company, and how did you make improvements?

**Iris**: I was fortunate because my company had already laid a strong foundation for observability. We continue to improve it by holding presentations, sharing articles, and being available for questions.

**Derek**: How have you encouraged application teams to take ownership of observability without overwhelming them?

**Iris**: We promote ownership by making it clear that each team is responsible for their monitoring and alerting. We’re here to guide them with documentation and support, but the responsibility lies with them.

**Derek**: What has been the reaction from teams when you enforce this?

**Iris**: Most teams respond positively. They often ask for guidance, and we have the support of management to reinforce this approach.

**Iris**: Thank you all for the insightful discussion! If you’re interested in sharing your stories or participating in future sessions, please reach out on Slack. Thank you for having me today!

## Raw YouTube Transcript

foreign thank you for joining us again um we had her for the Q a last uh last month a couple weeks ago I guess and she did a bang-up job so we we asked her to come back and uh I'll let you take it away well hello everyone thank you so much for joining today my name is Iris I'm a platform engineer with a focus on observability working at firefits currently uh and I'm a super passionate person when it comes to observability so uh yeah I I like to talk about it as much as I can and I like to share the observability culture all over not just in in my company so today I wanted to have a talk and I would like to have it as an open discussion I would love to hear about the observability culture in your companies as well um how I believe that observability is a team sport and it is not something that is being done just by one Central team in a company no matter how big or small it is I think everyone should put their hands and be involved in the observability but of course it's let's say observability has been there forever but now it's becoming this huge thing and now we have this uh amazing culture that is spreading but we're still not there so I feel like this conversation and these discussions are so important to put out there for us to discuss and then to take them to our companies and to learn obviously from each other so yeah this was just a a small intro uh please interrupt me because I do not have a good visibility so if you have a question please don't don't hesitate to unmute yourself and just uh just speak or if you have any any opinions about something that I'm saying I just want to have a spoiler alert these are mostly things that I've learned from my job and things that I believe in so of course if you have a completely different opinion please let me know let's discuss it it's it would be amazing so to have a conversation about this here okay so uh the first question that I ask all Engineers that are inter being interviewed for a com for a job in my team is what whose responsibility is observability because let's be honest observability is be it is being implemented different in in many different companies I've seen so many ways everywhere that have interviewed Forum that I work it's different so of course observability is everyone's responsibility it's simple and if your engineer says that answer to me and then I'm like okay okay we're on board we're getting somewhere uh and why do I believe this uh first of all in order to get deeper into why I believe that everyone uh is responsible for the durability in their company I want to give an overview of what I think of perfect observability system is and that's perfect let's take it to the grain of salt of course it's perfect for me my company it works in another person's company doesn't work so if you have any more ideas here this was just what came at the top of my mind so for me a perfect obserability system it has a centralized view of systems observability it's data Engineers can go and they can access their traces metrics logs dashboards alerts um everywhere they don't have to memorize a lot of URLs or have to go in many places to find the information so that's the first thing the second one is optimized data will optimize costs because doing observables doesn't necessarily mean that you will send everything that you can and rack up huge bills uh but it needs to be optimized data and some good old data that is actually going to help other teams troubleshooting and monitoring their systems obviously reliable alerting reliable and custom dashboard reach traces correlation between the different Telemetry signals so this is something that makes a perfect observability system for us and my job as an observability engineer especially in perfect is to provide that for the engineers but I cannot do it alone because no matter how much I try many are let's say most of this uh characteristics are a teamwork and something that other people also have to interview which means the engineers that are part of I would say our customers I don't know if anyone has any other ideas about what a perfect observability system it is for them I would love to hear more about it or if you completely agree with with my goals of having this this system but at least I'm with yourself and with them throughout me any moment ous thank you definitely agree with a lot of the things that you you said like it just yeah spot on okay uh sorry I'm not able to see the comments here but if there is something interesting we do let me know please okay so uh the question is okay you have this vision of a perfect observability system how do we get to this observability system well observability Engineers like myself my team with the knowledge to shape observability systems into the correct path uh I'm not saying that um observability is something that a simple engineer that is working that can't get into they can be uh participants and active participants in it but of course it is important that there are Engineers who know observability better than everyone who know what guidelines to implement uh what tools to use or how to optimize data it's very important that these people exist in the company uh because as I said before applicability is one of those fields that are moving so fast that it is impossible for a person that is working let's say uh with databases or in Java and their full-time job is is called getting back-end whatever else to keep up with this so it is important for people like like us to be in the company and to preach let's say these values and to bring all these good tools and good things that we that are being developed and are being shared in the community of course as I mentioned uh observability Engineers that also bring the data to collect and process Telemetry data um but my fourth favorite so far of course is open Telemetry which I'm a huge that's why we're here after all I'm a huge fan we have fans grafana Prometheus that are some of the tools that are open source that are being used so massively right now by uh technibility engineers and now I'm getting to what the rest of the presentation will be about it's not only observative Engineers that are needed when it seems to actively participate in optimizing the data that they're sending and things actively building their monitoring five people 10 people 20 that depending on the size of the durability team cannot do the work of two thousand three thousand depending on the work of so many Engineers everyone needs to have control of their own code so uh some of the things why observability is so important to be done as a team is because how crucial it is in a company I don't in some let's say that in some companies it's still not understood how important it is to have observability but that is well what I believe is because of the small scale but the more that the companies grow they understand how important it is to have highly available and reliable system it is crucial to have optimized performance because you could have you could uh allocate a lot of resources and you are not saving on cost basically you're spending too much money or you're cutting back on resources and you are basically not giving a good performance or a good experience to your users depending on the type of product it is also it is crucial for incident response this is something that is very important uh in my company incident response is very important considering that we have a lot of customers that rely on our product a very expensive products at time so having all the teams involved in the crucial in the incident response is extremely important we have around 2000 Engineers I actually thought that we were three thousand we were actually two thousand that are working completely different areas some of them I don't even know except for what they are so when I hear about and I'm like wow okay because it's so big and so diverse in in our company and uh if this Engineers were not involved in the part of building their own dashboard building their own alert knowing what data they're sending for their observability uh reasons they will not be able to respond to an incident in time simple as that if I was the one making the alerting for them it would be something extremely generic with some thresholds that I just thought would be right because I don't know their products or their code obviously um the dashboards would be completely generic but if those were done by a person inside the team itself it would be so easy for them because they know it inside out their work uh what are some of the issues that they might have so if an incident happens you reduce the the response from hours that it would be if it was for me dealing with their incidental to seconds or minutes if someone from their engineering team is involved also it is crucial for incident detection as I said before um us as Engineers of the rebuilt Engineers we can provide a set of alerting dashboards that are very generic that each team can have as a baseline to implement their monitoring system but I don't feel comfortable in a way to go and to implement custom alerts custom dashboards for these teams again it goes to this even if I wanted to there are thousands and thousands of Engineers Technologies hundreds of teams I would never be able to do that properly that's why it's so important that each team is involved and does it themselves if they need it they change it they don't need our permission they are owners of that solution it's extremely important and also critical for auto scaling um I say this because in perfect we rely heavily on Telemetry data to Auto scale because you never know the amount of traffic that you're going to have in your application but for example let's say for me or for another person in the observability if we were to set some Auto scaling rules for you to be based on CPU on memory to be something that is completely generic but an engineer that knows their system so well they would go and say let's do a custom auto scaling depending on the number of through output or whatever basically my bottom goal here would be to say that all this require people from the teams that are monitoring the system to take part in if it is done by people that are outside um I am an infrastructure engineer I work with kubernetes I work with VMS with Azure with cloud and I'm not very good at back-end coding let's say so if I was the one doing this I wouldn't do it properly no matter how much I tried and how much effort I put so let's not do observability for the sake of it yeah let's have some generic alerts just for having them no let's do it properly let's have the people that are working day to day with the product Implement them make it their baby as well it's very important to to show it to the engineers and I I am very proud to say that in my company we have a pretty good observability culture that has started a few years back so now it is uh widely accepted but I've seen it happen that teams rely heavily on on other people to set up alerting for them and that is something that scares me if something happens if an incident happens and I wouldn't be comfortable working in a team like that if I wasn't the one setting up my own monitoring for the system also observability can be so messy uh depending on the scale of the company and so many people that come and go everyone wants to add the custom metric the custom log it can get extremely messy it can get too too many metrics too many logs especially for example living debugging logs flowing all day you know because it can yeah you just sense I'm an engineer I want to have full obserability of my application it's not affecting my performance so I'm sending everything debugging info warning metrics I create like a bunch of them on top of each other uh or the same metric with different names and yeah what can me as uh as observability when you do about it alone I I detect it I see it and I say okay this is completely wrong but of course I'm going to need the collaboration of the engineers that are working with our product to actually change it otherwise it would never be optimized and it should never be um let's say good observability um practices or too many traces well I'm a firm believer that there is no such thing as too many traces I'm a big advocate of that but yeah it's it's possible that we are sending 100 of traces uh with a mess and completely no information there and watch what what can I do considering that I'm not the one doing the day-to-day developing I have to go and talk to the things that they need to be active participants on that without it for me it's there is no durability or it is something very generic just for the sake of it that happens so much and to be honest it rates me but there are also too many dashboards too many alerts uh if someone else creates the alerts or they're just created uh but automatically you have them rigging back and forth and nobody actually looks at them because they know that it is the spam so it is very important for people to go and for engineers to go there and say hey okay I don't need this alert I need this I need to create this and the same for the dashboards it can be thousands and thousands of dashboards and when you actually have an incident that's at midnight three a.m in the morning you wake up your your eyes cannot open and you have to scroll through thousands of those or like try to find a person that might know what this is it's very important to be about as simple as that uh yeah also it is it is not easy it makes it easy I think they go and uh Hand by hand it's it's let's say mostly the same thing with too many alerts and dashboards and you just cannot get the hang of it and you just give up so basically you do not rely on this to troubleshoot them on what do you rely maybe the application logs or it takes you hours to solve an incident which is not uh not great uh either too few or too many Telemetry signals I talked about too many but there is also one thing is to to feel because it is possible that you don't know how metrics are how logs are generated how traces are generated in this case that same place is considering right now it's this huge movement to to make them first class citizens many Engineers let's say they do not know how important tracing is and not how the great capabilities that they can achieve to that and my job as an engineer would be to advise them and to help them with it but not to do it for them because I don't know the type of information they might need it's simple um and it is not cheap uh absolutely it is not cheap depending on the scale of the information that we are having it could rack up to Millions I was reading some articles uh recently about some companies that were paying a huge bill in observability and the question on those articles were who wants to blame and the stack of different building is a team sport it is also the the blame is to share because the engineers of observability are clearly doing a great job at educating or maybe they're not even there that could be one of the cases or the teams are not participating basically you have this framework that just sends 100 of logging tracing metrics and that is all sent somewhere so yeah it's observability is something that is a very uh delicate topic when it comes to costs because so many Engineers consider it as a not a very important thing until they actually need for example when something happens and they don't receive an alert oh my God it is actually important or oh my God the information on that on the dashboard now would be so nice but at the same time we have to know what to collect how to and and to also advise our team to do it and to Empower that to do it themselves it's not an easy task especially when it's so comfortable for them to have all that information let's say for example logging I have a vision of talking in general but nothing against it but it's used so much uh they say okay I just will have the debug logs and even if if something happens I will I will have it there um yeah it's our job to to convince them to show them and to actually make a plan and follow it through together and that's I would say our mission as observability Engineers is not just to to teach people and to show them because sometimes that sounds like I don't know if it is the right word but like stack UPS oh yeah you do it it's not it's not that it's just um trying to do it in a in a collaborative matter to consider it as a teamwork that's a really team it's not an external team that just uh works completely away from you but it's let's say part of your team and at the same time it's not it's it serves as an advisory um wait let's see so how do we change to this observability culture it is very very difficult um I am because of my passion for observability I talk a lot about it and it is very very difficult to convince an engineer with a higher grading as you so to follow a certain observability culture if sometimes it's not even the engineers itself it could be senior leadership it could be higher ups I just do not believe it and some companies and this is very prevalent and it's very difficult to crack in there to actually implement it but I think that our job is to make a change we need to be ambassadors in our companies and I think what we're doing today and all these sessions are a great way for us to talk and to share opinions and to know how to approach our teams better how to to be better at what we're doing and to actually show the importance of our work we need to show the importance of the observability of the data in fact so you can just go and say hey observability a rainbow is amazing beautiful we have to show data and that's very important to collect and very easy as well considering the amount of incidents that are happening or you just show how easy it is to to get information about one aspect of the application that they never thought about it's important to provide up-to-date tools our job is amazing because we have we can work with some someone in modern technologies that are moving so fast at the same time it is very difficult because you need to provide tools that aren't mature enough you have to provide tools that the engineers need and are looking for and sometimes this can get overwhelming but it is part of the challenge and actually this is my favorite aspect of the job how fast it moves and how fast you have to adapt it's part of being an observability engineer uh yeah we have to provide guidelines uh not everyone knows I I mentioned this before but I'm a firm believer of not judging it might sound like a childish thing to say or kindergarten issue but I know that tech people judge each other oh you don't know that much about kubernetes parts of durability it's very important for us to offer help and to avoid Judgment of that sort because observability is not difficult but at the same time it's not easy as I said before there's it's very fast it's moving it's modern you need to read you need to adapt very fast and the person that is completely detached from that will not be able to do it so there is no place for judgment here and I think that if there is Judgment that doesn't make you a good observability engineer I think we're like the Saints of the engineers uh and also we need to give space and time to adapt um in some companies where the durability cultures exist it's easy obviously and if you have a majority of Engineers that follow these practices let's say uh it becomes very easy for it to spread but when you're starting from zero it needs time people that have been working in other companies that have completely different working mentality it's going to be difficult for them to just land in the company and be like ah okay now I accept observability as this amazing thing that it is no we have to give them space and time to adapt show them be patient be there to help to implement even on an engineer that is excellent his job but has no no idea how observable it works we have to we have to be there for them and I think the human aspect of this job is more important than the technical one because it is good it's important to be technically and to adapt and to build but also the human aspect and um spreading the culture and talking to people and being a people's person professionally you don't have to be an extrovert and have this uh beautiful conversations with it's a very important aspect of observability um these are all the slides that I had uh prepared for this presentation but I just want to say my my summary or let's say the message that I have through this is that we all need to work as a team to have great observability culture and great observability system and that's going to make our lives much easier so that's pretty much it I don't know if you have any questions please let me know it was a comment from Derek actually um I think when you're talking about what is important um uh an observability and they mention ability to form Rich queries against the data once it's in your analytics back end is when you're just asking what's important in the design of like a perfect system so you can pop all the perfect data that you want into your back end but if you don't have a way to quickly and effectively query that to get the answers you want then you're just going to be frustrated I think that's one of the reasons why we get frustrated with unstructured Vlogs so much is because you end up having to craft these complicated regexes and pattern matching of unstructured screens and it's really frustrating to try and find what you want so if we can do our part to make sure the data going in is nicely structured it makes it a lot easier to get the signals you want out of it absolutely absolutely agree I had a question too but I can go to the back if someone else have them oh no go ahead Derek I have one but I can I can go after so um I guess my question would be what was the observability culture like when you joined your company um and kind of like what was Ground Zero where were you starting from and then kind of how did you how did you make improvements to that or how did you kind of watch that culture improve so actually I'm pretty lucky when it comes to that extent where in this company when I joined I think the the previous team uh and the just a few members that are currently part of my team had done most of the heavy listing they had started so this the spreading of the observability culture and how to do it correctly so when I joined I think everything was was put in place uh but of course we are improving that day-to-day because you can have a culture there but you have to let's say you have to water it every day and you have to keep it and to improve it so what do we do we have presentation sessions uh with with other engineers in the company showing them how to use our tools best of or what our tools can do we we are always open the team for just a slack message to to help them on how to um create metrics dashboards even though the simplest questions were always there for them we also sharing sharing articles sharing of presentations for observability and it's actually interesting because many people read them and we're also making sure that we're also always on top of the new technologies in this case for example it was open Telemetry it was something that it was very uh well accepted in in far-fetch and some people were actually requested so we provided and and just saying like always providing with a with a news Technologies with the newest updates so nothing is missing and if let's I don't know if it's called I don't know bribing but for example we make sure that it's one of our Engineers who reads something on the Internet it's like oh this new cool feature in grafana that I saw can we have it we make sure that we've had it before we we always make sure that we're on top of things uh because that makes people want to use the system more and use it well and Implement all the all the new changes so it's great for us yeah that's really good if I can ask one more quick follow-up question then I can give someone else a turn um so in your slide deck you talk about how developing ownership right by everybody of observability is super important for Success because ultimately like application Engineers they're the ones who have the context for how their applications run and for the things they need to look for so how in your experience how have you gotten application teams to kind of take that ownership without making it feel like it's just one more thing you're trying to Chuck over the wall at them right because again like it's a unique spot that we're at as like infrastructure platform people because we're not like the best at writing code like we're not that's not going to be our strongest um skill so it feels weird to me at least in the past saying hey can can y'all go learn this thing and put it into practice and I'll coach you from the sidelines but I can't do it as well as you right yeah I think what helps as I said again we are lucky because we have a very good observability cultural already engraved so in most of the engineers just do it but yeah I think what what really helped was that we have given full ownership to to the teams on building their own dashboards alerting and sending their own Telemetry signals so basically uh when they come to us like hey can you help us set up an alert we're like well say you are full owners of that part of the observability and that actually helps uh the teams knowing that they're owners of that they they want to to make it grow and improve it rather than just living it there and we'll we've also made it very clear that we will not be going there and implementing um anything uh related monitoring so if you need monitoring you have to do it yourself we are here for you we provide guidance of course we have tons of documentation on how to do everything they're not alone but we made it very clear that we're not going to do uh that for you it's simple and considering that uh all our incident is coming from the alerting that is set up on our platform let's say they're kind of forced to to to implement it but I think just the fact that uh they're owners of that and it's part of their team responsibility coming from the higher structures of the company okay you'll have this amazing product that you are building maintaining but you also have observability that is your your full ownership so if it's not good then it is your responsibility not somebody else's so I think that has motivated the teams to to really work on there sometimes it happens that when a junior enters the team they're the ones that have to take care of the all the monitoring without knowing well the code but I think they soon realize how important it is for everyone to to put their hands into to contribute there otherwise so does not work out that's great thank you I have a question for you it is um so you're saying like you guys hold your ground as far as like not being the ones to instrument code for folks um what kind of reactions do you get from teams when you say that like are they like oh okay I'll do it or or did they uh like defensive pushback like what what's it like how do you deal with it and most of the time they're like oh okay is there a framework can you send me to the framework so that I can find it and we usually also have a team that helps uh build this framework so we send it to them but most of the time yeah we just hold our ground if they say that oh but I really need to help me like okay I'm gonna help it I don't know how to do it for yourself I don't know your code and of course there are cases that people are not happy and they could go to a higher structure in the company and complain but considering that this is not just something that as the durability team have decided today that oh we're not going to build anything for anyone because it's their responsibility it's not something that it's of course supported and signed by by upper management they're like sorry but you have to do it it's documented and you have to do it and it's simple as that it's great having support that that's awesome because I think you you've like you've nailed it like that is the key thing because you're right like if you don't have the management supporting that decision to like you know have a visibility team we're not going to instrument your code like yeah you're gonna have like back channeling and I mean I've I've experienced that it's really annoying um and and ends up kind of like ruining the thing that you're trying to build like as far as that observability culture yeah exactly uh as I said yeah in farfetch we are very very happy in this aspect because uh yeah being forced to instrument somebody's code that would be the the breaking point for me as an engineer in believing that observability is lived preaching with everyone and talking about it and then being forced to to do to go against these values that that we have that would be the breaking point for me I have to stay yeah I don't blame you um I have a question um so far-fetch is an e-commerce site um correct and so I was curious and Derek I guess actually this question is um inspired by your question in the hotel end users Channel this morning how do you um or do you like what do you use for a client-side instrumentation are you um well we are using a vendor currently um I'd rather not say uh but yeah we're using a vendor for that for that part which is very very small the rest the the rest of the platform is all instrumented by our own custom framework okay okay do you have that integrated at all or is it kind of like its own view of the client and then you have trouble correlating signals with your back end uh currently it is a bit segregated but we're working on on changing it because it's such a small part of our platform but yeah it's not fully fully integrated uh we're focusing more on the let's say on our own custom solution that we have locally and that one is a bit segregated from it and it has only specific information which is not great but we that's our goal for 2023 and 2024 to integrate everything together what's yours what's your custom solution involve like what what makes it custom I guess is the question I mean it's it's uh using open Telemetry now uh we're using thousands Prometheus grafana alert manager basically all open source and we built it uh let's say we have around 100 kubernetes clusters we have created our agent based on this open source uh and just adding them on each of the classes collecting information that's centralizing it oh so custom solution like you're talking specific specifically around tooling and not like I thought maybe you meant like um like having some like abstraction layer on top of like hotel which I've seen some organizations do no no no no okay okay cool any other thoughts or questions for for edius yeah I I guess I'm a little curious so you said you know there's no such thing as like too many traces and I know some people you know prefer to do some form of tail sampling so they're just they're still getting like a subset of you know randomized 200s and then like you know whatever chases with errors or latency or whatever attributes that they might be interested in um so I was just curious if you could expound on that a little bit more especially like you know when talking about like um increased costs with excessive amounts of data and yeah so tracing is something that we are working a lot because it's not one of the Telemetry signals that it's being used a lot in in the company so we are trying to Advocate it and push it a lot and for your when I say too few traces I would say uh 0.01 sampling like normal sampling let's say in younger so the teams were considering it extremely low and they weren't even caring about it even though some of them you could find very good information there for example I use it a lot to troubleshoot our Thanos queries it's amazing um so where we are right now we increased because we were able to change our our background back-end solution from Cassandra to grafana Temple which is also open source so of course because before we were spending a crazy amount of money for 0.01 tracing we were spending so much money I don't want to say numbers so there was no place for us to grow more it was completely out of budget so now we implemented Tempo and we are able to send more traces and we increase to five percent and increasing that just a few applications and actually uh implemented this change because of course it's a custom framework and they can they can do it however they want and we already went for 1 000 uh spans per second to 40 000 from just one application and uh this application is not even using most of what they're sending and imagine we have hundreds of applications all of this sending all that is going to be millions of spawns per second it's going to require a lot of computation power and it's going to require a lot of storage it's going to be a mess and uh the cost will be out of this world in in conclusion so I think that we really need to go there like which we're in the process of doing and try to see to implement better uh better tracing because just normal sampling is not working yeah we're looking into a tail base that's why open Telemetry was uh brought up in the first place so the perfect to look at the tail base and yeah we were sending too little or basically there was not enough information now we're sending too much so we are trying to find something that is uh in between until sampling is one of them and also we're also making changes to the framework so not everything goes through some of the information is just not usable and the engineers are not needed yeah it's a it's a work in progress that is taking a lot of our time I I'd rather spend more money and send more information obviously but at the same time we could not spend millions of dollars a year for it for information it's not going to be used I see a raised hand I did um I wanted to ask if you're doing anything around anomaly detection especially as you increase the volume of information coming in to avoid having to look at everything to look at those anomalies that are out of the time frame uh you mean anomaly detection in the the amount yeah well so the scenario of you know here's a here's a high volume event but it's normal and here's a high volume at what's normally a low time of day right just to look for anomalies and everything that you're receiving without having to have a human look at it Yeah we actually have some alerting implemented there based on standard deviation and uh it's like a simple prompt ql based alerting for that and there are accurate but so we haven't gone about that because it's a custom solution and we actually need to apply some machine learning there which it's uh we haven't really been able to get to it so it's do some alerts with the standard deviation that are okay uh they they can detect some anomalies and some very big changes but also are highly inaccurate so it's not my favorite but they are there we need to to work better on it but yeah for example if we have a huge anomaly or a huge flow of sudden that is not uh predicted it is okay let's say it's not perfect I know it's you can get into some really um interesting your Solutions with that that can't come with a high cost but there's some really interesting things you can do with that too so I would love to know more if you have any links any suggestions for me please let me know because the more solutions we can implement hahaha like you mentioned cost cost is always a worry obviously but if it's actually a quality solution that we are introducing uh of course it is uh a cost that is needed and that's it you know it's it's really enjoyable when you start to use tools like that to um solve the problems because then you see things you didn't expect to see um we were experimenting with something a few years ago and we intended to point at the top 10 um issuers and you know the engineer mistakenly didn't put the top 10 in so he was actually looking for anomalies across the entire user set so you know thousands of um thousands of customers and but they were in entities so you know you can start to see oh look they're not sending at this time of day when they usually are that's a low volume low Center but they're sending nothing and they usually since 70 per hour um and we could see that before it finally blew up from running out of memory but uh you know you really get to say now what can I do with that for my business so that was interesting we presented to them at the the team who wanted to own that didn't want to develop the expertise and that's tooling even though we could show them what it could do so a 500 gigabyte an ec2 instance was running that before it blew up yeah a challenge that we have currently is with our Matrix as well and uh detecting which applications are sending higher technology more number of times series and we have dashboards and alerts for that but nothing is based on machine learning it's just pure statistical data uh and uh it is being evaluated for standard deviation or simple query so some some more inside there would be amazing yeah that's the part that we always worry about too is you know we've got so much data to look at which one's important so you don't look at any other you put your tools against it um yeah that too much data problem is is very interesting to have especially as um the open celebrity standards evolve and add more Fields so now you have more fields to consider yeah and they're good it's good that those are coming absolutely yeah one thing that I wanted to ask you is um is your team involved at all in the creation of slos foreign yes but also no well let's say we are the ones that are providing the tool for creating slos okay uh so basically the teams can implement the structure or basically create alerting dashboards based on it but we are not the ones that are providing the guidelines and the instructions on how to create them we just provide the tooling is it um is it like an open source tool that you guys are using or is it some like commercials we're actually using uh we have our own alerting solution so we are doing the same uh for slos as well and we are currently using tunnels ruler to send the alerts and to evaluate them uh alert manager to send the alerts and we also have a custom tool that reads the slos that are written in terraform and make them compatible with a s permitus rule satanos roller can read them okay and do you um so the reason why I'm asking specifically about slos is because um one of the I'd say one of the practices a team should aim for is like using observability data um to help generate their slos or not generate to you know create create their their slos based on on observability data so I'm just wondering if that's something that uh your team is planning on providing guidance on um like what's are there any plans around that so basically we are providing the data and the tooling to do it uh most of the guidance are like how the SLO should be built most of them are our business and we're not really involved in that part so we cannot give guidelines so that's completely different but yeah for example for infrastructure we provide some basic guidelines of slos but I would say that yeah we just want to provide the the data they feed off of our metrics and the tooling that they can do it and how they can actually build them but know not really how we can go how they can go and uh or like how to calculate the slos or what is best and what is worse and we haven't gotten there I don't think we have the plans currently to to move into that as we have other teams that give advice or recommendations on that matter there is so much uh so much to do in in observability in in our company I think we've done a pretty good job so far but there is so much more that we can do is there something that like your team is is working on implementing um like that you're excited about in the in the near future well uh we decided that because of the huge amount of data and uh we do not have capability to to process it as as good as we would like we are currently trying to see if we can well let's let me put some precedent we are using different tools for different things we're using permitting styles for Matrix uh we're using open Telemetry for traces and open Telemetry for metrics to a certain degree as well uh grafana for dashboards and we're using a different tool for for logging and currently it's all a mess so what we're trying to do right now is we want to centralize them in one platform what we don't know yet if it's going to be an outside platform that we are feeding the data to our like a vendor or if we're going to to provide it ourselves so that's something that we're working on right now centralizing everything in one place and sending all um Telemetry signals through one transporter that they open Telemetry so we could have better correlation which we are also lacking currently awesome so exciting times it is amazing I love I love the work that you're doing right now and it's going to be the next two years at least I know after that it's going to be a very exciting for us well it's very very awesome um we're coming up on time um but I did want to give folks an opportunity to ask any other final burning questions nope everyone's questioned out thank you everyone so much for for joining and if uh anyone who's uh in in the audience is interested in um in in giving a presentation for hotel and practice or would like to participate in one of our hotel q and A's please reach out on slack we are more than happy to hear your stories share your stories so that we can continue to build this amazing open Telemetry Community thank you so much for having me today

