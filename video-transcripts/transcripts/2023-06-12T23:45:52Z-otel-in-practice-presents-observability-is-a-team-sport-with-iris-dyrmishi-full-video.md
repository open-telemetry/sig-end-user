# OTel in Practice Presents: Observability is a Team Sport with Iris Dyrmishi [FULL VIDEO]

Published on 2023-06-12T23:45:52Z

## Description

Observability ins't the responsibility of one single team. Join Iris Dyrmishi of Farfetch as she talks about how the collaboration ...

URL: https://www.youtube.com/watch?v=U1yLXnMONkc

## Summary

In this YouTube video, Iris, a platform engineer with a focus on observability at Farfetch, leads a discussion about the importance of observability culture in engineering teams. She emphasizes that observability should be a shared responsibility among all engineers, not just a centralized team, and discusses the characteristics of a perfect observability system. Key points include the need for a centralized view of telemetry data, optimized data usage to control costs, and the crucial role of each engineering team in setting up their own monitoring tools. Iris highlights the necessity for effective incident response and detection, the importance of collaboration in creating custom alerts, and the need for engineers to take ownership of their observability practices. Throughout the conversation, she encourages open dialogue and exchange of experiences among attendees, reinforcing that a robust observability culture is essential for operational success. The video features questions and interactions from participants, including discussions on practical tools and strategies for improving observability within organizations.

## Chapters

00:00:00 Introductions
00:01:04 Observability as a team sport
00:03:28 Characteristics of a perfect observability system
00:05:32 Importance of observability engineers
00:06:56 Tools for observability: OpenTelemetry, Grafana, Prometheus
00:08:40 Incident response and detection
00:10:24 Custom alerts and dashboards
00:12:08 Importance of team involvement in observability
00:20:48 Observability culture at Farfetch
00:39:52 Anomaly detection and machine learning in observability

## Transcript

**Iris:** Thank you for joining us again. We had her for the Q&A last month, a couple of weeks ago, I guess, and she did a bang-up job. So we asked her to come back, and I'll let you take it away.

### [00:01:04] Observability as a team sport

**Iris:** Well, hello everyone. Thank you so much for joining today. My name is Iris. I'm a platform engineer with a focus on observability working at Farfetch currently, and I'm a super passionate person when it comes to observability. I like to talk about it as much as I can, and I like to share the observability culture all over, not just in my company. So today I wanted to have a talk, and I would like to have it as an open discussion. I would love to hear about the observability culture in your companies as well. I believe that observability is a team sport, and it is not something that is being done just by one central team in a company, no matter how big or small it is. I think everyone should put their hands in and be involved in the observability. 

But of course, let's say observability has been there forever, but now it's becoming this huge thing, and now we have this amazing culture that is spreading, but we're still not there. I feel like this conversation and these discussions are so important to put out there for us to discuss and then to take them to our companies and to learn obviously from each other. 

So yeah, this was just a small intro. Please interrupt me because I do not have good visibility. If you have a question, please don't hesitate to unmute yourself and just speak, or if you have any opinions about something that I'm saying, I just want to have a spoiler alert. These are mostly things that I've learned from my job and things that I believe in. So of course, if you have a completely different opinion, please let me know. Let's discuss it. It would be amazing to have a conversation about this here.

Okay, so the first question that I ask all engineers that are being interviewed for a job in my team is, "Whose responsibility is observability?" Because let's be honest, observability is being implemented differently in many different companies. I've seen so many ways everywhere that I've interviewed for. It’s different. So of course, observability is everyone's responsibility. It's simple. If your engineer says that answer to me, then I'm like, "Okay, okay, we're on board, we're getting somewhere." 

### [00:03:28] Characteristics of a perfect observability system

And why do I believe this? First of all, in order to get deeper into why I believe that everyone is responsible for the durability in their company, I want to give an overview of what I think a perfect observability system is. And that's perfect, let's take it with a grain of salt. Of course, it's perfect for me; my company works, and in another person's company it doesn't work. So if you have any more ideas here, this was just what came to the top of my mind.

For me, a perfect observability system has a centralized view of systems observability. Data engineers can go and they can access their traces, metrics, logs, dashboards, alerts everywhere. They don’t have to memorize a lot of URLs or have to go to many places to find the information. So that's the first thing. The second one is optimized data will optimize costs because doing observability doesn’t necessarily mean that you will send everything that you can and rack up huge bills, but it needs to be optimized data and some good old data that is actually going to help other teams troubleshoot and monitor their systems. 

Obviously, reliable alerting, reliable and custom dashboards, rich traces, correlation between the different telemetry signals—so this is something that makes a perfect observability system for us. My job as an observability engineer, especially in perfect, is to provide that for the engineers. But I cannot do it alone because no matter how much I try, most of these characteristics are teamwork and something that other people also have to contribute, which means the engineers that are part of, I would say, our customers. I don’t know if anyone has any other ideas about what a perfect observability system is for them. I would love to hear more about it, or if you completely agree with my goals of having this system, but at least I'm with yourself and with them throughout me any moment.

**Audience Member:** Thank you! I definitely agree with a lot of the things that you said; like it just, yeah, spot on. 

**Iris:** Okay, sorry, I'm not able to see the comments here, but if there is something interesting, do let me know, please. 

### [00:05:32] Importance of observability engineers

Okay, so the question is, okay, you have this vision of a perfect observability system—how do we get to this observability system? Well, observability engineers like myself, my team, with the knowledge to shape observability systems into the correct path. I'm not saying that observability is something that a simple engineer that is working can't get into; they can be participants and active participants in it. But of course, it is important that there are engineers who know observability better than everyone, who know what guidelines to implement, what tools to use, or how to optimize data. It's very important that these people exist in the company because, as I said before, observability is one of those fields that is moving so fast that it is impossible for a person that is working, let’s say, with databases or in Java, and their full-time job is called getting back-end whatever else to keep up with this. 

### [00:06:56] Tools for observability: OpenTelemetry, Grafana, Prometheus

So it is important for people like us to be in the company and to preach, let's say, these values and to bring all these good tools and good things that are being developed and are being shared in the community. Of course, as I mentioned, observability engineers also bring the data to collect and process telemetry data. But my fourth favorite so far, of course, is OpenTelemetry, which I'm a huge fan of. That's why we're here after all. I'm a huge fan. We have Grafana, Prometheus—that are some of the tools that are open source that are being used so massively right now by observability engineers. 

And now I'm getting to what the rest of the presentation will be about. It's not only observability engineers that are needed; teams need to actively participate in optimizing the data that they're sending and things, actively building their monitoring. Five people, ten people, twenty, depending on the size of the observability team, cannot do the work of two thousand, three thousand, depending on the work of so many engineers. Everyone needs to have control of their own code. 

Some of the reasons why observability is so important to be done as a team is because of how crucial it is in a company. In some companies, it’s still not understood how important it is to have observability. But that is, well, what I believe is because of the small scale. The more that the companies grow, they understand how important it is to have a highly available and reliable system. It is crucial to have optimized performance because you could allocate a lot of resources and you are not saving on costs. Basically, you're spending too much money or you're cutting back on resources, and you are basically not giving a good performance or a good experience to your users, depending on the type of product. 

### [00:08:40] Incident response and detection

Also, it is crucial for incident response. This is something that is very important. In my company, incident response is very important considering that we have a lot of customers that rely on our product—very expensive products at times. So having all the teams involved in the crucial incident response is extremely important. We have around two thousand engineers. I actually thought that we were three thousand; we were actually two thousand that are working in completely different areas, some of them I don’t even know except for what they are. So when I hear about them, I'm like, "Wow, okay," because it's so big and so diverse in our company. 

If these engineers were not involved in the part of building their own dashboards, building their own alerts, knowing what data they're sending for their observability reasons, they would not be able to respond to an incident in time. Simple as that. If I was the one making the alerting for them, it would be something extremely generic with some thresholds that I just thought would be right because I don’t know their products or their code, obviously. The dashboards would be completely generic. But if those were done by a person inside the team itself, it would be so easy for them because they know it inside out, their work, what some of the issues that they might have. 

### [00:10:24] Custom alerts and dashboards

So if an incident happens, you reduce the response from hours that it would be if it was for me dealing with their incident to seconds or minutes if someone from their engineering team is involved. Also, it is crucial for incident detection. As I said before, us as observability engineers can provide a set of alerting dashboards that are very generic, that each team can have as a baseline to implement their monitoring system, but I don’t feel comfortable in a way to go and implement custom alerts, custom dashboards for these teams. 

Again, it goes to this: even if I wanted to, there are thousands and thousands of engineers, technologies, hundreds of teams. I would never be able to do that properly. That's why it's so important that each team is involved and does it themselves. If they need it, they change it; they don’t need our permission. They are owners of that solution. It’s extremely important and also critical for auto-scaling. I say this because in Farfetch, we rely heavily on telemetry data to auto-scale because you never know the amount of traffic that you're going to have in your application. 

### [00:12:08] Importance of team involvement in observability

But for example, let’s say for me or for another person in observability, if we were to set some auto-scaling rules for you, it would be based on CPU or memory. It would be something completely generic, but an engineer that knows their system so well would go and say, "Let’s do a custom auto-scaling depending on the number of throughput," or whatever. Basically, my bottom goal here would be to say that all this requires people from the teams that are monitoring the system to take part in. If it is done by people that are outside—I am an infrastructure engineer. I work with Kubernetes, I work with VMs, with Azure, with cloud, and I'm not very good at back-end coding, let's say. So if I was the one doing this, I wouldn’t do it properly, no matter how much I tried and how much effort I put. 

So let’s not do observability for the sake of it. Yeah, let’s have some generic alerts just for having them. No, let’s do it properly. Let's have the people that are working day-to-day with the product implement them, make it their baby as well. It's very important to show it to the engineers. I am very proud to say that in my company we have a pretty good observability culture that has started a few years back. Now it is widely accepted, but I've seen it happen that teams rely heavily on other people to set up alerting for them, and that is something that scares me. If something happens, if an incident happens, I wouldn't be comfortable working in a team like that if I wasn't the one setting up my own monitoring for the system. 

Also, observability can be so messy depending on the scale of the company. So many people come and go. Everyone wants to add the custom metric, the custom log. It can get extremely messy. It can get too many metrics, too many logs, especially, for example, living debugging logs flowing all day. You know, because it can—yeah, you just sense it. I’m an engineer; I want to have full observability of my application. It's not affecting my performance, so I'm sending everything: debugging info, warning metrics. I create a bunch of them on top of each other, or the same metric with different names. 

What can I, as an observability engineer, do about it alone? I detect it. I see it, and I say, "Okay, this is completely wrong." But of course, I'm going to need the collaboration of the engineers that are working with our product to actually change it. Otherwise, it would never be optimized, and it should never be, let's say, good observability. Too many traces—well, I'm a firm believer that there is no such thing as too many traces. I'm a big advocate of that. But yeah, it's possible that we are sending 100% of traces with a mess and completely no information there. 

What can I do considering that I'm not the one doing the day-to-day developing? I have to go and talk to the teams that they need to be active participants in that. Without it, for me, there is no durability, or it is something very generic just for the sake of it. That happens so much, and to be honest, it irks me. But there are also too many dashboards, too many alerts. If someone else creates the alerts or they're just created automatically, you have them ringing back and forth, and nobody actually looks at them because they know that it is spam. 

So it is very important for people to go, and for engineers to go there and say, "Hey, okay, I don't need this alert. I need this. I need to create this." And the same for the dashboards. It can be thousands and thousands of dashboards, and when you actually have an incident at midnight, 3 a.m. in the morning, you wake up, your eyes cannot open, and you have to scroll through thousands of those or try to find a person that might know what this is. 

It's very important to be about as simple as that. Yeah, also, it is not easy. It makes it easy. I think they go hand in hand. It’s, let’s say, mostly the same thing with too many alerts and dashboards, and you just cannot get the hang of it, and you just give up. So basically, you do not rely on this to troubleshoot. What do you rely on? Maybe the application logs? Or it takes you hours to solve an incident, which is not great either. 

Too few or too many telemetry signals—I talked about too many, but there is also one thing: it is possible that you don't know how metrics are generated, how logs are generated, how traces are generated. In this case, that same place is considering right now. It's this huge movement to make them first-class citizens. Many engineers, let’s say, do not know how important tracing is and the great capabilities that they can achieve with that. My job as an engineer would be to advise them and to help them with it, but not to do it for them because I don't know the type of information they might need. It's simple. 

And it is not cheap. Absolutely it is not cheap. Depending on the scale of the information that we are having, it could rack up to millions. I was reading some articles recently about some companies that were paying a huge bill in observability, and the question in those articles was, "Who wants to blame?" The stack of different billing is a team sport. The blame is shared because the engineers of observability are clearly doing a great job at educating, or maybe they're not even there. That could be one of the cases. Or the teams are not participating. Basically, you have this framework that just sends 100% logging, tracing, metrics, and that is all sent somewhere. 

So yeah, observability is something that is a very delicate topic when it comes to costs because so many engineers consider it as not a very important thing until they actually need it. For example, when something happens and they don't receive an alert, "Oh my God, it is actually important!" Or, "Oh my God, the information on that dashboard now would be so nice." But at the same time, we have to know what to collect, how to do it, and to also advise our team to do it and to empower them to do it themselves. 

It's not an easy task, especially when it's so comfortable for them to have all that information. Let's say, for example, logging. I have a vision of talking in general, but nothing against it, but it's used so much. They say, "Okay, I just will have the debug logs, and even if something happens, I will have it there." Yeah, it's our job to convince them, to show them, and to actually make a plan and follow it through together. 

I would say our mission as observability engineers is not just to teach people and to show them because sometimes that sounds like, I don't know if it is the right word, but like stack-ups, "Oh yeah, you do it." It's not that. It's just trying to do it in a collaborative manner to consider it as a teamwork. It's a team; it's not an external team that just works completely away from you, but it's, let's say, part of your team. And at the same time, it's not—it serves as an advisory.

So how do we change to this observability culture? It is very, very difficult. Because of my passion for observability, I talk a lot about it, and it is very, very difficult to convince an engineer with a higher grading as you to follow a certain observability culture if sometimes it's not even the engineers themselves. It could be senior leadership, it could be higher-ups. I just do not believe it in some companies, and this is very prevalent, and it's very difficult to crack in there to actually implement it. 

But I think that our job is to make a change. We need to be ambassadors in our companies, and I think what we're doing today and all these sessions are a great way for us to talk and to share opinions and to know how to approach our teams better, how to be better at what we're doing, and to actually show the importance of our work. We need to show the importance of the observability of the data. In fact, you can just go and say, "Hey, observability, a rainbow is amazing, beautiful!" We have to show data, and that's very important to collect and very easy as well considering the amount of incidents that are happening. 

### [00:20:48] Observability culture at Farfetch

Or you just show how easy it is to get information about one aspect of the application that they never thought about. It's important to provide up-to-date tools. Our job is amazing because we can work with some modern technologies that are moving so fast. At the same time, it is very difficult because you need to provide tools that aren’t mature enough. You have to provide tools that the engineers need and are looking for, and sometimes this can get overwhelming. But it is part of the challenge, and actually, this is my favorite aspect of the job: how fast it moves and how fast you have to adapt. It's part of being an observability engineer. 

We have to provide guidelines. Not everyone knows; I mentioned this before, but I'm a firm believer of not judging. It might sound like a childish thing to say or a kindergarten issue, but I know that tech people judge each other. "Oh, you don't know that much about Kubernetes parts of durability." It's very important for us to offer help and to avoid judgment of that sort because observability is not difficult, but at the same time, it's not easy. 

As I said before, it's very fast, it's moving, it's modern. You need to read; you need to adapt very fast, and the person that is completely detached from that will not be able to do it. So there is no place for judgment here, and I think that if there is judgment, that doesn't make you a good observability engineer. I think we’re like the saints of the engineers. 

And also, we need to give space and time to adapt. In some companies where the durability cultures exist, it's easy, obviously. If you have a majority of engineers that follow these practices, let’s say, it becomes very easy for it to spread. But when you're starting from zero, it needs time. People that have been working in other companies that have completely different working mentalities, it's going to be difficult for them to just land in the company and be like, "Ah, okay, now I accept observability as this amazing thing that it is." 

No, we have to give them space and time to adapt, show them, be patient, be there to help, to implement. Even an engineer that is excellent at his job but has no idea how observability works, we have to be there for them. I think the human aspect of this job is more important than the technical one. It’s important to be technically savvy and to adapt and to build, but also the human aspect and spreading the culture and talking to people and being a people’s person professionally. You don't have to be an extrovert and have these beautiful conversations with everybody; it’s a very important aspect of observability.

These are all the slides that I had prepared for this presentation, but I just want to say my summary, or let's say the message that I have through this, is that we all need to work as a team to have a great observability culture and a great observability system, and that's going to make our lives much easier. 

So that's pretty much it. I don't know if you have any questions, please let me know.

**Audience Member:** It was a comment from Derek, actually. I think when you're talking about what is important in observability, they mention the ability to form rich queries against the data once it's in your analytics backend. 

**Derek:** When you're just asking what's important in the design of like a perfect system, you can pop all the perfect data that you want into your backend, but if you don't have a way to quickly and effectively query that to get the answers you want, then you're just going to be frustrated. I think that's one of the reasons why we get frustrated with unstructured logs so much is because you end up having to craft these complicated regexes and pattern matching of unstructured screams, and it's really frustrating to try and find what you want. 

So if we can do our part to make sure the data going in is nicely structured, it makes it a lot easier to get the signals you want out of it.

**Iris:** Absolutely, absolutely agree. I had a question too, but I can go to the back if someone else has them.

**Audience Member:** Oh no, go ahead, Derek. I have one, but I can go after.

**Derek:** I guess my question would be what was the observability culture like when you joined your company? And kind of like what was Ground Zero? Where were you starting from? And then kind of how did you make improvements to that? Or how did you kind of watch that culture improve?

**Iris:** So actually, I'm pretty lucky when it comes to that extent where in this company, when I joined, I think the previous team and just a few members that are currently part of my team had done most of the heavy lifting. They had started the spreading of the observability culture and how to do it correctly. So when I joined, I think everything was put in place. But of course, we are improving that day-to-day because you can have a culture there, but you have to, let’s say, you have to water it every day, and you have to keep it and improve it. 

So what do we do? We have presentation sessions with other engineers in the company, showing them how to use our tools best, or what our tools can do. We are always open—the team—for just a Slack message to help them on how to create metrics dashboards. Even the simplest questions, we are always there for them. We are also sharing articles, sharing presentations for observability. And it's actually interesting because many people read them. We are also making sure that we're always on top of the new technologies. In this case, for example, it was OpenTelemetry. It was something that was very well accepted in Farfetch, and some people actually requested it, so we provided it. 

Just saying, we are always providing with new technologies, with the newest updates so nothing is missing. And if—let's—I don’t know if it’s called bribing, but for example, we make sure that it’s one of our engineers who reads something on the Internet, it’s like, "Oh, this new cool feature in Grafana that I saw. Can we have it?" We make sure that we have it before we always make sure that we’re on top of things because that makes people want to use the system more and use it well and implement all the new changes. So it's great for us.

**Audience Member:** Yeah, that's really good. If I can ask one more quick follow-up question, then I can give someone else a turn. In your slide deck, you talk about how developing ownership by everybody of observability is super important for success because ultimately, like application engineers, they’re the ones who have the context for how their applications run and for the things they need to look for. 

So how, in your experience, have you gotten application teams to kind of take that ownership without making it feel like it’s just one more thing you’re trying to chuck over the wall at them? Right? Because again, like it’s a unique spot that we’re at as infrastructure platform people because we’re not like the best at writing code. Like, that’s not going to be our strongest skill. So it feels weird to me, at least in the past, saying, "Hey, can y'all go learn this thing and put it into practice, and I’ll coach you from the sidelines, but I can’t do it as well as you."

**Iris:** Yeah, I think what helps, as I said again, we are lucky because we have a very good observability culture already ingrained. So in most of the engineers, they just do it. But yeah, I think what really helped was that we have given full ownership to the teams on building their own dashboards, alerting, and sending their own telemetry signals. So basically, when they come to us like, "Hey, can you help us set up an alert?" We’re like, "Well, say, you are full owners of that part of the observability." 

That actually helps the teams knowing that they’re owners of that. They want to make it grow and improve it rather than just leaving it there. We’ve also made it very clear that we will not be going there and implementing anything related to monitoring. So if you need monitoring, you have to do it yourself. We are here for you; we provide guidance. Of course, we have tons of documentation on how to do everything. They’re not alone, but we made it very clear that we’re not going to do that for you. It’s simple. And considering that all our incidents are coming from the alerting that is set up on our platform, let’s say they’re kind of forced to implement it. 

But I think just the fact that they’re owners of that and it’s part of their team responsibility coming from the higher structures of the company—"Okay, you’ll have this amazing product that you are building and maintaining, but you also have observability that is your full ownership. So if it’s not good, then it is your responsibility, not somebody else’s." I think that has motivated the teams to really work on that. Sometimes it happens that when a junior enters the team, they’re the ones that have to take care of all the monitoring without knowing well the code. But I think they soon realize how important it is for everyone to put their hands in to contribute; otherwise, it does not work out.

**Audience Member:** That's great, thank you. I have a question for you. It is, you're saying like you guys hold your ground as far as not being the ones to instrument code for folks. What kind of reactions do you get from teams when you say that? Like, are they like, "Oh, okay, I'll do it," or did they like defensive pushback? What’s it like? How do you deal with it?

**Iris:** Most of the time, they're like, "Oh, okay. Is there a framework? Can you send me to the framework so that I can find it?" We usually also have a team that helps build this framework, so we send it to them. But most of the time, yeah, we just hold our ground. If they say, "Oh, but I really need help," we’re like, "Okay, I'm gonna help you. You need to do it for yourself. I don’t know your code." 

Of course, there are cases that people are not happy, and they could go to a higher structure in the company and complain. But considering that this is not just something that the observability team has decided today, that "Oh, we're not going to build anything for anyone," because it’s their responsibility, it’s not something that is, of course, supported and signed by upper management. They’re like, "Sorry, but you have to do it. It’s documented, and you have to do it." And it’s simple as that.

**Audience Member:** That’s great having support. That’s awesome because I think you've nailed it. That is the key thing because you're right: if you don’t have the management supporting that decision to have an observability team, we're not going to instrument your code—yeah, you're gonna have back-channeling. I mean, I've experienced that. It's really annoying and ends up kind of ruining the thing that you're trying to build as far as that observability culture.

**Iris:** Yeah, exactly. As I said, in Farfetch, we are very, very happy in this aspect because being forced to instrument somebody’s code—that would be the breaking point for me as an engineer in believing that observability is lived preaching with everyone and talking about it and then being forced to go against these values that we have—that would be the breaking point for me. I have to stay.

**Audience Member:** Yeah, I don't blame you. I have a question. Farfetch is an e-commerce site, correct? 

**Iris:** Yes.

**Audience Member:** I was curious, and Derek, I guess actually this question is inspired by your question in the hotel end users channel this morning: how do you, or do you, like what do you use for client-side instrumentation?

**Iris:** We are using a vendor currently. I’d rather not say, but yeah, we’re using a vendor for that part, which is very, very small. The rest of the platform is all instrumented by our own custom framework.

**Audience Member:** Okay. Do you have that integrated at all, or is it kind of like its own view of the client, and then you have trouble correlating signals with your backend?

**Iris:** Currently, it is a bit segregated, but we're working on changing it because it's such a small part of our platform. But yeah, it's not fully integrated. We're focusing more on our own custom solution that we have locally, and that one is a bit segregated from it and it has only specific information, which is not great. But that’s our goal for 2023 and 2024—to integrate everything together.

**Audience Member:** What does your custom solution involve? What makes it custom, I guess is the question.

**Iris:** It’s using OpenTelemetry now. We’re using Prometheus, Grafana, Alertmanager—basically all open source—and we built it. Let’s say we have around 100 Kubernetes clusters. We have created our agent based on this open source and just adding them on each of the clusters, collecting information, centralizing it.

**Audience Member:** Oh, so custom solution, like you’re talking specifically around tooling and not like—I thought maybe you meant like having some abstraction layer on top of OpenTelemetry, which I’ve seen some organizations do.

**Iris:** No, no, no.

**Audience Member:** Okay, okay, cool. Any other thoughts or questions for Iris?

**Audience Member:** Yeah, I guess I’m a little curious. So you said, you know, there’s no such thing as like too many traces, and I know some people, you know, prefer to do some form of tail sampling. So they’re just—they’re still getting like a subset of, you know, randomized 200s and then like, you know, whatever traces with errors or latency or whatever attributes that they might be interested in. 

So I was just curious if you could expound on that a little bit more, especially like, you know, when talking about like increased costs with excessive amounts of data.

**Iris:** Yeah, so tracing is something that we are working a lot on because it's not one of the telemetry signals that are being used a lot in the company. So we are trying to advocate it and push it a lot. And for your—when I say too few traces, I would say 0.01 sampling, like normal sampling, let’s say. In younger teams, they were considering it extremely low, and they weren’t even caring about it even though some of them you could find very good information there. 

For example, I use it a lot to troubleshoot our Thanos queries—it's amazing! So where we are right now, we increased because we were able to change our backend solution from Cassandra to Grafana Tempo, which is also open source. So of course, because before, we were spending a crazy amount of money for 0.01 tracing. We were spending so much money. I don’t want to say numbers. There was no place for us to grow more; it was completely out of budget. 

So now we implemented Tempo, and we are able to send more traces, and we increased to five percent, increasing that just a few applications. We actually implemented this change because, of course, it’s a custom framework, and they can do it however they want. We already went from 1,000 spans per second to 40,000 from just one application, and this application is not even using most of what they're sending. 

Imagine we have hundreds of applications, all of this sending—that is going to be millions of spans per second. It’s going to require a lot of computation power, and it’s going to require a lot of storage. It’s going to be a mess, and the cost will be out of this world. 

In conclusion, I think that we really need to go there. Like, we’re in the process of doing it and try to see to implement better tracing because just normal sampling is not working. Yeah, we're looking into tail-based—that's why OpenTelemetry was brought up in the first place. 

So the perfect would be to look at the tail-based, and yeah, we were sending too little or basically there was not enough information. Now we're sending too much, so we are trying to find something that is in between. Tail sampling is one of them, and also we're also making changes to the framework so not everything goes through. Some of the information is just not usable, and the engineers are not needed. 

Yeah, it's a work in progress that is taking a lot of our time. I’d rather spend more money and send more information, obviously, but at the same time, we could not spend millions of dollars a year for information that’s not going to be used.

**Audience Member:** I see a raised hand.

**Audience Member:** I did! I wanted to ask if you’re doing anything around anomaly detection, especially as you increase the volume of information coming in to avoid having to look at everything—look at those anomalies that are out of the time frame?

**Iris:** You mean anomaly detection in the amount?

**Audience Member:** Well, so the scenario of, you know, here’s a high-volume event, but it’s normal, and here’s a high volume at what’s normally a low time of day, right? Just to look for anomalies and everything that you’re receiving without having to have a human look at it.

### [00:39:52] Anomaly detection and machine learning in observability

**Iris:** Yeah, we actually have some alerting implemented there based on standard deviation, and it’s like a simple PromQL-based alerting for that. And they’re accurate, but we haven’t gone about that because it’s a custom solution, and we actually need to apply some machine learning there, which we haven’t really been able to get to. 

So it’s doing some alerts with the standard deviation that are okay. They can detect some anomalies and some very big changes, but also are highly inaccurate, so it’s not my favorite, but they are there. We need to work better on it, but yeah, for example, if we have a huge anomaly or a huge flow of sudden that is not predicted, it is okay. Let’s say it’s not perfect. 

I know you can get into some really interesting solutions with that that can come with a high cost, but there are some really interesting things you can do with that too. So I would love to know more if you have any links, any suggestions for me, please let me know because the more solutions we can implement!

**Audience Member:** Like you mentioned, cost is always a worry, obviously, but if it’s actually a quality solution that we are introducing, of course it is a cost that is needed.

**Audience Member:** And that’s it, you know? It’s really enjoyable when you start to use tools like that to solve the problems because then you see things you didn’t expect to see.

We were experimenting with something a few years ago, and we intended to point at the top 10 issuers. And you know, the engineer mistakenly didn’t put the top 10 in, so he was actually looking for anomalies across the entire user set. So, you know, thousands of customers, and we could start to see, "Oh look, they’re not sending at this time of day when they usually are—that’s a low volume, low center, but they’re sending nothing when they usually send 70 per hour." 

And we could see that before it finally blew up from running out of memory. But, you know, you really get to say, "Now what can I do with that for my business?" So that was interesting. We presented to them at the team who wanted to own that but didn’t want to develop the expertise and that tooling, even though we could show them what it could do. 

So a 500-gigabyte EC2 instance was running that before it blew up.

**Iris:** Yeah, a challenge that we have currently is with our metrics as well and detecting which applications are sending higher technology, more number of times series. We have dashboards and alerts for that, but nothing is based on machine learning. It’s just pure statistical data, and it is being evaluated for standard deviation or simple query. 

So some more insight there would be amazing!

**Audience Member:** Yeah, that too much data problem is very interesting to have, especially as the OpenTelemetry standards evolve and add more fields, so now you have more fields to consider.

**Iris:** Yeah, and they’re good. It’s good that those are coming.

**Audience Member:** Absolutely. One thing that I wanted to ask you is, is your team involved at all in the creation of SLOs?

**Iris:** Yes, but also no. Well, let's say we are the ones that are providing the tool for creating SLOs. 

So basically, the teams can implement the structure or create alerting dashboards based on it, but we are not the ones that are providing the guidelines and the instructions on how to create them. We just provide the tooling.

**Audience Member:** Is it like an open source tool that you guys are using or is it some commercial?

**Iris:** We’re actually using—we have our own alerting solution, so we are doing the same for SLOs as well. We are currently using Thanos Ruler to send the alerts and to evaluate them, Alertmanager to send the alerts, and we also have a custom tool that reads the SLOs that are written in Terraform and makes them compatible with a SLO’s Prometheus rule.

**Audience Member:** And do you—so the reason why I’m asking specifically about SLOs is because one of the—I’d say one of the practices a team should aim for is like using observability data to help generate their SLOs or not generate, to create their SLOs based on observability data. So I’m just wondering if that’s something that your team is planning on providing guidance on? 

**Iris:** Basically, we are providing the data and the tooling to do it. Most of the guidance on how the SLO should be built, most of them are our business, and we’re not really involved in that part. So we cannot give guidelines. 

So that’s completely different, but yeah, for example, for infrastructure we provide some basic guidelines of SLOs, but I would say that yeah, we just want to provide the data that they feed off of our metrics and the tooling that they can do it and how they can actually build them, but no, not really how they can go and, or like how to calculate the SLOs or what is best and what is worse. We haven’t gotten there, and I don’t think we have plans currently to move into that as we have other teams that give advice or recommendations on that matter. 

There is so much to do in observability in our company. I think we’ve done a pretty good job so far, but there is so much more that we can do.

**Audience Member:** Is there something that like your team is working on implementing that you’re excited about in the near future?

**Iris:** Well, we decided that because of the huge amount of data and we do not have capability to process it as well as we would like, we are currently trying to see if we can—well, let me put some precedent: we are using different tools for different things. We’re using Prometheus for metrics; we’re using OpenTelemetry for traces and OpenTelemetry for metrics to a certain degree as well, Grafana for dashboards, and we’re using a different tool for logging. 

Currently, it’s all a mess, so what we’re trying to do right now is we want to centralize them in one platform. What we don’t know yet is if it’s going to be an outside platform that we are feeding the data to or like a vendor, or if we’re going to provide it ourselves. So that’s something that we’re working on right now—centralizing everything in one place and sending all telemetry signals through one transporter, that is OpenTelemetry, so we could have better correlation, which we are also lacking currently.

**Audience Member:** Awesome, so exciting times!

**Iris:** It is amazing! I love the work that you’re doing right now, and it’s going to be the next two years at least. I know after that it’s going to be very exciting for us.

**Audience Member:** Well, it’s very, very awesome. We’re coming up on time, but I did want to give folks an opportunity to ask any other final burning questions.

**Audience Member:** Nope, everyone’s questioned out.

**Iris:** Thank you everyone so much for joining, and if anyone who’s in the audience is interested in giving a presentation for OpenTelemetry or would like to participate in one of our OpenTelemetry Q&As, please reach out on Slack. We are more than happy to hear your stories, share your stories so that we can continue to build this amazing OpenTelemetry community. Thank you so much for having me today!

## Raw YouTube Transcript

foreign thank you for joining us again um we had her for the Q a last uh last month a couple weeks ago I guess and she did a bang-up job so we we asked her to come back and uh I'll let you take it away well hello everyone thank you so much for joining today my name is Iris I'm a platform engineer with a focus on observability working at firefits currently uh and I'm a super passionate person when it comes to observability so uh yeah I I like to talk about it as much as I can and I like to share the observability culture all over not just in in my company so today I wanted to have a talk and I would like to have it as an open discussion I would love to hear about the observability culture in your companies as well um how I believe that observability is a team sport and it is not something that is being done just by one Central team in a company no matter how big or small it is I think everyone should put their hands and be involved in the observability but of course it's let's say observability has been there forever but now it's becoming this huge thing and now we have this uh amazing culture that is spreading but we're still not there so I feel like this conversation and these discussions are so important to put out there for us to discuss and then to take them to our companies and to learn obviously from each other so yeah this was just a a small intro uh please interrupt me because I do not have a good visibility so if you have a question please don't don't hesitate to unmute yourself and just uh just speak or if you have any any opinions about something that I'm saying I just want to have a spoiler alert these are mostly things that I've learned from my job and things that I believe in so of course if you have a completely different opinion please let me know let's discuss it it's it would be amazing so to have a conversation about this here okay so uh the first question that I ask all Engineers that are inter being interviewed for a com for a job in my team is what whose responsibility is observability because let's be honest observability is be it is being implemented different in in many different companies I've seen so many ways everywhere that have interviewed Forum that I work it's different so of course observability is everyone's responsibility it's simple and if your engineer says that answer to me and then I'm like okay okay we're on board we're getting somewhere uh and why do I believe this uh first of all in order to get deeper into why I believe that everyone uh is responsible for the durability in their company I want to give an overview of what I think of perfect observability system is and that's perfect let's take it to the grain of salt of course it's perfect for me my company it works in another person's company doesn't work so if you have any more ideas here this was just what came at the top of my mind so for me a perfect obserability system it has a centralized view of systems observability it's data Engineers can go and they can access their traces metrics logs dashboards alerts um everywhere they don't have to memorize a lot of URLs or have to go in many places to find the information so that's the first thing the second one is optimized data will optimize costs because doing observables doesn't necessarily mean that you will send everything that you can and rack up huge bills uh but it needs to be optimized data and some good old data that is actually going to help other teams troubleshooting and monitoring their systems obviously reliable alerting reliable and custom dashboard reach traces correlation between the different Telemetry signals so this is something that makes a perfect observability system for us and my job as an observability engineer especially in perfect is to provide that for the engineers but I cannot do it alone because no matter how much I try many are let's say most of this uh characteristics are a teamwork and something that other people also have to interview which means the engineers that are part of I would say our customers I don't know if anyone has any other ideas about what a perfect observability system it is for them I would love to hear more about it or if you completely agree with with my goals of having this this system but at least I'm with yourself and with them throughout me any moment ous thank you definitely agree with a lot of the things that you you said like it just yeah spot on okay uh sorry I'm not able to see the comments here but if there is something interesting we do let me know please okay so uh the question is okay you have this vision of a perfect observability system how do we get to this observability system well observability Engineers like myself my team with the knowledge to shape observability systems into the correct path uh I'm not saying that um observability is something that a simple engineer that is working that can't get into they can be uh participants and active participants in it but of course it is important that there are Engineers who know observability better than everyone who know what guidelines to implement uh what tools to use or how to optimize data it's very important that these people exist in the company uh because as I said before applicability is one of those fields that are moving so fast that it is impossible for a person that is working let's say uh with databases or in Java and their full-time job is is called getting back-end whatever else to keep up with this so it is important for people like like us to be in the company and to preach let's say these values and to bring all these good tools and good things that we that are being developed and are being shared in the community of course as I mentioned uh observability Engineers that also bring the data to collect and process Telemetry data um but my fourth favorite so far of course is open Telemetry which I'm a huge that's why we're here after all I'm a huge fan we have fans grafana Prometheus that are some of the tools that are open source that are being used so massively right now by uh technibility engineers and now I'm getting to what the rest of the presentation will be about it's not only observative Engineers that are needed when it seems to actively participate in optimizing the data that they're sending and things actively building their monitoring five people 10 people 20 that depending on the size of the durability team cannot do the work of two thousand three thousand depending on the work of so many Engineers everyone needs to have control of their own code so uh some of the things why observability is so important to be done as a team is because how crucial it is in a company I don't in some let's say that in some companies it's still not understood how important it is to have observability but that is well what I believe is because of the small scale but the more that the companies grow they understand how important it is to have highly available and reliable system it is crucial to have optimized performance because you could have you could uh allocate a lot of resources and you are not saving on cost basically you're spending too much money or you're cutting back on resources and you are basically not giving a good performance or a good experience to your users depending on the type of product it is also it is crucial for incident response this is something that is very important uh in my company incident response is very important considering that we have a lot of customers that rely on our product a very expensive products at time so having all the teams involved in the crucial in the incident response is extremely important we have around 2000 Engineers I actually thought that we were three thousand we were actually two thousand that are working completely different areas some of them I don't even know except for what they are so when I hear about and I'm like wow okay because it's so big and so diverse in in our company and uh if this Engineers were not involved in the part of building their own dashboard building their own alert knowing what data they're sending for their observability uh reasons they will not be able to respond to an incident in time simple as that if I was the one making the alerting for them it would be something extremely generic with some thresholds that I just thought would be right because I don't know their products or their code obviously um the dashboards would be completely generic but if those were done by a person inside the team itself it would be so easy for them because they know it inside out their work uh what are some of the issues that they might have so if an incident happens you reduce the the response from hours that it would be if it was for me dealing with their incidental to seconds or minutes if someone from their engineering team is involved also it is crucial for incident detection as I said before um us as Engineers of the rebuilt Engineers we can provide a set of alerting dashboards that are very generic that each team can have as a baseline to implement their monitoring system but I don't feel comfortable in a way to go and to implement custom alerts custom dashboards for these teams again it goes to this even if I wanted to there are thousands and thousands of Engineers Technologies hundreds of teams I would never be able to do that properly that's why it's so important that each team is involved and does it themselves if they need it they change it they don't need our permission they are owners of that solution it's extremely important and also critical for auto scaling um I say this because in perfect we rely heavily on Telemetry data to Auto scale because you never know the amount of traffic that you're going to have in your application but for example let's say for me or for another person in the observability if we were to set some Auto scaling rules for you to be based on CPU on memory to be something that is completely generic but an engineer that knows their system so well they would go and say let's do a custom auto scaling depending on the number of through output or whatever basically my bottom goal here would be to say that all this require people from the teams that are monitoring the system to take part in if it is done by people that are outside um I am an infrastructure engineer I work with kubernetes I work with VMS with Azure with cloud and I'm not very good at back-end coding let's say so if I was the one doing this I wouldn't do it properly no matter how much I tried and how much effort I put so let's not do observability for the sake of it yeah let's have some generic alerts just for having them no let's do it properly let's have the people that are working day to day with the product Implement them make it their baby as well it's very important to to show it to the engineers and I I am very proud to say that in my company we have a pretty good observability culture that has started a few years back so now it is uh widely accepted but I've seen it happen that teams rely heavily on on other people to set up alerting for them and that is something that scares me if something happens if an incident happens and I wouldn't be comfortable working in a team like that if I wasn't the one setting up my own monitoring for the system also observability can be so messy uh depending on the scale of the company and so many people that come and go everyone wants to add the custom metric the custom log it can get extremely messy it can get too too many metrics too many logs especially for example living debugging logs flowing all day you know because it can yeah you just sense I'm an engineer I want to have full obserability of my application it's not affecting my performance so I'm sending everything debugging info warning metrics I create like a bunch of them on top of each other uh or the same metric with different names and yeah what can me as uh as observability when you do about it alone I I detect it I see it and I say okay this is completely wrong but of course I'm going to need the collaboration of the engineers that are working with our product to actually change it otherwise it would never be optimized and it should never be um let's say good observability um practices or too many traces well I'm a firm believer that there is no such thing as too many traces I'm a big advocate of that but yeah it's it's possible that we are sending 100 of traces uh with a mess and completely no information there and watch what what can I do considering that I'm not the one doing the day-to-day developing I have to go and talk to the things that they need to be active participants on that without it for me it's there is no durability or it is something very generic just for the sake of it that happens so much and to be honest it rates me but there are also too many dashboards too many alerts uh if someone else creates the alerts or they're just created uh but automatically you have them rigging back and forth and nobody actually looks at them because they know that it is the spam so it is very important for people to go and for engineers to go there and say hey okay I don't need this alert I need this I need to create this and the same for the dashboards it can be thousands and thousands of dashboards and when you actually have an incident that's at midnight three a.m in the morning you wake up your your eyes cannot open and you have to scroll through thousands of those or like try to find a person that might know what this is it's very important to be about as simple as that uh yeah also it is it is not easy it makes it easy I think they go and uh Hand by hand it's it's let's say mostly the same thing with too many alerts and dashboards and you just cannot get the hang of it and you just give up so basically you do not rely on this to troubleshoot them on what do you rely maybe the application logs or it takes you hours to solve an incident which is not uh not great uh either too few or too many Telemetry signals I talked about too many but there is also one thing is to to feel because it is possible that you don't know how metrics are how logs are generated how traces are generated in this case that same place is considering right now it's this huge movement to to make them first class citizens many Engineers let's say they do not know how important tracing is and not how the great capabilities that they can achieve to that and my job as an engineer would be to advise them and to help them with it but not to do it for them because I don't know the type of information they might need it's simple um and it is not cheap uh absolutely it is not cheap depending on the scale of the information that we are having it could rack up to Millions I was reading some articles uh recently about some companies that were paying a huge bill in observability and the question on those articles were who wants to blame and the stack of different building is a team sport it is also the the blame is to share because the engineers of observability are clearly doing a great job at educating or maybe they're not even there that could be one of the cases or the teams are not participating basically you have this framework that just sends 100 of logging tracing metrics and that is all sent somewhere so yeah it's observability is something that is a very uh delicate topic when it comes to costs because so many Engineers consider it as a not a very important thing until they actually need for example when something happens and they don't receive an alert oh my God it is actually important or oh my God the information on that on the dashboard now would be so nice but at the same time we have to know what to collect how to and and to also advise our team to do it and to Empower that to do it themselves it's not an easy task especially when it's so comfortable for them to have all that information let's say for example logging I have a vision of talking in general but nothing against it but it's used so much uh they say okay I just will have the debug logs and even if if something happens I will I will have it there um yeah it's our job to to convince them to show them and to actually make a plan and follow it through together and that's I would say our mission as observability Engineers is not just to to teach people and to show them because sometimes that sounds like I don't know if it is the right word but like stack UPS oh yeah you do it it's not it's not that it's just um trying to do it in a in a collaborative matter to consider it as a teamwork that's a really team it's not an external team that just uh works completely away from you but it's let's say part of your team and at the same time it's not it's it serves as an advisory um wait let's see so how do we change to this observability culture it is very very difficult um I am because of my passion for observability I talk a lot about it and it is very very difficult to convince an engineer with a higher grading as you so to follow a certain observability culture if sometimes it's not even the engineers itself it could be senior leadership it could be higher ups I just do not believe it and some companies and this is very prevalent and it's very difficult to crack in there to actually implement it but I think that our job is to make a change we need to be ambassadors in our companies and I think what we're doing today and all these sessions are a great way for us to talk and to share opinions and to know how to approach our teams better how to to be better at what we're doing and to actually show the importance of our work we need to show the importance of the observability of the data in fact so you can just go and say hey observability a rainbow is amazing beautiful we have to show data and that's very important to collect and very easy as well considering the amount of incidents that are happening or you just show how easy it is to to get information about one aspect of the application that they never thought about it's important to provide up-to-date tools our job is amazing because we have we can work with some someone in modern technologies that are moving so fast at the same time it is very difficult because you need to provide tools that aren't mature enough you have to provide tools that the engineers need and are looking for and sometimes this can get overwhelming but it is part of the challenge and actually this is my favorite aspect of the job how fast it moves and how fast you have to adapt it's part of being an observability engineer uh yeah we have to provide guidelines uh not everyone knows I I mentioned this before but I'm a firm believer of not judging it might sound like a childish thing to say or kindergarten issue but I know that tech people judge each other oh you don't know that much about kubernetes parts of durability it's very important for us to offer help and to avoid Judgment of that sort because observability is not difficult but at the same time it's not easy as I said before there's it's very fast it's moving it's modern you need to read you need to adapt very fast and the person that is completely detached from that will not be able to do it so there is no place for judgment here and I think that if there is Judgment that doesn't make you a good observability engineer I think we're like the Saints of the engineers uh and also we need to give space and time to adapt um in some companies where the durability cultures exist it's easy obviously and if you have a majority of Engineers that follow these practices let's say uh it becomes very easy for it to spread but when you're starting from zero it needs time people that have been working in other companies that have completely different working mentality it's going to be difficult for them to just land in the company and be like ah okay now I accept observability as this amazing thing that it is no we have to give them space and time to adapt show them be patient be there to help to implement even on an engineer that is excellent his job but has no no idea how observable it works we have to we have to be there for them and I think the human aspect of this job is more important than the technical one because it is good it's important to be technically and to adapt and to build but also the human aspect and um spreading the culture and talking to people and being a people's person professionally you don't have to be an extrovert and have this uh beautiful conversations with it's a very important aspect of observability um these are all the slides that I had uh prepared for this presentation but I just want to say my my summary or let's say the message that I have through this is that we all need to work as a team to have great observability culture and great observability system and that's going to make our lives much easier so that's pretty much it I don't know if you have any questions please let me know it was a comment from Derek actually um I think when you're talking about what is important um uh an observability and they mention ability to form Rich queries against the data once it's in your analytics back end is when you're just asking what's important in the design of like a perfect system so you can pop all the perfect data that you want into your back end but if you don't have a way to quickly and effectively query that to get the answers you want then you're just going to be frustrated I think that's one of the reasons why we get frustrated with unstructured Vlogs so much is because you end up having to craft these complicated regexes and pattern matching of unstructured screens and it's really frustrating to try and find what you want so if we can do our part to make sure the data going in is nicely structured it makes it a lot easier to get the signals you want out of it absolutely absolutely agree I had a question too but I can go to the back if someone else have them oh no go ahead Derek I have one but I can I can go after so um I guess my question would be what was the observability culture like when you joined your company um and kind of like what was Ground Zero where were you starting from and then kind of how did you how did you make improvements to that or how did you kind of watch that culture improve so actually I'm pretty lucky when it comes to that extent where in this company when I joined I think the the previous team uh and the just a few members that are currently part of my team had done most of the heavy listing they had started so this the spreading of the observability culture and how to do it correctly so when I joined I think everything was was put in place uh but of course we are improving that day-to-day because you can have a culture there but you have to let's say you have to water it every day and you have to keep it and to improve it so what do we do we have presentation sessions uh with with other engineers in the company showing them how to use our tools best of or what our tools can do we we are always open the team for just a slack message to to help them on how to um create metrics dashboards even though the simplest questions were always there for them we also sharing sharing articles sharing of presentations for observability and it's actually interesting because many people read them and we're also making sure that we're also always on top of the new technologies in this case for example it was open Telemetry it was something that it was very uh well accepted in in far-fetch and some people were actually requested so we provided and and just saying like always providing with a with a news Technologies with the newest updates so nothing is missing and if let's I don't know if it's called I don't know bribing but for example we make sure that it's one of our Engineers who reads something on the Internet it's like oh this new cool feature in grafana that I saw can we have it we make sure that we've had it before we we always make sure that we're on top of things uh because that makes people want to use the system more and use it well and Implement all the all the new changes so it's great for us yeah that's really good if I can ask one more quick follow-up question then I can give someone else a turn um so in your slide deck you talk about how developing ownership right by everybody of observability is super important for Success because ultimately like application Engineers they're the ones who have the context for how their applications run and for the things they need to look for so how in your experience how have you gotten application teams to kind of take that ownership without making it feel like it's just one more thing you're trying to Chuck over the wall at them right because again like it's a unique spot that we're at as like infrastructure platform people because we're not like the best at writing code like we're not that's not going to be our strongest um skill so it feels weird to me at least in the past saying hey can can y'all go learn this thing and put it into practice and I'll coach you from the sidelines but I can't do it as well as you right yeah I think what helps as I said again we are lucky because we have a very good observability cultural already engraved so in most of the engineers just do it but yeah I think what what really helped was that we have given full ownership to to the teams on building their own dashboards alerting and sending their own Telemetry signals so basically uh when they come to us like hey can you help us set up an alert we're like well say you are full owners of that part of the observability and that actually helps uh the teams knowing that they're owners of that they they want to to make it grow and improve it rather than just living it there and we'll we've also made it very clear that we will not be going there and implementing um anything uh related monitoring so if you need monitoring you have to do it yourself we are here for you we provide guidance of course we have tons of documentation on how to do everything they're not alone but we made it very clear that we're not going to do uh that for you it's simple and considering that uh all our incident is coming from the alerting that is set up on our platform let's say they're kind of forced to to to implement it but I think just the fact that uh they're owners of that and it's part of their team responsibility coming from the higher structures of the company okay you'll have this amazing product that you are building maintaining but you also have observability that is your your full ownership so if it's not good then it is your responsibility not somebody else's so I think that has motivated the teams to to really work on there sometimes it happens that when a junior enters the team they're the ones that have to take care of the all the monitoring without knowing well the code but I think they soon realize how important it is for everyone to to put their hands into to contribute there otherwise so does not work out that's great thank you I have a question for you it is um so you're saying like you guys hold your ground as far as like not being the ones to instrument code for folks um what kind of reactions do you get from teams when you say that like are they like oh okay I'll do it or or did they uh like defensive pushback like what what's it like how do you deal with it and most of the time they're like oh okay is there a framework can you send me to the framework so that I can find it and we usually also have a team that helps uh build this framework so we send it to them but most of the time yeah we just hold our ground if they say that oh but I really need to help me like okay I'm gonna help it I don't know how to do it for yourself I don't know your code and of course there are cases that people are not happy and they could go to a higher structure in the company and complain but considering that this is not just something that as the durability team have decided today that oh we're not going to build anything for anyone because it's their responsibility it's not something that it's of course supported and signed by by upper management they're like sorry but you have to do it it's documented and you have to do it and it's simple as that it's great having support that that's awesome because I think you you've like you've nailed it like that is the key thing because you're right like if you don't have the management supporting that decision to like you know have a visibility team we're not going to instrument your code like yeah you're gonna have like back channeling and I mean I've I've experienced that it's really annoying um and and ends up kind of like ruining the thing that you're trying to build like as far as that observability culture yeah exactly uh as I said yeah in farfetch we are very very happy in this aspect because uh yeah being forced to instrument somebody's code that would be the the breaking point for me as an engineer in believing that observability is lived preaching with everyone and talking about it and then being forced to to do to go against these values that that we have that would be the breaking point for me I have to stay yeah I don't blame you um I have a question um so far-fetch is an e-commerce site um correct and so I was curious and Derek I guess actually this question is um inspired by your question in the hotel end users Channel this morning how do you um or do you like what do you use for a client-side instrumentation are you um well we are using a vendor currently um I'd rather not say uh but yeah we're using a vendor for that for that part which is very very small the rest the the rest of the platform is all instrumented by our own custom framework okay okay do you have that integrated at all or is it kind of like its own view of the client and then you have trouble correlating signals with your back end uh currently it is a bit segregated but we're working on on changing it because it's such a small part of our platform but yeah it's not fully fully integrated uh we're focusing more on the let's say on our own custom solution that we have locally and that one is a bit segregated from it and it has only specific information which is not great but we that's our goal for 2023 and 2024 to integrate everything together what's yours what's your custom solution involve like what what makes it custom I guess is the question I mean it's it's uh using open Telemetry now uh we're using thousands Prometheus grafana alert manager basically all open source and we built it uh let's say we have around 100 kubernetes clusters we have created our agent based on this open source uh and just adding them on each of the classes collecting information that's centralizing it oh so custom solution like you're talking specific specifically around tooling and not like I thought maybe you meant like um like having some like abstraction layer on top of like hotel which I've seen some organizations do no no no no okay okay cool any other thoughts or questions for for edius yeah I I guess I'm a little curious so you said you know there's no such thing as like too many traces and I know some people you know prefer to do some form of tail sampling so they're just they're still getting like a subset of you know randomized 200s and then like you know whatever chases with errors or latency or whatever attributes that they might be interested in um so I was just curious if you could expound on that a little bit more especially like you know when talking about like um increased costs with excessive amounts of data and yeah so tracing is something that we are working a lot because it's not one of the Telemetry signals that it's being used a lot in in the company so we are trying to Advocate it and push it a lot and for your when I say too few traces I would say uh 0.01 sampling like normal sampling let's say in younger so the teams were considering it extremely low and they weren't even caring about it even though some of them you could find very good information there for example I use it a lot to troubleshoot our Thanos queries it's amazing um so where we are right now we increased because we were able to change our our background back-end solution from Cassandra to grafana Temple which is also open source so of course because before we were spending a crazy amount of money for 0.01 tracing we were spending so much money I don't want to say numbers so there was no place for us to grow more it was completely out of budget so now we implemented Tempo and we are able to send more traces and we increase to five percent and increasing that just a few applications and actually uh implemented this change because of course it's a custom framework and they can they can do it however they want and we already went for 1 000 uh spans per second to 40 000 from just one application and uh this application is not even using most of what they're sending and imagine we have hundreds of applications all of this sending all that is going to be millions of spawns per second it's going to require a lot of computation power and it's going to require a lot of storage it's going to be a mess and uh the cost will be out of this world in in conclusion so I think that we really need to go there like which we're in the process of doing and try to see to implement better uh better tracing because just normal sampling is not working yeah we're looking into a tail base that's why open Telemetry was uh brought up in the first place so the perfect to look at the tail base and yeah we were sending too little or basically there was not enough information now we're sending too much so we are trying to find something that is uh in between until sampling is one of them and also we're also making changes to the framework so not everything goes through some of the information is just not usable and the engineers are not needed yeah it's a it's a work in progress that is taking a lot of our time I I'd rather spend more money and send more information obviously but at the same time we could not spend millions of dollars a year for it for information it's not going to be used I see a raised hand I did um I wanted to ask if you're doing anything around anomaly detection especially as you increase the volume of information coming in to avoid having to look at everything to look at those anomalies that are out of the time frame uh you mean anomaly detection in the the amount yeah well so the scenario of you know here's a here's a high volume event but it's normal and here's a high volume at what's normally a low time of day right just to look for anomalies and everything that you're receiving without having to have a human look at it Yeah we actually have some alerting implemented there based on standard deviation and uh it's like a simple prompt ql based alerting for that and there are accurate but so we haven't gone about that because it's a custom solution and we actually need to apply some machine learning there which it's uh we haven't really been able to get to it so it's do some alerts with the standard deviation that are okay uh they they can detect some anomalies and some very big changes but also are highly inaccurate so it's not my favorite but they are there we need to to work better on it but yeah for example if we have a huge anomaly or a huge flow of sudden that is not uh predicted it is okay let's say it's not perfect I know it's you can get into some really um interesting your Solutions with that that can't come with a high cost but there's some really interesting things you can do with that too so I would love to know more if you have any links any suggestions for me please let me know because the more solutions we can implement hahaha like you mentioned cost cost is always a worry obviously but if it's actually a quality solution that we are introducing uh of course it is uh a cost that is needed and that's it you know it's it's really enjoyable when you start to use tools like that to um solve the problems because then you see things you didn't expect to see um we were experimenting with something a few years ago and we intended to point at the top 10 um issuers and you know the engineer mistakenly didn't put the top 10 in so he was actually looking for anomalies across the entire user set so you know thousands of um thousands of customers and but they were in entities so you know you can start to see oh look they're not sending at this time of day when they usually are that's a low volume low Center but they're sending nothing and they usually since 70 per hour um and we could see that before it finally blew up from running out of memory but uh you know you really get to say now what can I do with that for my business so that was interesting we presented to them at the the team who wanted to own that didn't want to develop the expertise and that's tooling even though we could show them what it could do so a 500 gigabyte an ec2 instance was running that before it blew up yeah a challenge that we have currently is with our Matrix as well and uh detecting which applications are sending higher technology more number of times series and we have dashboards and alerts for that but nothing is based on machine learning it's just pure statistical data uh and uh it is being evaluated for standard deviation or simple query so some some more inside there would be amazing yeah that's the part that we always worry about too is you know we've got so much data to look at which one's important so you don't look at any other you put your tools against it um yeah that too much data problem is is very interesting to have especially as um the open celebrity standards evolve and add more Fields so now you have more fields to consider yeah and they're good it's good that those are coming absolutely yeah one thing that I wanted to ask you is um is your team involved at all in the creation of slos foreign yes but also no well let's say we are the ones that are providing the tool for creating slos okay uh so basically the teams can implement the structure or basically create alerting dashboards based on it but we are not the ones that are providing the guidelines and the instructions on how to create them we just provide the tooling is it um is it like an open source tool that you guys are using or is it some like commercials we're actually using uh we have our own alerting solution so we are doing the same uh for slos as well and we are currently using tunnels ruler to send the alerts and to evaluate them uh alert manager to send the alerts and we also have a custom tool that reads the slos that are written in terraform and make them compatible with a s permitus rule satanos roller can read them okay and do you um so the reason why I'm asking specifically about slos is because um one of the I'd say one of the practices a team should aim for is like using observability data um to help generate their slos or not generate to you know create create their their slos based on on observability data so I'm just wondering if that's something that uh your team is planning on providing guidance on um like what's are there any plans around that so basically we are providing the data and the tooling to do it uh most of the guidance are like how the SLO should be built most of them are our business and we're not really involved in that part so we cannot give guidelines so that's completely different but yeah for example for infrastructure we provide some basic guidelines of slos but I would say that yeah we just want to provide the the data they feed off of our metrics and the tooling that they can do it and how they can actually build them but know not really how we can go how they can go and uh or like how to calculate the slos or what is best and what is worse and we haven't gotten there I don't think we have the plans currently to to move into that as we have other teams that give advice or recommendations on that matter there is so much uh so much to do in in observability in in our company I think we've done a pretty good job so far but there is so much more that we can do is there something that like your team is is working on implementing um like that you're excited about in the in the near future well uh we decided that because of the huge amount of data and uh we do not have capability to to process it as as good as we would like we are currently trying to see if we can well let's let me put some precedent we are using different tools for different things we're using permitting styles for Matrix uh we're using open Telemetry for traces and open Telemetry for metrics to a certain degree as well uh grafana for dashboards and we're using a different tool for for logging and currently it's all a mess so what we're trying to do right now is we want to centralize them in one platform what we don't know yet if it's going to be an outside platform that we are feeding the data to our like a vendor or if we're going to to provide it ourselves so that's something that we're working on right now centralizing everything in one place and sending all um Telemetry signals through one transporter that they open Telemetry so we could have better correlation which we are also lacking currently awesome so exciting times it is amazing I love I love the work that you're doing right now and it's going to be the next two years at least I know after that it's going to be a very exciting for us well it's very very awesome um we're coming up on time um but I did want to give folks an opportunity to ask any other final burning questions nope everyone's questioned out thank you everyone so much for for joining and if uh anyone who's uh in in the audience is interested in um in in giving a presentation for hotel and practice or would like to participate in one of our hotel q and A's please reach out on slack we are more than happy to hear your stories share your stories so that we can continue to build this amazing open Telemetry Community thank you so much for having me today

