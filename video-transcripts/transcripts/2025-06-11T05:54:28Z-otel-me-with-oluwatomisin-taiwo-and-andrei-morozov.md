# OTel Me...with Oluwatomisin Taiwo and Andrei Morozov

Published on 2025-06-11T05:54:28Z

## Description

oin us for the next edition of OTel Me. This time, we'll hear from Oluwatomisin Taiwo and Andrei Morozov of Compass Digital as ...

URL: https://www.youtube.com/watch?v=tTCuTAPE5aQ

## Summary

In this episode of "Hotel Me," hosts Adriana Vila and Andre Kapolski engage with guests Tommy and Andre M. from Compass Digital, discussing their roles in ensuring system reliability and observability. They delve into the architecture of their systems, which primarily leverage AWS services like Lambda and ECS with a focus on using OpenTelemetry for observability. Key challenges addressed include the need for distributed tracing in microservices, performance monitoring, and effective error tracking. The conversation highlights the importance of context propagation and manual instrumentation for critical business logic, as well as the evolving landscape of observability practices. The session concludes with insights on community engagement and future directions for OpenTelemetry, emphasizing the need for practical examples and improved tooling.

## Chapters

00:00:00 Introductions
00:01:36 Guest introduction: Tommy
00:03:08 Guest introduction: Andre M
00:05:20 Discussion about system architecture
00:10:24 Need for observability in microservices
00:12:48 Introduction of OpenTelemetry
00:19:50 Discussion about instrumentation challenges
00:25:36 Audience Q&A session
00:36:48 Discussion on data management and optimization
00:43:12 Future directions for OpenTelemetry

## Transcript

### [00:00:00] Introductions

**Adriana:** Heat. Heat. Hello everyone and welcome to our latest hotel me. I'm super excited for those who are able to join and for those who are not, we do have this recording available after the fact both on LinkedIn and YouTube. My name is Adriana Vila and I am one of the maintainers of the hotel end-user SIG, and I am happy to introduce my co-presenter today, Andre.

**Andre:** Hi everyone, and yeah, awesome to be here. My name is Andre Kapolski and I'm a fairly recent contributor to the end-user SIG. So yeah, I'm super excited to hear more from our guests today.

### [00:01:36] Guest introduction: Tommy

**Adriana:** Yeah, and I think this is a perfect segue as we bring on our guests. And as we do, folks on the chat, please feel free to say where you're watching from.

**Andre:** Yeah, I can perhaps start. I'm watching from or I'm joining from the Czech Republic and in the EU, specifically in Brno. Adriana, what about you? Where I don't think you mentioned it, have you?

**Adriana:** Oh, yeah. That's right. I'm in Toronto, Canada, so it's I guess 1:00 for me and I guess it's evening for you, right? You're probably 7 o'clock in the...

**Andre:** That's correct. That's correct.

**Adriana:** Yeah. Awesome. Awesome. All right. I guess let's bring our guests on. Hello everyone.

**Tommy:** Hello.

**Andre:** Hey. So, why don't you both introduce yourselves? Let's start with Tommy.

**Tommy:** My name is Tommy. I'm an SRE here at Compass Digital and my primary responsibility essentially is ensuring the reliability, scalability, and observability of our web platform. I work closely with the engineering team, product teams, and test engineers to ensure that our systems are robust and we have actionable insights into what's going on within our services. So yeah, that's me.

**Adriana:** And where are you calling from?

**Tommy:** Oh yeah, I'm calling from Cambridge, Ontario in Canada.

**Adriana:** Awesome. We're practically neighbors, a few hours away from each other.

**Tommy:** Oh yeah. Awesome.

**Andre:** Okay. And then we have another Andre today.

### [00:03:08] Guest introduction: Andre M

**Andre M:** Hi. I'm out from the Bonneville Durham region. So, same type of area. And for me, I've been in this industry for 11 years, but always been since I was a kid into the tech stuff. So I work with Tommy and the same team SRE at Compass Digital here and very similar responsibilities across observability to on-call PI, all the type of stuff for our systems.

**Adriana:** Awesome. Well, we're super excited to have you both on and just a reminder for folks watching, if you do have questions along the way, please feel free to post them in the chat. And also we will be taking questions at the end if anyone has questions. So I guess let us get started. So I know you guys both talked a little bit about your intros. Maybe you could talk a little bit about your respective roles at Compass Digital in a little bit more detail.

**Tommy:** Sounds good. I'll start.

### [00:05:20] Discussion about system architecture

**Andre M:** Okay. Yeah, sorry. I'll go first then. As I said, most of my role is primarily focused on ensuring the reliability and scalability of our systems. Very recently or for the most part that I've been here, I've been working on one of our products. It's a company I actually bought over is e-club to help get observability into what they're doing there. And largely, I work with things like AWS SDK, the CDK, TF rather, and PA duty. We have observability back-end data trace where we send all our open telemetry stuff to. So pretty much just SRE. There's nothing too crazy happening on there, but I believe as we go along on this podcast, I'll get more nuanced into what I do and how your architecture is structured and the stack there.

**Adriana:** Sounds good. And Andre M, if you could elaborate a little bit on your role?

**Andre M:** Yeah, certainly. So when I started with Compass about 5 years now, I started off as a senior software engineer. So that's my background in software engineering, less so on the ops and infra side, but that's kind of been a new love with Terraform and services. But primarily about the developer experience, the platform engineering aspect of it. And more recently, it's more about understanding the system that you can worry about and they'll tell us that linkability how potentially across that and give us the insights with our platform and a vendor that we utilize that with. So just to give you some pretty cool things is like identifying any services that are looping into each other, for instance, via API rather than just like calling it internally with a function. So it's more about just setting up these guard rails and looking into the optimization of our system. And the other aspect is because we are a microservices shop, it's been a challenge for us to get that context propagation and seeing all the services all together. And now we're able to kind of have that full aspect all together and actually run real, I guess, business impacts and analysis onto that. Yeah, that's primarily an area I've been looking at more or less.

**Tommy:** Cool. Folks, can you start with can you tell us a bit more about the system that you are operating? What is it? What is its architecture? What programming languages are used and how it is deployed and so on?

**Tommy:** Okay. So we have two products. So I'll talk about the one that I primarily focus on. For us, our main infrastructure and architecture all runs, 90 to 95% is all lambdas. We have about 300 plus lambdas in our ecosystem. We run a hybrid of DynamoDB with Aurora RDS and that's the exact thing progress and we're slowly getting into Fargate services. So that's a little bit different model but that's the primary architecture of our ecosystem. You know, the standard services like S3, HTTP gateway, just standard service stuff that the ecosystem provides.

**Andre M:** Oh yeah. On the E-Club side, what we have is a blend of Python and Django on the back end and then JavaScript, TypeScript on the front end. And we run everything on that side on AWS ECS, which gives us that flexibility and consistency across all our environments. 

**Tommy:** Okay, thank you. Our CI/CD pipelines automate the process of building those Docker images and then deploying them onto ECS so we can roll out those changes quickly and safely. But to do a quick walkthrough across this little diagram that I have here, imagine a user interacting with the application. What they hit first is the load balancers that we have here and then it distributes traffic to the web service that we have on the back end which runs ECS and is instrumented with the open telemetry SDK. Now as the web service or web services begin to process this request, what it does is offload some heavy tasks or async tasks to this Celery worker down here which also runs on ECS and is also instrumented with the open telemetry SDK. Thanks to the context, oops typo, context propagation here, the trace context can flow seamlessly between the web service and the worker. With the introduction of the open telemetry SDK, we can follow a request end to end from inception to the end, right? Both these services, that's the web service and Celery worker emit traces, logs, and metrics. We sample them, make sure they have high cardinality and they are batched also for efficiency. All this data that I'm talking about is sent to open telemetry collectors that run as a sidecar on ECS to send this data to our observability back end. So this is pretty much in a nutshell what the architecture on the E-Club side of what we are doing looks like.

### [00:10:24] Need for observability in microservices

**Adriana:** Awesome. Thanks for that. As a follow-up question, given your system architecture, what were the top three problems that you were facing that compelled you to say, "Hey, I need observability"?

### [00:12:48] Introduction of OpenTelemetry

**Tommy:** Oh yeah. Sure. I'm sure you know and probably everyone listening knows that observability in a distributed system is never a solved problem. Some of the nuanced challenges that we faced was again, as I said, distributed tracing. With microservices, a single user request can touch so many services and background jobs and without distributed tracing, it's almost impossible to reconstruct that full journey of a request from the user's end to the response back to the user, especially when something goes wrong. For instance, if a user says that they had a slow order placement, we need to trace our request from the web front end, the back end, the Celery workers, and even to third-party APIs. The introduction of open telemetry helps us stitch all these things together and also have consistent context propagation across all these boundaries. One of the things that we were also looking at was performance monitoring because we rely on Celery heavily on E-Club for background processing. However, these things can be black boxes if you don't instrument them properly. So we need to know not just if a task has succeeded, but how long it took, what resources were consumed, and whether it's causing bottlenecks along the way. Introducing Celery with open telemetry required us to go with out-of-the-box solutions. Again, thanks for that. To be able to add custom spans and metrics to these things and error tracking. You know, errors can manifest in logs, traces, or metrics, wherever. The most important thing for us at the time was to be able to correlate these things together, so that's having your trace ID and span ID in logs and being able to look at that trace ID in the log and correlate it with an actual trace. A spike in error logs sometimes might correspond rather with a specific trace or a drop in a metric that we're tracking. What we've done is we've worked to ensure that our telemetry includes enough context, like as I said, the trace ID, request ID so we can pivot between the logs, traces, and metrics seamlessly. So yeah, that's what got us to where we are now.

**Andre M:** Awesome. Andre, do you have anything else to add?

**Andre M:** So for our end, I think the primary problem again really was that context propagation because again the microservice and distributed nature of that. I think the other part that we weren't even really aware about is just how the system really behaves. So a big part of it is tying back into alerting. So depending on what vendor you use, it might have automatic alerting, but being able to provide that to our developers in our case via Terraform. So developers can open up PR, create their own alerts, and now we have that across our ecosystems has really changed a lot of how we're able to kind of empower the different developer teams to just take control of their own alerting and systems rather than just one team owning that.

**Adriana:** Can you tell us a bit more about how your systems are instrumented? What types of instrumentation are you using?

**Andre M:** Certainly. On our platform, currently for us, we do use our vendor agent. So in our end, the agent's really nice for the most part. It works seamlessly with both of our infrastructure pieces for Lambda. Lambda has the concept of a Lambda layer where you can basically put on something. So their agent just goes onto that, kind of bolts right on and works essentially right out of the box. The second part to that that we had to play around with a little bit was our ECS Fargate. There is a solution from AWS called AWS FireLens which allows you to kind of have your task logs, standard error, and send them out to basically be sent to a specific sidecar where then that sidecar will run something like Fluent Bit or the Kube and essentially take your logs and ship them to where you need them.

**Adriana:** Yeah, that makes sense. I would ask a follow-up right away. So, are you using auto-instrumentation or are you manually instrumenting your services?

**Andre M:** So for us, because we do have the 300 lambdas, each one needs, we basically have like a core library that all these lambdas will all utilize, very thin layer. But in there we do have hotel instrumentation for the Node.js. I believe it's just the auto-instrument or I think we might have changed that for the finetuner because it needs to be small because lambdas, you want to keep them really tiny. So I think we do have the custom one where it's not the auto-instrument.

**Tommy:** And yeah, how was your experience with doing that so far?

**Andre M:** I haven't heard any issues. I didn't do that one. It was actually our coworker Matt who did that one, but I didn't see any too many issues with that. The PR went through pretty smoothly and the primary purpose was we use our own custom logger, one that isn't supported by the hotel or vendor. So we had to use that to kind of pull in the trace ID from the headers and everything else and be able to actually populate the spans that we require.

**Adriana:** Cool. So, to clarify then in a nutshell, you basically have like a wrapper around hotel from what it sounds like.

**Andre M:** Kind of, it's, I guess so.

**Adriana:** Yeah. And question for you around just to keep on the instrumentation thread, who is responsible for instrumenting?

**Andre M:** Well, that's an interesting journey. So we actually do have a new project upcoming. So the way our system has kind of evolved with Terraform in particular is, you know, we're IA first for no matter what we do, infrastructure as code. A big part of that is we use the cloud development kit extension on Terraform. So it gives us the ability to kind of have this inheritance model and allows us to build these L1, L2 constructs. If anyone's familiar with that, it's a very useful way of building abstractions for your infrastructure and enabling teams to handle that as well. In our case, what we're trying to build right now is kind of an L3 construct which is like an application service level where we can give it to developers and they can essentially have all the nuts and bolts already instrumented out of the box with the right permissions and the right access and the right agents and everything else. We kind of support this golden path for them.

**Tommy:** Awesome. Do you have anything else that you want to add?

**Tommy:** No, not really. As I said, what I think we're trying to get to a point where all of these things are, would I say, prepackaged for the devs to be able to use them, to leverage them, to do their own instrumentation on their own. I would say for the most part we are automating many of the collector setup or the open telemetry setup. But I think at the beginning what we did, especially on the E-Club side, was to auto-instrument so we can have quick wins and move quickly. But when we're getting down to things like business logic, salary tasks, and things that were unique to the E-Club side, we manually created spans along some critical code paths and added some custom attributes here and there so we can get more than just what open telemetry offers out of the box. For E-Club, it's just a little slightly different from what we have on the concentric side.

**Adriana:** Nice. That's really cool because I think that's a path that a lot of organizations start with anyway. Like the auto-instrumentation is that quick win and then it's like, "Oh yeah, we're lacking a little bit more. Let's go with the manual instrumentation." That's awesome.

**Adriana:** We do have a question from someone in the audience from Buddha. Hi Buddha, nice to have you join us. Buddha asks, "Are you currently using all hotel signals, logs, metrics, traces, profiles or a combination of them?"

### [00:19:50] Discussion about instrumentation challenges

**Tommy:** So we are using logs, metrics, and traces. Logs are coming through depending on which infrastructure piece. So if it's Fargate, it'll come through the AWS FireLens and Fluent Bit solution and then be sent to our agent, sorry, to our vendor. Otherwise, if it's Lambda, it comes straight from the Lambda layer agent. Then for metrics, that comes in through a different connector with our vendor. They have a solution we install in our cloud environment and then it kind of pulls all the metrics over to that. Now profiles is interesting. We don't use profiles, but the use case I think for profiles is enabling maybe common attributes or standardization, and we handle that through again our vendor has a solution out of the box that allows us to kind of set the schema and richer data as it gets ingested.

**Adriana:** Great. And then another follow-up question from Buddha. And by the way, actually before I ask that, there was a question from Manish, who's asking if we are recording the session and yes, we are recording the session. It should be available on LinkedIn and YouTube on demand after the fact. So, great question. The follow-up question from Buddha was, "Was this how you started, or slowly built up to it?" Who would like to take that one on?

**Tommy:** Was that one for like the Terraform aspect or just in general?

**Adriana:** I'm assuming it was for the instrumentation in general, I would say.

**Tommy:** Yeah, at least one for Tommy. Tommy had a fun time doing that.

**Tommy:** Okay. Well, this was not how we started. And for context, I joined about 10 months ago. As I joined, we did not have open telemetry setup anywhere, at least best of my knowledge. I know that some of the motivations for adopting open telemetry on our side was standardization because we had like a patchwork of monitoring tools, like logs, metrics, everything scattered all over the place. But open telemetry gave us a way to now unify all these and reduce cognitive load and the interaction overhead that comes with that. Additionally, we wanted to be vendor neutral. You know, using open telemetry allows us not to be locked to a single vendor. I know I'm getting into the motivations for why we adopted it, but it also cuts back into the fact that we did not start out that way slowly built into it and these were some of the decisions that drove us to using open telemetry. 

**Adriana:** Yeah, I hope that answers the question.

**Tommy:** I hope so as well. How long did it take you to get into the point where you are at currently?

**Tommy:** I'd say we're still getting there, right? If I remember correctly, we started somewhere around September. Andre, keep me honest here. I think by March, we closed the chapter on that one. From, I would say from conceptualization but I say from implementation to when we thought that we were in a good place, so roughly seven months, right? Trying out stuff and yeah, I think we're in a good place now.

**Adriana:** All righty. We have a question in chat from Manish. When we talk of open telemetry data, there is always a huge amount of data. How are you optimizing your storage and managing doing it?

**Tommy:** Okay. I'll take this one a bit and then I'll let Andre finish off because he's the geek with that. One of the things we do is sampling, of course. I know when we query for stuff, we do a sampling ratio of I think 1 to 1,000 for traces at least on the E- Club side. This is tuned in this way to balance cost, performance, and visibility. Of course, we monitor the effectiveness of this sampling setting and adjust it dynamically during incidents or specific services that require deeper analysis. And for logs, for querying and most of the other things we do with logs, we enforce a scan limit of 500 GB per query to keep the back end performant and cost-effective. But I know again, Andre geeks out on things like this. I think you'll be able to give a better response.

**Andre M:** Technically, we got a really big budget for our vendors. So we actually have zero sampling at the moment. So we just grab it all. We're just trying to understand from that point and slim it down. The first exercise that we kind of ran through was grab all the logs, kind of run like a pivot on it like the error logs. Do I count on general logs and are there any useless logs that are being constantly created by the system or just noise, and can we remove that? The answer was yes. I think we had something like two billion, three billion logs coming in that are just like success that we just didn't need to have in place for it. So yeah, we could clean up the system a lot. We go through all our pods and kind of hand out tickets to them as we find through the investigation and be like, "Yeah, let's get rid of this," or "You make it a debug log potentially if you guys actually need it, enable it during your debug sessions." Otherwise, let's not try to have too much noise in our ecosystem. I will say though that one caveat is because we are running Lambda, that one agent and CloudWatch requires certain logs to come out. So we can't get away from certain noise. Right now I think we're ingesting 54 million logs a day that we just can't get away from and they're just noise at the moment. So another motivation to move to Fargate where we have a bit more control over the system as we adapt a bit more of the observability ecosystem.

### [00:25:36] Audience Q&A session

**Adriana:** Cool. Cool. Yeah, Manish, if you have any follow-ups, feel free to post them in chat. I would continue with the next question about collectors. Can you folks tell us a bit about the way how you set up your collector or collectors? What is the distribution and what is the architecture overall? Super curious about that.

**Tommy:** Okay, well I'll begin again with the E-Club side. For context, as I said, I handle most of the E-Club side, the concentric. For E-Club, what we do is try to make our collector environment aware. So in Sandbox, because what we have is Sandbox, staging, and production on E-Club. In Sandbox, we sample a little more aggressively to capture as much data as possible for debugging, especially for me, right? To really know what's going on there and also help the devs really debug stuff that they have going on there. But in production, what we do is a little more probabilistic, I hope I got that right, to balance visibility with cost. What we do is we dynamically also adjust that sampling ratio if there's an incident and then we temporarily increase that to capture more data. The collector definitely does all the batching, filtering, and exporting telemetry to the back end, our vendor. Of course, this is done in a YAML config to define our pipeline for traces, metrics, and logs and to also set the environment-specific parameters like service name, the host type, and then all the open telemetry options we want to set on that. So that's what we have for our collector on E-Club. As I said at the beginning, it runs as a sidecar through the services that Celery, the Django application also.

**Andre M:** So as a follow-up on your collector setup, do you have a way to manage a fleet of your collectors? It sounds like you have a number of collectors. Do you have a way to manage them effectively? Like do you use OpAmp for that or do you use like kind of a homegrown thing? And also, do you use like a collector gateway to funnel all of your collectors into like a central gateway before pushing that off to your observability?

**Tommy:** To be honest, not really. We just manage the fleet using ECS as I said and then we run them outside car to our standalone services and the environment variables that I said help us customize those things I said earlier which are the sampling rates, the exporter, and the service names. So we don't really manage the fleets independently or independent of our services. We just run them as a sidecar and let us do the rest for us. We also monitor the performance, right? Definitely. Things like the Q length and then we have some drop spans along the way. So we can scale them horizontally if we do need to. For high throughput services, like I can't remember the name, one of our Celery, I think it's Celery Flour, right? We have of course a higher instance for that to manage the size and scale of how things can be and to avoid bottlenecks.

**Adriana:** So just to clarify, do all your sidecar collectors then, do each of them go directly to your back end?

**Tommy:** Oh, okay, got it.

**Adriana:** And it sounds like Andre M had a screen share for us.

**Andre M:** Yeah. And just it's a visual, essentially the same idea that Tommy was walking through there. Before we get started though, shout out to SkyDraw for anyone using that. The best platform for brainstorming ideas and a little whiteboard place. Essentially the ECS Fargate, again I was mentioning the AWS FireLens configuration for us. The management, all of it comes back to Terraform. That allows us to easily manage our collectors because even the configurations, everything in code. So it makes that part a little bit easier. If that's what you're trying to guess, prelude to our vendor does have that instance within our account, but we don't utilize that as we found that sometimes the logs are being dropped. Either we under-provisioned it and we found that if we just directly send them straight to the vendor, there weren't any issues that we've noticed aside from losing some of the benefits of that additional collecting if we have to do some data masking or dropping those logs prior to being sent and ingested by the vendor there. So definitely an area we're trying to balance. The other part is obviously the Lambda, which again is super straightforward. It's just straight from the Lambda and straight to the vendor again without any additional pieces there.

**Adriana:** So, it may be an area now that we're talking here, an area for improvement because if we have those 54 million logs that are just noise, we might benefit by going to that filtering agent first. So very useful.

**Tommy:** Cool. Thanks for sharing. You already mentioned areas for improvement and I'm wondering, do you have any challenges with open telemetry currently and anything that perhaps the project could do better to serve you as its user? 

**Tommy:** Well, I'll say for me, one of the biggest challenges was just initial setup, right? Configuring the connector and ensuring context propagation across all the services which required me to do manual instrumentation and of course, there's a learning curve. Telemetry is powerful, but again, the API offers is just so large. Of course, with in terms of best practices, we are still evolving. One of the things that I'll probably like to see, and I think I'm speaking to the community when I say this, is maybe more complex example projects, speaking to myself too, of course, putting it out there. For me, it's just more real-world examples, more complex setups like probably what we have. Maybe, you know, somebody just doing something that big and putting it out there for anybody to be able to use, but I say for me that's just been the challenge. Nothing too crazy.

**Andre M:** What about you? Do you have anything in mind?

**Andre M:** Not so much I can give feedback to really improve. I mean, the whole time has just been learning, learning, learning, right? There's just a lot. I mean, it's first exposure to it really for me. It's not even that I had experience with other observability tooling. It's just like straight to our vendor and then also hotel, so just learning the best of the best right off the bat basically and then learn the history. For me, it's just been a constant uphill battle to learn all the different intricacies. Probably just want to echo Tommy's message, just more examples, right? Being able to pull things down, play with it. Getting into your hands really quickly. That's usually the best place to learn.

**Adriana:** I totally agree. I think we're at OpenTelemetry has been around long enough now that I think many organizations are kind of past that 101 phase and are eager to get into like the more gnarly use cases. That's why, you know, we really appreciate folks like you jumping onto these Hotel Me sessions talking about your mature hotel setups because it really helps others in the community. We definitely appreciate that.

**Adriana:** Switching gears a little bit, we were wondering if you could, if you have had any interactions with the hotel community, things around, you know, if you got stuck on a particular issue, how did you go about it? Did you go on Slack or did you like Google stuff? How did that work for you?

**Andre M:** I remember the first thing it was for E-Club. When we were trying to instrument it originally, there was an extension we use for, is it parallel processing Tommy? Do you remember that one? It's not AWS Wake, there's another one.

**Tommy:** Well, it's a Django app that we run. The problem with that is that there's an extension we use. I think it optimizes performance and the problem with that is that it somehow was fighting hotel.

**Andre M:** Yeah, I really can't remember the name but then I know they were fighting for the threading running in the background. I think that was one of the challenges that we had in the beginning. I really can't remember what specifically it was, but I know that was a big uphill battle back then.

**Tommy:** Yeah. So we had an open issue on GitHub with that. Luckily, our vendor was able to get their agent working, so it worked for that environment but I think the issue is still open but wasn't sure if it was like upstream with Django or was it so much with hotel itself.

**Adriana:** Cool. Cool. As a follow-up, have you gotten to the point where you have made any contributions to hotel or planning to make any contributions?

**Tommy:** 100%. Nothing yet in terms of contribution, commit, but 100% we'll probably be looking at something within the ecosystem of probably node or go, one of those two.

**Adriana:** Awesome. Final thing. I think we've covered most of the base questions. We do have a cool audience question that came in from Esther. It says, "How do you see open telemetry evolving in your workflow over the next year?" Really great question.

### [00:36:48] Discussion on data management and optimization

**Andre M:** That is a tough question. Well because I mean open telemetry, I mean, it's more about is it so much like the framework itself or is it more about the data we're getting out of it? Because for me, I think the value, I mean both are great, right? Because one gives us a standardized tooling and framework to be able to switch vendors or anything else we have the capability now, but for me, I think the real value comes into what type of data do we really pull out of that open telemetry and what can we really get from it? I think that's where real value is. So being able to now take that data and apply user journeys and business events, like those things I think are where our organization can really expand or benefit from for the most part.

**Tommy:** Do you have anything to add?

**Tommy:** Not much, but I think just like OpenTelemetry also keeps maturing in our observability journey, especially around logs and for complex setups like Celery or serverless that's with Lambda because I know there were also some battles fought there. I also think we are probably going to have more out-of-the-box integration and tools to simplify the configuration for our collector, right? I mean the ecosystem is growing so fast. I would not be surprised to see us adopting it a little bit more. As Andre said, we are growing every day. We are learning every day. But I think we'll be leaning more towards probably much better or more standard industry practices when it comes to adopting hotel and using it to really observe our system. So, that's what I see happening over the next year, but who knows?

**Adriana:** Sounds good. Sounds good. Yeah, we have more audience questions. Today, we just talked over at the end with Adrian that we have a really, really engaged audience. So, that's amazing. Manish is asking how are you doing the instrumentation of legacy systems? Is anything like that happening? Do you have a use case for that?

**Andre M:** I think we have some older .NET projects, but it seems as if our vendor supports out of the box and anything I think even with that solution, the vendor allows us to still have it in hotel format as it comes through. So, not really a concern for on our side from what I've seen so far.

**Adriana:** All righty. All righty. And one more from Buddha. How do you report on business value metrics for decision-makers? What metrics do you report on? Yeah, I know I think Tommy you mentioned that for that business logic you had to do some manual instrumentation. Can you tell us a bit more about that?

**Tommy:** Yeah. So, as I said earlier, there were certain business logic that we needed to especially on the salary side because that's where we upload stuff to really see how data was flowing through that and then report on, I think specifically it was a reporting service or I think an order service at the time. What we got out of that really was just the rise and fall or the spike of the, would I say, the others process within a certain period or within a certain trace. But you know, reporting back to business people, I think that one comes more out of the box with our vendor and we, as Andre said, build stuff that aggregates the amount of logs that we use that translate into how much we pay for these things. Even though we have a big budget, we still don't want to overshoot that. But I think again Andre would have more context into this because he builds most of the stuff that I report back to business leaders and that's something he's so in love with.

**Andre M:** So I think for me, the biggest one at least it's been months in development is the most important key metric I've come to love recently is users. So what is a single user doing across a system and aggregate that over a certain period of time. That alone allowed us to find a particular leak where we had a single user was a bug in our client and within our service that wasn't like a batch request where this one user within a short period of time was invoking over 200,000 requests into our system. With Lambda pay as you go, it's not a good model there. Without this observability in place, we wouldn't even know that there is, you know, one single user somehow invoking 200,000 requests within like a six-hour period. These tips, it's small little things where now we can focus on a single user or aggregate it across a grouping and be able to find behaviors and it's really good when you throw into like a time series or flame graph and you can see these kind of like fat spots where you can just visually identify something doesn't look good here. Investigate.

### [00:43:12] Future directions for OpenTelemetry

**Adriana:** That's great. Thank you so much, both of you, Andre and Tommy for joining us and thank you to my co-host, the other Andre for joining. And also this is Andre K's first time on a stream. So, big kudos. Awesome job as co-hosts. I want to give a shout out. As always, we appreciate all the stories that we hear from our community members showing how they use open telemetry out in the wild. It really helps us and helps show folks in the community like, you know, open telemetry is here to stay. We've got some gnarly use cases of it being used out in the wild and it's great to hear those stories. Coming up next, stay tuned. We should have another Hotel Me planned, I think, in July. So stay tuned on the hotel socials for that. In the meantime, coming up at the end of this month, we have Hotel Community Day. It's part of Open Observability Con. It's combined with Hotel Community Day. That's happening on the heels of Open Source Summit in Denver. Open Source Summit North America in Denver. If you're around for Open Source Summit, check out Hotel Community Day and Open Observability Con that are happening together. I think we've got a couple of KubeCons also coming up. I want to say this week is KubeCon China. Next week, I'm actually jetting off to Japan at the end of this week for KubeCon Japan. It's the first KubeCon Japan, so very exciting stuff. If anyone's around for KubeCon Japan, come say hello. We'd love to meet you and talk. I think that is a wrap. Again, for those who attended, tell your friends if they missed it. The recording will be available both on LinkedIn and YouTube. Also, check out the hotel end-user SIG. We also have a group on CNCF Slack. We are called end-user. So come share your stories on the hotel end-user SIG chat. Also, if you'd love to join us for one of our meetings, we meet every two weeks. I think our next one is next week. Thank you everyone once again for joining and we'll see you next time.

**Tommy:** Thanks so much for having us.

## Raw YouTube Transcript

Heat. Heat. Hello everyone and welcome to our latest hotel me. super excited for the those who are able to join and for those who are not we do have the uh this recording available after the fact both on LinkedIn and YouTube. My name is Adriana Vila and I am one of the maintainers of the hotel enduser SIG and I am happy to introduce my uh co- uh presenter today Andre. Hi everyone and yeah awesome to be here. My name is Andre Kapolski and I'm a fairly recent uh contributor to and end user sik. So yeah, I'm super excited to hear more from our our guests today. Yeah, and I think uh this is a perfect segue as we bring on our guests. And as we do um folks on the chat, please feel free to uh say where you're uh where you're watching from. Yeah. I can perhaps start. I'm watching from or I'm joining from the Czech Republic uh and in in EU uh and specifically in Berno. Adriana, what about you? Where I I don't think you mentioned it, have you? Oh, yeah. That's right. I'm in Toronto, Canada, so it's I guess 1:00 for me and I guess it's evening for you, right? You're probably 7 o'clock in the That's correct. That's correct. Yeah. Awesome. Awesome. All right. Uh I guess let's bring our guests on. Hello everyone. Hello. Hey. So, uh why don't uh you both introduce yourselves? Let's start with Tommy. My name is Tommy. Uh I'm an S sur here at Compass Digital and uh my primary responsibility essentially is uh ensuring the reliability, scalability and observability of our web platform. uh I work closely with the engineering team, product teams, test engineers to ensure that our systems are robust and we have actionable insights into what's going on within uh our services. So yeah, that's me. And where are you uh where are you calling? Oh yeah, I'm calling from Cambridge, Ontario in Canada. Awesome. We're we're like practically neighbors a few a few hours away from each other. Oh yeah. Awesome. Okay. And and then we have another Andre today. Um, hi. I'm out from Boneville Durm region. So, so same type of area. And uh, for me, I've been in this industry for 11 years, but always been since I was a kid and into the tech stuff. So uh I work with Tommy and the same team S3 at Compass uh digital here and uh very similar responsibilities across observability to um on call PI all the type of stuff for uh for our systems. Awesome. Uh well we're super excited to have you both on and just a reminder for folks watching if you do have questions along the way please feel free to post them on the chat. Um and also we will be taking questions at the end um if uh if anyone has questions. So I guess uh let us get started. So I know you guys both uh talked a little bit about um you know g gave your intros. Maybe you could talk a little bit about uh your respective roles at uh at Compass Digital in a little bit more detail. Sounds good. I'll start. Okay. Yeah, sorry. Uh I I'll go first then. Um as I said, um most of my roles primarily focused um on ensuring the reliability and scalability of our systems. Uh very recently or for the most part that I've been here, I've been working on uh one of our products. It's a company I actually bought over is e-club um to help get you know observability into what they're doing there. Um and largely you know I work with things like uh AWS SDK uh the CDK TF rather and uh PA duty um we have observability back end data trace where we send all our open telemetry stuff to um so uh pretty much just S sur uh there's not nothing too crazy happening on there but uh I believe as we go um along on this podcast I'll get more um nuanced into what I do and how your architecture is structured and the stack there. Sounds good. And uh Andre M uh if you could elaborate a little bit on on your Yeah, certainly. So when I started with Compass about 5 years now, um I started off as a senior software engineer. So that's my background in software engineering less so on the ops and infra side but that's uh kind of been a new love with terraform and um services but um primarily about the developer experience the platform engineering aspect of it and more recently angle it's more about understanding the uh system that you can worry about uh and they'll tell us that linkability how potentially across that and give us the insights with our platform and a vendor that we utilize that with. So just to give you some pretty cool things is like identifying any services that are you know looping into each other for instance uh via API rather than just like calling it internally with a function. So it's more about just like setting up these guard rails um and looking into the optimization of our system. And the other aspect is because we are a microser shop uh it's uh it's been a challenge for us to get that context propagation and seeing all the services all together. Um and now we're able to kind of have that full aspect all together and actually run real um I guess business impacts and analysis onto that. Um yeah that's primarily an area I've been looking at more or less. Cool. Uh folks uh can you to start with can you tell us a bit more about about the system that you are operating? What is it what is its architecture? What uh programming languages are used and yeah u yeah how how it is deployed and and so on. Okay. So we have two products. So I'll talk about the one that I primarily focus on. So for us our main infrastructure and architecture all runs the 90 95% is all lambdas. We have about 300 plus lambdas in our ecosystem. We run hybrid of dynamotp with Aurora RDS and that's the exact thing progress and we're slowly getting into argate services. Um so that's a little bit different model but uh that's the primary architecture of our ecosystem you know the standard services like S3 HP gateway just standard service stuff that as ecosystem provides Tommy. Oh yeah. Uh on the E-Club side, what we have is a blend of Python and Django on the back end. Um and then JavaScript, TypeScript on the front end and we run everything on that side on AWS ECS which uh gives us that flexibility and consistency across all our environment. Okay, thank you. Um so our CI/CD pipelines automate the process of building those Docker images and then deploying them onto ECS so we can roll out those changes quickly and safely. uh but to you know do a quick walk through uh across this little diagram that I have here. Uh so imagine a user interacting with application. What they hit first is the load balancers that we have here and then uh it distributes traffic to the web service that we have on the back end which runs ECS and instrumented with the open telemetry uh SDK. Now as this web service this web service or web services begin to process this request uh what it does is to offload uh some heavy tasks or async tasks to this celery worker down here which also runs on ECS and is also instrumented with uh open telemetry SDK and uh thanks to the context oops typo context propagation here uh the trace context can flow seamlessly between the web service uh and the worker and um with the introduction of the open telemetry SDK now we can follow a request end to end from um inception to the end right and um both these services that's the web service and celery worker the emit traces logs um are metrics um and we sample them make sure they um they they have high cardality and they are batched also for efficiency and all these data that you know I'm talking about ascent to open telemetry collectors that run as a sidecar on um ECS to send this data to our obserility back end. So this is uh pretty much in a nutshell what the uh architecture on the e-club side of uh what we are doing looks like. Awesome. Thanks for thanks for that. Um as a as a follow-up question, you know, given given your your your system architecture, um what was what what were the top three problems that you were facing that kind of compelled you to um to go like you know to say hey I need observability. Oh yeah. Uh sure. uh I'm sure you know and uh probably everyone listening knows that observability in a distributed system is never a solved problem and uh some of the nuance challenges that we faced was again as I said distributed tracing um with microservices a single user request can touch so many services and background jobs and uh without distributed tracing it's almost impossible to reconstruct that full journey of a request from the user's end to you know the the response back to the user especially when something goes wrong right so for instance if a user says that they had a slow user slow order placement rather um we need to trace our request from the web front end the back end the salary workers and even to third party APIs and the introduction of open telemetry helps us stitch all these things together right and also have consistent context propagation um across all these boundaries and um one of the things that we were also looking at was performance monitoring because we rely on celery heavily on E-club for background processing. However, uh these things can be black boxes if you don't instrument them properly. So, we need to know not just if a task has succeeded but how long it took uh what resources consumed and whether it's causing bottlenecks along the way. um introducing celery with uh sorry instrumenting celery with open telemetry um required us to go you know with out ofthebox solutions uh again thanks for that um and to to be able to add custom spans and metrics to these things and um also error tracking um you know errors can manifest in logs traces or metrics um wherever and um the most important thing um for us at the time was to be able to correlate these things together so that's having your trace ID and span ID and logs and uh being able to look at that trace ID in the log and correlate it with an actual trace. Right? So a spike in error log sometimes might cor correspond rather with a specific trace or a drop in a metric that probably we're tracking. Right? So what we've done is we've worked to ensure that uh our telemetry includes enough context like as I said the trace ID request ID so we can pivot between the logs traces and the metrics um seamlessly. So um yeah that's what got us to where we are now. Awesome. Um Andre uh do you have other Andre uh do you have any anything else to add? Uh so for our end I think the primary problem again really was that context propagation because again the microser and distributed um nature of that. I think the other part that we weren't even really aware about is just how the system really behaves. So a big part of it is uh tying back into alerting. So depending on what vendor you use it might have automatic alerting but being able to provide that to our developers in our case uh via Terraform. So developers can open up PR, create their own alerts, and now we have that across our ecosystems has really changed a lot of um how we're able to kind of empower the different developer teams to just take control of their own alerting and systems rather than um you know just one team kind of owning that. Uh can you tell us a bit more about how are your systems instrumented like what yeah what types of instrumentation are you using? Certainly. Uh so on uh our platform uh currently for us we do use our vendor uh agent. Uh so in our end um the agent's really nice for the most part. Uh it uh works seamlessly with both of our infrastructure pieces of for lambda. Lambda has the concept of a lambda layer where you can basically put on something. So their agent just goes onto that kind of bolts right on and works essentially right out of the box. Uh and the second part to that that we had to play around with a little bit was our ECS Fargate. Uh there is a solution from Adabus called uh adabus fire lens which allows you to kind of have your task um logs uh standard error and center out to basically be sent to a specific um sidecar where then that sidecar will run something like fluent bit or um they cubit uh and essentially take your logs and ship them to where you need them. Yeah, that makes sense. Uh I would ask a followup right away. Uh so are you using hotel out auto out auto out auto out auto out auto out auto out auto out auto out auto out auto out auto instrumentation or are you manually instrumenting your your services? So for us so because we do have the 300 lambdas each one need we basically have like a core library that all these lambdas will all utilize very thin layer but in there we do have um hotel instrumentation for the Node.js JS I believe it's just the auto instrument or I think we might uh we might have changed that for the finetuner because it needs to be small uh because lamp is you want to keep them really tiny. So I think we we do have the um uh the custom one where it's not the auto instrument and uh yeah how was how was the how was your experience with with doing that so far? Uh I haven't heard any um issues. So I didn't do that one. It was actually our coworker Matt who did that one, but I didn't see any too many I didn't see any issues with that. PR went through pretty smoothly and uh the prim primary purpose was we use our own custom logger uh one that isn't supported by um hotel or vendor. So we had to use that to kind of pull in the trace ID from the um headers and everything else and be able to actually populate the spans that we require. Cool. So, so to clarify then in in a nutshell, you you basically have like a wrapper around hotel from what it sounds like. Uh kind of it's um I guess so. Yeah. Yeah. I guess so. Yeah. Cool. Cool. And um question for you around uh just to keep on the instrumentation thread um who is responsible for instrumenting? Well, that's that's an interesting journey. So we actually do have a new project upcoming. So the way our system has kind of evolved um with Terraform in particular is you know we're IA first for no matter what we do infrastructure as code and a big part of that is we use the cloud development kit extension on Terraform. So it gives us the ability to kind of have this interherence model and allows us to build these L1 L2 constructs. Um so if anyone's familiar with that it's a very useful way of uh I guess building abstractions for your infrastructure and enabling teams to kind of um handle that as well. So uh in our case what we're trying to build right now is kind of an L3 construct which is like an application service um level where we can give it to developers and they can essentially have all the nuts and bolts kind of already instrumented out of the box uh with the right permissions and the right access and the right agents and everything else. Um, and we kind of support this golden path for them. Awesome. Um, Tommy, do you have anything else that you want to add? Um, no, not really. We, as I said, what I think we're trying to get to a point where all of these things are, would I say, prepackaged for the devs to be able to um to use them, to leverage them, to do their own instrumentation on their own. And um I would say for the most part we are automating um many of the um many of our collector setup or the open telemetry setup. But uh I think at the beginning what we did especially on the e-club side was to auto instrument so we can have quick wins and and move quickly. But um when we're getting down to things like business logic um salary tasks and things that were unique to the e-club side, we manually created spans along some critical code paths um and added some custom attributes here and there so we can um get more than just what open telemetry offers out of the box. So I think um for e-club it's just a little slightly different from u what we have on the centric side. Nice. Nice. That's really cool because I think that's a path that a lot of organizations start with anyway. Like the auto instrumentation is that quick win and then it's like, oh yeah, we're we're lacking a little bit more. Let's go with the manual instrumentation. That's awesome. Um, we do have a question from uh someone in the audience from Buddha. Um, hi Buddha, nice to have you join us. So, uh, Buddha asks, are you currently using all hotel signals, logs, metrics, traces, profiles or a combination of them? Uh so we are using logs, metrics and traces. Uh so logs are coming through depending on uh which infrastructure piece. So if it's Fargate, it'll come through the AWS fire lens and fluent bit solution and then be sent to our agent sorry to our vendor. Otherwise if it's uh lambda it come straight from the um lambda layer agent um with the um yeah with the lamb layer. And then for metrics uh that comes in through a different connector with our vendor. they have a solution we install in our uh cloud environment and then it kind of pulls all the metrics over to that. Now profiles is interesting. We don't use profiles but uh the use case I think for profiles is enabling maybe uh common attributes or standardization uh and so we handle that through again our vendor has a solution out of the box uh that allows us to kind of set the schema and richer data as it gets ingested. Great. Um, and then another follow-up question from Buddha. And by the way, actually before I I asked that, um, there was a question from, uh, Manish, um, who's asking if we are recording the session and yes, we are recording the session. So, it should be available on on LinkedIn and, um, YouTube on demand after the fact. So, um, great question. So, uh, the follow-up question from Buddha was, uh, was this how you started, uh, or slowly built up to it? Um, who would like to take that one on? Um, was that one for like the terraform aspect or just in general? I I'm assuming it was for the instrumentation in general, I would say. Yeah. Uh, at least one for Tommy. Tommy had a fun time doing that. Okay. Well, um, this was not how we started. And for context, I I joined about 10 months ago. And, um, yeah, as time I joined, we did not have open telemetry setup anywhere, at least best of my knowledge. And um I know that some of yeah a couple of the motivations um for adopting open telemetry on our side was um standardization um to because we had like a patchwork of of monitoring tools like um logs metrics everything scattered all over the place but open telemetry gave us a way to now unify all these and to reduce uh cognitive load and then the interaction overhead that comes with that. Um and uh additionally we wanted to be vendor neutral you know um using op telemetry allows us not to be locked to a single vendor so I say um I know I'm getting into the motivations for why we adopted it but it also cuts back into um the fact that we did not start out that way slowly built into it and these were some of the decisions that drove um us to using of telemetry Uh yeah, I hope that answer question. I hope so as well. How long did it take you to to get into the point where you are at currently? Uh I'd say we're still we're still getting there, right? Um and if I remember correctly, we started somewhere around September. Andre, keep me honest here. And I think by March, we closed the chapter on that one. um from I would say from conceptualization but I say from implementation to when we thought that we were in a good place so roughly seven months right um trying out stuff and uh yeah I think we're in a good place now all righty uh yeah we have a we have a question in chat from Manish uh when we talk of open telemetry data there is always a huge amount of data how are you optimizing your storage and managing doing it. Uh, okay. I I I'll take this one a bit and then I'll let Andre finish off cuz he's the geek with that. Um, one of the things we do is sampling, of course. Uh, and I know when we query for stuff, we do a sampling ratio of I think 1 to 1,000 for traces at least on the E- Club side. Um, this is so uh this is tuned in this way to balance cost, performance and and visibility. And of course we monitor the effectiveness of this sampling setting and adjust it dynamically during incidents or specific services um that require deeper analysis. And for logs for querying um and most of the other things we do with logs we enforce a scan limit of 500 GB per query to keep the back end performant and cost cost effective. Um but I know again Android geeks out on on things like this. So I think you'll be able to give uh a better response. Technically we got a really big budget for our vendors. So we actually have uh zero sampling at the moment. So we just we grab it all. Um we're just trying to understand from that point and uh slim it down and um you know the first exercise that we we kind of ran through was grab all the logs kind of run like a pivot on it like the error logs do I count on uh you know sorry just general logs and are there any useless logs that are being constantly created by the system or just noise and can we remove that and the answer was yes so I think we had something like two billion three billion logs coming in that are just like success that we just didn't need to have in place for it. Um, so yeah, we could been cleaning up the system a lot. Uh, we go through all our pods and kind of hand out tickets to them as we find through the investigation and be like, "Yeah, let's get rid of this or uh you make it a debug log potentially if you guys actually need it, enable it during your debug sessions." Otherwise, let's not try to have too much noise uh into our ecosystem. I will say though that one caveat is because we are running Lambda that that one agent and Cloudatch requires certain logs to come out. So we can't get away from certain noise like right now I think we're ingesting 54 million logs a day um that we just we can't get away from and they're just noise at the moment. So uh another motivation to move to Fargate where we have a bit more control over system as we uh adapt a bit more of the observability ecosystem. Cool. Cool. Uh yeah Manish if you have any follow-ups feel free to feel free to post them in chat and I would continue with the next question about collectors can you folks tell us a bit about the way how you set up your collector or collectors what is the distribution and yeah what is the architecture overall super curious about that okay well I I'll begin again with the e-club side and again for context um as I said I I handle most of the e-lub side does pick stuff um concentric. Um and for E- Club, what we do is try to make our collector um environment aware. Um so in Sandbox, cuz what we have is Sandbox, staging and production on E- Club. So on Sandbox, we sample a little more aggressively to capture as much data as possible for debugging especially for me, right? To really know what's going on there and um also help the devs um really debug um stuff that they have going on there. But in production what we do is a little more probabilistic. I hope I got that right. To balance visibility with cost, right? And um what we do is we dynamically also adjust that sampling ratio um if there's an incident and then we temporarily increase that to capture more data. Um and again the collector definitely does all the batching filtering um and exporting telemetry to the back end our vendor. And uh of course this is done in a yaml config to define our pipeline for traces for metrics and log and to also set the environment specific parameters like service name the host type and then all the open telemetry options we want to set um on that. So that's what we have for our collector on club and uh as I said at the beginning it runs as a sidec car through the services that celery the um the jungle application also. So as a followup um on your um collector setup so um do you have um so I guess two two questions. Do you have a way to manage like a fleet of your your because it sounds like you have a number of collectors. Do you have a way to manage them effectively? Like do you use op amp for that or do you use like kind of a homegrown thing? And also do you do you use like a collector gateway um to like funnel all of your collectors into like a central gateway before pushing that off to your your uh observability? To be honest, not really. uh we just manage the fleet using ECS as I said and then um we run them outside car to our standalone services and the um environment variables that I said help us customize those things I said earlier which are the sampling rates um the exporter and the service names so we don't really have uh we don't really manage the fleets independently um or independent of our services we just run them as a sidecar and lets us do the rest for us Um, so I I think that's pretty much it. But we we also monitor the performance, right? Definitely. Um, things like the Q length and then we have some drop spans along the way. So we can scale them horizontally if we do need to. Um, but for um high throughput services like the I can't remember the name, one of our of our celery I think it's celery flour, right? Um we have of course a higher instance for that to manage um the size and scale of how things can be and to avoid bottlenecks. Um so yeah so do so just to clarify do all your um site car collectors then do each of them go directly to your back end? Oh okay got it. And it sounds like uh Andre M had a a screen share for us. Yeah. So um and just it's a visual essentially the same idea that um Tommy was walking through there. Um before we get started though, shout out to Sky Draw for anyone using that. The best platform for uh brainstorming ideas and a little whiteboard place. Uh but uh essentially the ECS Fargate again I was mentioning the ads fire lens configuration for us. um the management all of it comes back to Terraform. So that allows us to easily manage our collectors because even the configurations everything in code. So it makes that part a little bit easier. um if that's what you're trying to guess prelude to uh our vendor does have that uh instance within our um account but we don't utilize that as we found that uh sometimes the logs are being dropped either we underprovisioned it and we found that if we just directly send them straight to the vendor there wasn't any issues that uh we've noticed aside us losing some of the benefits of that additional collecting uh if we have to do some data masking or dropping those logs prior to um being sent and ingested by the vendor there. So, um definitely a area we're trying to balance. Um but, uh the other part is obviously the lambda, which again is super straightforward. It's just straight from the the lambda and straight to the vendor again um without any um additional pieces there. So, that may be an area now that we're talking here um area for improvement um because if we have those 54 million logs that are just noise, we might benefit by going to that uh filtering agent first. So very useful. Cool. Thanks for sharing. Yeah. Um you already mentioned uh this like areas for improvement and I'm wondering uh do you have any challenges with open telemetry currently and any any thing that uh perhaps the project could could do better to to serve you as a as as its user? Well, I'll say for me one of the biggest challenges was just initial setup, right? Uh configuring the connector and ensuring context propagation across all the services which required me to do you know manual instrumentation and uh of course there's a learning curve uh telemetry is powerful uh but again the API office is just so large and uh of course with in terms of best practices we are still evolving. Uh, one of the things that I'll probably like to see, um, and I think I'm speaking to the community when I say this is, uh, maybe more complex example project, speaking to myself too, of course, putting it out there. Um, but I would say the community has been has been really helpful, right? There there were so many things that I benefited from from GitHub mainly and then uh, a bunch of articles that people have written on medium, right? So, um, for me it's just more real world examples, more complex setups like probably what we have. Uh, maybe, you know, somebody just doing something that that big and putting out there for anybody to be able to use, but it say for me that's just been the the challenge. Um, not nothing too crazy. Andre, what about you? Do you have anything in mind? Um, not so much I can give feedback to really improve. I mean the whole uh time has just been learning learning learning right there's just a lot um I mean it's first exposure to it really for me it's not even that I had experience with other observability tooling it's just like straight to our vendor and then also hotel so just learning the best of the best right off the bat basically right uh and then learn the history so for me it's just it's been a constant um uphill battle to learn all the different intricacies um probably just want to echo Tommy's um message just more examples right being able pull things down, play with it. Uh getting into your hands really quickly. That's that's usually the best place to uh to learn. Yeah, I I totally agree. I think uh I think we're at we're open telemetry has been around long enough now that I I think many organizations are kind of past that 101 phase and are are eager to get into like the the more gnarly use cases. And and that's why, you know, we really appreciate folks like you jumping on onto these Hotel Me sessions talking about like your mature hotel setups because it really really helps others in the community. Um so we definitely appreciate that. Um switching gears a little bit. Um we were wondering um if you could uh if you have had any like interactions with the um hotel community um things around you know like you know if you got stucker in on on a particular issue like how how did you go about it if you got stuck like did you did you uh go on Slack or did you like Google stuff? Um how how how did that work for you? Uh I remember the first thing it was for E- Club. Um when we were trying to instrument it originally it was uh there's an extension we use for is it parallel processing Tommy? Do you remember that one? It's not uh W's wake there's another one. Well it's a Django app that we run. Uh and the problem with that is that uh there's an extension we use uh I think it optimizes performance and the problem with that is that it somehow was fighting hotel. Um yeah uh um I really can't remember the name but then I I I know they were fighting for the um thread the threading running in the background. Uh I think that that was one of the challenges that we had in the beginning. I really can't remember what specifically was but uh I know that was a big uphill battle back then. Yeah. So we had a open I think issue on GitHub with that. Um but uh luckily our vendor was able to um get their agent working so um it worked for that environment but I think um I think it's issue is still open but wasn't sure if it was like upstream with Django or was it so much with hotel itself. Cool. Cool. Um yeah thanks for that. Um, as a followup, um, have you gotten to the point where you have made any contributions to hotel, um, or, uh, or planning to make any contributions? 100%. Uh, nothing yet in terms of contribution uh, commit, but 100% we'll probably be looking at something uh, within the ecosystem of um, probably node or go, one of those two. Awesome. Um, final thing. So, I I think we've we've covered I think most of the um most of the the the base questions. We do have a a cool audience question that came in um from Esther. Uh it says, "How do you see open telemetry evolving in your workflow over the next year?" Really great question. H that is a it's a tough question. So well because I mean open telemetry I mean it's more about is it so much like the framework itself or is more about the data we're getting out of it because for me I think the the value I mean both both are great right because one gives us a standardized um tooling and framework to be able to switch vendors or anything else we have the capability now but for me I think the real value comes into what type of data do we really pull out of that open telemetry uh and what can we really get from it um I think that's where real value is. So being able to now take that data and apply user journeys and business events like those things I think are where um our organization can really uh I guess expand or benefit from for the most part. Mhm. Tom, do you have anything to add? uh not much but uh I think just like open test metro also keep maturing in our observability journey especially around logs and for complex setups like accelerary um or serless that's with lambda because I know there were there were also some battles fought there uh I also think we are probably going to have more out of the box integration um and tools to simplify the configuration for our collector right so I mean the ecosystem is growing so fast. So, um I would not be surprised to see us adopting it um a little bit more. Um as Andre said, we are growing every day. We are learning every day. Um but I think we'll be leaning more towards probably much better or more standard industry practices when it comes to um adopting hotel and uh using it to really observe our system. So uh that's what I see happening over the next year but who knows. Sounds good. Sounds good. Uh yeah we have more audience questions. Uh today we just we just uh talked over at the end with Adrian that we have really really engaged audience. So that's amazing. Uh so Manish is asking how are you doing the instrumentation of legacy systems? Is anything like that happening in like do do you have a use case for that? um think we have some older.net projects but it seems as if um our vendor supports out of the box and anything I think we even with that solution the vendor allows us to still have it in hotel format as it comes through. So um not really a concern for on our side from what I've seen so far. All righty. All righty. Uh and uh one more from Buddha. Uh how do you report on business value metrics for decision makers? What metrics do you report on? Yeah, I know I think Tommy you mentioned it that for that business logic you had to do some manual instrumentation. Can you tell us a bit more about that? Yeah. So, as I said earlier, there were um certain business logic that we needed to especially on the salary side cuz that's where we upload um stuff to um to really see how um data was flowing through that and then report on I think specifically it was it was a reporting service or I think an order service at the time. Um so what we got out of that really was just the rise and fall or the spike of um the would I say the others process within that within a certain period or within a certain trace. Um but you know reporting back to business people I think that one comes more out of the box with our um vendor and um we as Andre said build stuff that aggregates the amount of logs that we use um that translate into how much we pay for these things even though we have a big budget you know we we still don't want to be we still don't want to end up overshooting that um but I think again Andre would have more context into this cuz he builds most of the stuff that I report back to business leaders and that that's something he's so in love with. So I I would allow him take the floor on that. So I think for me the biggest one at least it's been months in development is um the most important key metric I've come to love recently is um users. So what is a single user doing across a system and aggregate that over a certain period of time. So um that alone allowed us to find a particular leak where we had a single user was a bug in our client uh and within our service uh that wasn't like a batch request where this one user within a short period of time was invoking over 200,000 requests into our system and with lambda pay as you go it's not a good uh not a good model there. So without this observability in place, we wouldn't even know that there is, you know, one single user somehow invoking 200,000 requests within like a six-hour period. Um, so these tips, it's small little things uh where now we can focus on a single user or aggregate it across a grouping uh and be able to find behaviors and it's really good when you throw into like a time series or flame graph and you can see these kind of like fat spots where you can just visually identify something doesn't look good here. Investigate That's great. Um, thank you so much um both of you, Andre and and Tommy for uh for joining us and and thank you to my co-host, the other Andre um for joining uh and um and also this is uh Andre K's first uh first time on on a stream. So like big uh big kudos. Um, awesome job as as co-hosts. So, I want to give a shout out. Um, and you know, as always, we appreciate all the stories that we hear from our community members showing how they use open telemetry out in the wild. It really helps us. Um, and it helps show folks in the community like, you know, open telemetry is here to stay. We are we've got some like gnarly use cases of it being used out in the wild and it's it's great to hear those stories. Um, coming up next, stay tuned. We should have uh another Hotel Me planned, I think, in July. So, stay tuned on the hotel socials for that. Um, in the meantime, um, coming up at the end of this month, we have Hotel Community Day. Um, it's part of, uh, Open Observability Con. So, it's it's, uh, combined with Hotel Community Day. So that's happening um on the heels of Open Source Summit in uh Denver. Open Source Summit North America in Denver. So if you're around for Open Source Summit, check out Hotel Community Day and Open Observability Con that are happening together. And I think we've got a couple of CubeCons also um coming up. We've got I think I want to say this week is CubeCon um China. And then next week, I'm actually jetting off to Japan at the end of this week for CubeCon Japan. Um, and it's the first CubeCon Japan. So, very exciting stuff. So, if if anyone's around um for CubeCon Japan, come say hello. We'd love to uh would love to to meet you and talk. Um, and yeah, I think that is a wrap. And again for uh for those who attended tell your friends if they missed it the recording will be available um both on LinkedIn YouTube also check out the hotel end user s also have a um uh a group on uh on CNCF Slack we are called um I think we're called-ig- user so come share your stories on the hotel and user sig um chat and also if if you'd love to join us for one of our meetings. We meet every two weeks, so I think our next one is next week. So, yeah, thank you everyone once again for for joining and we'll see you next time. Thanks so much for having us.

