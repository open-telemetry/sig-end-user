# OTel Q&amp;A with  Dan Ravenstone

Published on 2024-11-04T21:03:07Z

## Description

Join Dan Ravenstone, staff engineer at Top Hat, for OTel Q&A! During his 25 years as an observability engineer, he has had a ...

URL: https://www.youtube.com/watch?v=BMkckCQN8eg

## Summary

In this YouTube Q&A session, Dan Ravenstone, a staff engineer at Top Hat, discusses the transition from monitoring to observability, sharing his extensive experience in the field. The conversation, led by Adrian, touches on the significance of open telemetry and the challenges organizations face in adopting it, particularly concerning cultural shifts and communication within teams. Dan highlights the importance of understanding user experiences when determining what to instrument and how to implement observability effectively. He emphasizes that more data does not necessarily equate to better observability and discusses the need for teams to move away from outdated practices based on logs to leverage the full benefits of distributed tracing. Overall, the discussion underscores the necessity of aligning observability practices with user-centric approaches to ensure service reliability and business success, especially in a field where user experience directly impacts financial outcomes.

## Chapters

00:00:00 Introductions
00:06:30 Guest introduction: Dan Ravenstone
00:10:15 Discussion on monitoring vs observability
00:15:47 Importance of user experience in observability
00:19:50 Challenges in adopting open telemetry
00:25:30 Discussion on vendor libraries vs open telemetry
00:30:05 Communication challenges in observability
00:34:30 Cultural shifts in instrumentation
00:40:00 Instrumentation culture at Top Hat
00:45:30 Q&A session with audience questions

## Transcript

### [00:00:00] Introductions

**Speaker 1:** welcome everyone to otel Q&A and thanks Dan for joining us for otel Q&A especially on we got you on such short notice. I'm glad we're able to squeeze you in for October, especially me too. Cucon is coming up in November, so for folks who are going, a few of us will be there. Dan and I will be there from theel and user Sig, so it's going to be busy.

**Dan:** yeah, it's going to be busy. Where is that happening by the way?

**Speaker 1:** it's in Salt Lake City.

**Dan:** Salt Lake City, nice. I've never been there myself personally.

**Speaker 1:** yeah, me neither.

**Dan:** oh, so this be an experience for you too?

**Speaker 1:** yeah.

**Dan:** exciting.

**Speaker 1:** um, start asking you questions. I realize I'm the one here supposed to be with the breaks. Folks, it's gonna be a fun one.

**Dan:** oh yeah, not sure about informative but it will be fun.

**Speaker 1:** well here we go. Okay, let's start. Uh, let's start with the question. So Dan, why don't you introduce yourself and tell us what you do?

**Dan:** hello, I am Dan Ravenstone. I am a staff engineer at Top Hat. I've been there for, I guess, two and a half years maybe? No, a little bit longer than that. Come May, I think it'll be three. A little of my background, I have been doing monitoring observability for the bulk of my technical career. I started back in um, where I actually this and I usually refer to as like where the bug bit, the monitoring bug bit me was when I was at a company called affiliat, which was at Young in 401 area in Toronto. Affiliat managed the info.org regist domain registry in the back end of it, uh for back in the day. I started there in texport, but then I one day opened up my big mouth and said, "Hey listen, I'm tired of our customers calling us up and telling us our stuff is down. Can we fix this?" And so they said, "Well then you ask for it, you get it." So I went out and that's how I started learning about monitoring tools. And that's back in the day of Big Brother, Nagios, Cacti, all those fun things. And I just loved it. This kind of like, I think one of the things I liked about monitoring in those days was that like I like looking for patterns. I like detective novels. I like all the fun things. And what this do is like kind of allow for somebody to kind of like be in the front lines in the operations and dive in and find problems and report them back. And then like the better, more, the better your monitoring tools were, the more likely you were able to follow and find the problems a lot sooner before they become actually customer impact. And I was always been my kind of like push when I did this. And of course, as things progressed and modern and changed, we got into the wonderful world of observability and a whole different way of looking at it. And I fell in love with that obviously as well and tried to grow with the times and see how I, you know, how being proactive is a lot better than being reactive, which is kind of like, you would think is an obvious thing, but it takes a lot of people time to kind of get their heads wrapped around. Um, so yeah, that is kind of uh how I got to the, how I got to this part of the world and a little bit about me. There's tons of other things, but we don't want to go into those because then we'll get lost in the weeds and we'll never get out and we'll be lost forever.

**Speaker 1:** sounds good. Well, thanks. Um, and you know, like you've been in this for a while, like when it started out with monitoring evolved into observability, what kinds of like big changes have you seen um in the course of that time?

### [00:06:30] Guest introduction: Dan Ravenstone

**Dan:** uh, let me start off the things I haven't, I haven't seen change, which is still a problem, alert. Okay, alerting still has not changed surprisingly. Um, but what has changed, and I think this is one of the things that um we're trying to get to, I, I so I focus on alerting because I was by myself time to think. What has changed? So now that you know my secret, I just thought myself a couple more minutes. So, but what has changed though in observability and in the open Telemetry especially too is it allowed for us to actually look at our services a lot differently where monitoring was kind of a very largely reactive kind of thing, and we only had the symptoms of what was going on. So if everybody has ever used Nagios, is well aware of the sort of out of the box HST monitoring sort of plugin you usually include, which is like your CPU, your memory, your disc SP, and these things have been carried throughout time in, you know, to con be indicators of a problem, but that's usually not quite the case anymore, is it? Um, especially way when we have Kubernetes and containers and other tools now that these things shouldn't really play a role in us. So it's like more like, so what else is impacting the service? And we never really had those, that ability to look at it until we started sort of aggregating our logs. And even when we aggregated our logs, it was still strangely problematic, um, because you know, we had guidelines on how to do structured logging and that kind of thing, but still people would do their own thing and things were never, and so even with all the good documentation in sort of best practice around it, we still logs were still problematic. And again, you still have to still collect a bunch of data before you can actually understand, "Oh wait a minute, this is a problem," before actually kind of knowing beforehand. What observability has done has taught us is how to sort of like set things up in a more of a way that we're looking at it as an overall user experience and we don't get sort of sucked into minutia sometimes. And there's because there's a lot of red herrings in operations when you're especially when you're in the middle of incident, you're looking at all this data and you're like, "What is the actual problem?" We have high CPU over here and we have slow load times over here, we have latency here, we have some errors here, but which one of these is the smoking, I would not, I don't want to use a gun, but for lack of a better word, smoking gun, right? And I see that with observability has changed how we look at that. However, that's for this probably this group a lot of people are familiar with it. There's a whole world of engineers out there who are still having, are still trying to get their heads wrapped around this concept. And this is why I'm glad we have these conversations and I try to get those people, this is like, this is targeted towards them of why this is so important, why you need to start looking at this because there's a number of good things. One, you know what the user is experiencing and that's all that matters. If you know what the user is experiencing, that means you can translate that to how much you're making, like very, very, like very, it's a cost thing and it's a money thing. If you know if your users are happy, that means you're making money because they're spending money because they're going to keep using your service. If they're not happy, they're not going to be using your service and they're going to walk away and use something else or just not use it that often and try to avoid as much as possible. That means you're not gonna be making the money anyway. Um, and I'm gonna get in the weed, so I'll, you're gonna hear me say that a lot because I just do. So you Adrian, it's all up to you to rope me back in.

**Speaker 1:** don’t worry, don’t worry. But I do wanna, um, you said something really interesting, um, in terms of translating it, um, in terms of money because you know, like observability adoption and maybe I'm getting ahead of myself here, but I like the way you said, um, in terms of observability adoption, like it's as much a top-down thing as a bottom-up thing. And one way to speak to the benefits of observability is in dollars and cents and executives speak in dollars and cents. So, um, proving the worth of having an observable system I think becomes really, really important and being able to use that language, um, with executives to be able to convince them I think is very valuable.

**Dan:** so, and, and that's what I've been trying to focus more on. And like there are like there's quantifiable, quantifiable reasons why to look at this. We can do that why observability, excuse me, uh, provides value toward us at the end of the day. But I think we're still getting our, we're still trying to get that figured out, I feel like that. Um, anyway, you know what before we jump in, let's keep moving forward because I, I, I could go into a whole other area and I don't know, I don't want to sort of suck a lot of time in there, but it is one of those things I think that does have, is worth having conversations not with just those in the community but also talk, like I mean if you're, you know out there find out what your what the observability sort of platform or the concepts are your where you work, what are the drivers, why aren't you doing it? Why aren't your, why aren't you using these things? And ask those questions and then like, you know, that maybe we could, that could help and feed that back to community because that could help us build out the reasons why you should be doing this because it does make better sense eventually. Um, but yeah, that's a sort of a general call to anyone who's out there who needs, who wants to sort of get going on this and need help.

**Speaker 1:** yeah, I completely agree. And I think this, uh, this takes us to our next question, um, which is, uh, you know, like you've, you've done this for a while now, like first the monitoring space now moving into the observability space. What do you think are some of the main challenges that most organizations are facing these days when it comes to observability?

### [00:10:15] Discussion on monitoring vs observability

**Dan:** uh, there's a lot of challenges and I've been trying to get my head wrapped around some of this stuff. One is communication, I think so some of it, I think it's mostly cultural really. If you think about like, I mean there's not a technical reason why people shouldn't go to this other than resourcing in time. Uh, so and, and I mean this might be jumping ahead of a little bit, but where I am right now, we, we are using a current vendor, uh, which will remain nameless, but we use a current vendor right now for our monitoring capabilities, but we're getting away from them because of the library we're using is, is vendor specific and that means that we're basically, when it comes to doing any kind of telemetry, it has to use that particular library and theirs is only getting, and that's, that's one challenge because if you think about like, okay, so you're using this one vendor, it's cost you x amount per month or per year, it's exuberant. However to get translate or get off of that one and go into another one where you have more options like with open Telemetry takes time. It takes, it's not like an, like anybody who's ever gone on any kind of migration from one tool to another knows how long it takes. And if you're, and you have to have a few things in place to actually say why you should do this. So one is like, of course, obviously the cost, right? Well, but you know, it's hard for some executives to say, well, we're spending this but it'll cost us this much to get over here and then we'll see the cost value. So there's this huge challenge of just trying to shift things over, spend the time to do it and then do it properly. And that's a whole other challenge is to say, okay, yeah, doing open Telemetry is one thing, but doing it properly, like every other software or any other tool that you have out there, you have to still use it for the right reasons and do it properly. If you don't, you'll still run into issues and whatever you're trying to sort of get away from over here will still might come up in a different way over here. So there's those, that's, that's one of the challenges is just getting that, just doing the whole shift over. Uh, I think that's a where a lot of people and they don't know how to do it. And so, and what happens too is then they do a shift, they try it out and then they get burned for because, you know, they don't know how to do sampling or something like that and then they feed it off to some tool like let's say a tool that you get charged by event volume, you're going to, even though they're using otel, they still became like, oh, I saw one company who did that. They start shifting using otel, but they started setting all this volume, all this data to it. They didn't do it right. They were having like a hundred thousand spans per trace, which is a huge amount and they weren't really getting a lot of value and their costs are soaring. And so it's like, well, you know, then they kind of just drop and they, well, we'll figure it out another way. We'll just use some other tools to figure out when there's problems with our service, which is sort of like a defe kind of attitude, but it does happen because they don't, you know, because there's a lot of work involved doing this is more than just a simple lift and shift. It's a lot more involved to it. Um, trying to think of other challenges, like those, that to me is like some of the big, some of the bigger challenges and I think a lot of people could probably emphasize with that one as well. Um, there are others obviously, but I mean, I don't, we don't have a whole lot of time and we could probably go on just you and I alone, I bet you we both have experiences on some of those other challenges we have to experience. But I mean, uh, I like to see like, like how do I get, how do we get past that is kind of like where I want to target, how do we get past these challenges and move forward, which I don't have a proper answer to, but I will want to talk about it.

**Speaker 1:** you touched upon something that um, I think is worth mentioning as well, which is, um, moving away from like vendor libraries to open Telemetry. Um, because you know, even, even though I, I think one of the, one of the wonderful things about open Telemetry is that it's vendor neutral, um, and so it makes it once you've moved all of your instrumentation to that, super easy to switch vendors, I would say relatively easy, right? Because they're still like, yeah, like at least on from the instrumentation standpoint, it makes it super easy. Um, but, but it, the challenge though is convincing folks to move away from those vendor libraries and I think that becomes, um, that becomes a bit of a sticking point because it's like they, you need to move away from them because maybe that vendor is too expensive and you don't want to go with them and you want to try something fresh, but there's so much time and effort and learning curve all that stuff associated with reinstrumentation. And that what are your thoughts around that?

### [00:15:47] Importance of user experience in observability

**Dan:** well, and that's a very, very powerful point you're making because first off with vendor libraries you get in the habit of doing things a certain way, right? And so again, I do not want to, there's no like, but the thing is, so some vendors may do things one way, but it's not part of the industry standard or the best practice we see across the industry. So they may like name things a certain way or do things a certain way and people get used to that. So not only is it just shifting a library over, if it was just a matter of shifting a library to one or the other, that's in itself is a challenge, but there's also all the behavior that gets attributed to using that older library that some, for some things you got to go, well listen, you don't need to look at it like that anymore. You need to look at it like this now. And so it's not just a technical shift, it's also a cultural shift and how they even like envision how their service is working with the new library with using open Telemetry. Um, because open Telemetry is current, it's trying to, it's keep, it's adding new things as we speak as every day. Uh, it's, you know, it's, you know, we're seeing a lot more love now for mobile, which is wonderful. I'm so excited to see some of that because I feel that's largely been ignored for a long time. But we're seeing these things getting pulled together and we're having this, and it's kind of like keep following these sort of patterns and following these, this, these standards. And so make forces the developers to think a certain way. A lot of them are not used to that yet. A lot of them still want to develop. And I realize one of the things I noticed too was for developers perspective, I don't, I Adrian, you, you do, you're a coder by nature, right? Like that's kind of, yeah, yeah, that's when you, yeah, that's your back. So it's not mine, but I know that when I did do some coding though, I would always print a line at certain parts in my code to say, "Okay, where am I at?" So you're even like logging was, is kind of part of your local development experience, totally. Right? Now we're telling them, don't worry so much about logging, use instrumentation. Well, you still have to have a mechanism for them to actually print that to screen. And so it's just changing how they even develop in the beginning. Um, and that, and that's, I think where we're facing some challenges there is that it's not just a, it's, it's technical, it's cultural, it's, it's about even how, what you've been taught in the universities and how you even taught how to develop is changing a little bit and not everybody's quick to embrace change, not, and you know, especially when you've got your CEO or your, or your, or your, um, your features team or product team saying we need this out now. Well, you know, you're not going to worry about trying to learn a new way of doing things and it's, you know, oh well now how my service is working from open Telemetry perspective. It's like, well I just need to get this out so I just slap these log lines in, boom, gone. It passed, you know, and we're going now it's in production and we move on. Right? So there's that, that it's that kind of like also encouraging and supporting our developers to have the time to learn this and work with it so they can actually use it to their benefit and perhaps be able to do better code before it even hits staging or development and then, and then once it's a production, you know how it should be based on all your stuff, but that's, it's getting that linking those people together, getting them to work together in that thinking, that's, that's a huge challenge.

**Speaker 1:** yeah, absolutely. And, and getting, getting them used to, as you said, like the nuances of, you know, instrumenting with open Telemetry, which open Telemetry wants to standardize how you're doing like your main signals in a certain way, right? Which as you said, maybe different from how you've been used to doing it, whether it's like through your print statements or vendor libraries or whatever. It's kind of like, so back in the day, um, this might be pre a lot of folks, I don't know, but I used to work a lot with SNMP and what I liked about SNMP was it was basically a protocol that was agreed upon by the community and it was very basic, but even then I saw this with logging too, but even with SNMP, I would still come across MIBs, I would still come across libraries that were not properly done. So it required me even like because I knew it, I would have to go in and kind of redo maybe a MIB to so I can translate the OID properly so when it goes into our, or create a MIB because it wasn't available, it just, it just had these voids available from an interface or something and you just have to guess what they were. Um, even with standards in place like that, something has been in our sort of part of our, our industry for so long, no longer really being used anymore. Um, well actually still is by network devices, but that's not hopefully we don't have to worry about that right now. Um, is, is that, you know, even with those things in place, it still takes time for people to even then there's still mistakes and things happen. So with a new thing like open Telemetry and, and being setting standards, it's taking a long time for people to understand how to use it to do it, to set it up properly and we get their most value out of that instrumentation, the most value out of using that from the beginning of the development cycle.

### [00:19:50] Challenges in adopting open telemetry

**Speaker 1:** yeah, absolutely. Um, I do want to switch gears a little bit and talk um, a little bit about specifically um, the challenges that you're facing currently in, in your role um, when it comes to observability.

### [00:30:05] Communication challenges in observability

**Dan:** oh, I'm really not my own mental health problems. Okay, that's different. Um, yeah, so my, my challenges have pretty much always been, I feel like it's always been the same. It's communication and not because I'm my challenge for me. So I've been in this business for a while. So what happens sometimes is as I make the mistake or the or make the assumption that folks know what I'm talking about and I have to be careful sometimes. Like for, like I can have these conversations with many people in the same community is like within the monitoring observability community, and we could go on for hours about little nuances of something, whatever, maybe people logging or tracing anything like that. And we can go on and talk about that as ad nauseum. However, when you, when you're talking to other people and you, you talk about basic things like um, like the red method, so rate of errors, um, rate of requests, durational requests, which is a very basic kind of concept with the monitoring world, um, and which has helped kind of like we see a lot in even with observability too to a certain degree, uh, people don't understand that. And you make the assumption that they actually kind of look at things in that light, but they don't. They still look at it from the old school. And then like honestly, like at some places I've been, it's like they still see CPU as an indicator of a performance problem, but it shouldn't be in my opinion. It should because that's not CPU can, is cheap. You can get more. So that should be a scaling problem. Make sure you just, you're scaling properly, you should be good. However, I make the assumption people know what I'm talking about and that's one thing is I got to con, so I have to constantly be patient and communicate the same message over and over again to make sure they understand because people, even if I tell them two or three times, they still don't clue in. I had one conversation with one engineer for about a month. Um, and it took a month for them to actually understand what I was driving at. And this is like because they were in different time zones, so that's why it took a little bit longer because and so, but it took a while before they actually, "Oh, that's what you meant." And as soon as they were able to make that change, everything was working better and like, "Oh, now I see where you're driving at. I now see the point you're trying to make." And it's just you have to be diligent, you have to be patient, you have to communicate, and you have to change how you communicate sometimes too. Like you just can't say the same words over. You have to rethink, "Okay, you're not getting what I'm saying. What will help you understand?" And then try to reward it so people understand. Um, and you met my friend Damian who, who I work with, he's actually the same place as I am, but Damian who also works with me on a lot of stuff and we, he's more of a, he has a different background, but he sometimes steps in and sort of translate what I'm saying because sometimes it helps to have a different, because he puts a different spin on it and all, "Oh, now I get it." So that's those are some of the biggest things I find, um, to today are still the biggest sort of people get it. It's just a matter of once, but just getting them just lead you leave the horse to water type thing, right? But I am, it's just leading them there. Once they're there, they eventually do start to drink because they kind of get, "Oh, I see this is good for me. I will do it now."

**Speaker 1:** for sure. But getting there, there is sometimes you gotta kind of be patient. You gotta pause for a while and say, "Okay, yeah, you want to look at the pretty flowers for a moment, that's cool. You watch TR some grass, great." Okay, finally I'm equating developers and engineers to horses, so hopefully nobody will take offense to that, but in the nicest possible way.

**Dan:** honestly, I mean, you know, it's not like I'm trying to hurt. I think you like you such a good point though with, with like experimenting with different ways of doing the messaging because as you said, like you can't just keep repeating the same thing and then hoping that people are going to get it. Um, you have to, you have to come up with, with different ways of saying it and, and another thing that you and I have discussed before also is you can't tell them that they have an ugly baby. You can't say, "Your thing sucks. My way is the better deal," because people will get very offended, rightfully so. So you have to be very tactful in your messaging.

**Dan:** exactly. No, you're right because they'll just take offense and then they'll walk and then everything you say afterwards it falls on deaf ears. So you don't want to attack them. You've got to draw them out. And we, we wear a lot of different hats in this role when it comes to observability, I feel. It's not just about knowing how to do it technically, it's about how to assisting folks to see how they can learn from it and grow with it too. And that is a huge challenge. Uh, that's why I've done like a lot of my videos I've done too, like, uh, not too many of them been exposed publicly. I've done them internally with at work, but I will put on different characters. Like I would honestly like do different characters to help people understand just so they have something so it's not just another boring, "Oh here we go, get told off about monitoring observability. Yes, we know Dan logs are not monitoring." Okay, like I, but I try to build it up so they, it resonates a little bit because they're more engaged because, "Oh Dan's put on a funny wig and he's making a funny voice," but that point made actually does make sense. What, so then you know, it starts to click eventually. I hope, maybe I'm wrong. Maybe they just look at me and laugh and just walk away.

**Speaker 1:** nothing sticks.

**Dan:** nothing sticks, yeah.

**Speaker 1:** I like that though because you're, you're also like putting on different personas, right? And I think people also res, like different personas resonate with, with different people. They have to, at the end of the day, it's got to be how, how does this benefit me, right? Because otherwise if they don't see where they fit into all this, then it's falling on deaf ears.

**Dan:** exactly, how does it benefit me? That's a good valid point and, and it's getting them to see that there is a benefit to them. It's just like, "Alright, well you just have to sort of showcase this several different times." Um, I know somewhere along the way I heard somebody say that how when you're me creating a message for your organization, you got to do it like seven times in like different formats to get it so people all kind of get it and think about it and understand it. And I think that's got some truth to it to be honest because even to this day, I'm still telling people, "By the way, I've, you know, this is the reason why we're doing this," and you kind of go through that process. So, um, yeah, no, it's totally, totally true.

**Dan:** yeah, and I, I think there's also the seeing is believing sort of thing to it too, which is like, yeah, okay, understand how like open Telemetry works in theory, right? And but it's not until you put it in practice and instrument your code for the first time and even if it's like some, you know, I added a trace, I added a log, whatever, then you see it go through the collector and enter your observability backend and you're like, "What? My life is transformed!" Just even, and you go like, "Now what I do?" 

**Dan:** yeah, right. It's like you crave more, right? Like give them taste it and it proofers in the pudding, as the saying goes, is a very, very true too because I have seen a lot of success once you get the ball rolling, they start to implement something and I, and I, I've seen it over the years in different tooling. Um, I think only once have I seen it not work, but that's only because I, that was more my arrogance than anything else. So I've dropped arrogance at the door after that and never did it again because I want to use a specific tool for the job and realize that that's not the way to approach it. Um, and that's the other thing too as engineers, as leaders, technical leaders, we should never push a tool in my opinion. I think we should always look at what the organization needs, what's most important thing to most value to the organization and then making sure we have the right practices in place, the right standards in place and that will event then we can figure out what tooling we'll use after that. Tooling should be always the last thing you should decide on how you want to visualize or work with your data, that's my just my two cents. I don't know how the people that might cause problems, but who knows.

**Speaker 1:** now here's a question for you along the lines of tooling because we see this a lot in tech where you've got like a team, they start using a tool, but then the corporate standard for tooling comes out. How do you reconcile, you know, the teams that are off doing their, their own thing with the corporate standard? Because sometimes it's, it's really hard to get folks unstuck from that.

**Dan:** you, you can, you know what, you got to, I honestly personally that when it comes to that thing is like, okay, try to get them to, as long as they're following best practices, following industry standards, then you can only push so far. You know, like you could say, "Hey listen, you know, stop using this particular tool because it's archaic, it doesn't give you the same value for what you have." But you know, and then you're going to have push back. I mean, it doesn't matter where you are, even when everybody's on board, you're going to get push back because people want to do things their own way, right? There's always that kind of like, "I know better, I know what I'm doing," and you have to be delicate and you have to be patient and you have to be tactful. Um, so it's better to just, I would say just try to encourage them to use best practice and standards, but don't try to push them off the tool if that's, I mean, if somebody, if somebody higher up wants to make that decision and call them out on it, let them do it. But you want to, I think it's more important to have the standards in place because if you can kind of tie everything together and you have a proper process flow and everybody can still can't get to the root of the cause of the issue, then that's better than just saying, "You need to use x for your stuff moving forward, tough nuts," because that, that kind of thing doesn't work either in my opinion.

**Speaker 1:** yeah, that makes a lot of sense, especially like, you know, I, I'd almost prefer to like, "Okay, you use whatever observability backend you want as long as we're all instrumenting with open Telemetry." I feel like that, that is, that is the main thing.

**Dan:** yeah, that is the bar. Exactly. Now when you, um, when you joined your organization, um, were, like was Top Hat doing observability at the time? What was, what was the landscape like at the time?

**Dan:** so at that time, so we were at, so we were not, but we were, um, so this is one of those things where I kind of like I had to be very patient, but they were like, so when I first joined, we were using a vendor. Um, everything was, there was instrumentation in place, there was logging in place, there were, um, well I should say there was traces in place and there was logging in place. They were linked, but they weren't, um, it was done, the initially was rolled out about five years ago years ago, but no other real work was done, just kind of building on what they already knew moving forward. I was asked to come on and actually bring to take the company off the vendor and migrate to open Telemetry. So that was two and a half years ago and we are almost there. Like this is, uh, I think, uh, we have one more service to migrate over to open Telemetry this month and then we're done. So it's taken to an almost about two and a half years to get to kind of get to this point.

**Speaker 1:** amazing.

**Dan:** yeah. It was hard obviously, but it was also, it was challenging. Um, had to be a lot of patience involved. Um, and what I did though in the beginning was actually kind of go back to remember the, uh, when we were at Monorama together, I did a talk on alerting and I did that talk because I kind of was, that was a precursor to where I really wanted to go, which is actually getting into SLOs, right? But that was sort of like, "Here, here's how you can first sort of understand your landscape when it comes to alerts and why." So you've got, and if you can categorize them in a certain way and realize a bulk of them were a lot of noise and doesn't have any value, help people start seeing those things in that kind of light and then start showing the value of using service level objectives and how that can actually help with end user journeys, help actually bring the user experience home to the developer, so they understand, "Okay, I don't need to alert on, you know, when the load balancer is hitting 5xx errors when I should be alerting on whether or not the service is being impacted by that and how it's being impacted." And then, yeah, I mean those 5xx are important, but where are they important? How is that impacting the user experience? If it's for some feature that we don't really care about yet, we don't care about the 5xx because we don't need, or we don't need to be alerted about that in the middle of the night. But there's, so that this was how we were trying to get that process going and then they start doing the migration. "Okay, now that we know how our service should look, now we can do our instrumentation on that," moving forward. So we've, we've done started off a lot of instrumentation, but now we have those user journeys to reflect upon and go, "Okay, we're missing this, this function in our auto instrumentation. Let's make sure that's part of that because that impacts the user's experience." So yeah, so it's been a while, uh, but we are getting closer and closer to getting over open Telemetry. We've set a lot of service level objectives. Two of them have already helped out, like alerted us like two days before an incident actually happened. So we're already seeing some value in this. So it does, again, proof this is in the pudding, right? And we're already seeing some of that and we're, you know, it's, it's great. A lot of churn, but in a lot of like turmoil, a lot of folks were like, "Oh no, we're losing this," but they're seeing that they're getting more value out of how we're doing things moving forward and they're able to query things better because they're able to have more richer data available at their fingertips.

**Speaker 1:** so now in terms of vendor, did you end up switching vendors from the one that you were using when you first came in or?

**Dan:** so we had the one vendor we were using, we're still on that same vendor, uh, but as of the end of this month, we're no longer going to be using that vendor. We'll be on another vendor moving forward because we're pulling all their vendor specific SDKs out and we're just putting in open Telemetry in and we're doing that replacement. So it's kind of like now we're getting to the point where the last in that, in those open Telemetry SDKs are pointing to a different vendor for our visualization of data.

### [00:34:30] Cultural shifts in instrumentation

**Speaker 1:** cool. Awesome. Um, alright, moving on. Um, yeah, so the next question I had is, um, you know, now that you've gone through the exercise of, um, you know, in of having of doing the instrumentation, so actually around instrumentation, is there, um, what what's the instrumentation culture like at Top Hat? Um, is it because you know, one, one thing that I've spoken about in, in the past is like it really instrumentation has to be the responsibility of the developers. Do you think that there is that attitude of developers instrumenting their own code where you're at?

**Dan:** not as of yet. Um, and there's a, the reason why, and this is nothing against anybody, so I hope anybody who's at Top Hat listening don't take offense. I love you all. Um, no, but it, it's because we were kind of like once we started getting going on this whole transition, we had a very short period of time. So there's been a few instances where staff engineers and principal engineers have stepped in and done some of the work, laid the groundwork for the other team and they kind of just went, flipped over things and did things. The most effort they put in was actually in the user journeys themselves. So the auto instrumentation has been more largely being kind of like we've, we moved this, put this in and they've just kind of looked, done it. So they haven't really, it hasn't part of been ingrained in part of their cultural thinking yet. It's still largely kind of like a checkbox for them to sort of, "Okay, we've gotten, we've, we've moved, migrated to open Telemetry, job done, move on to something else." And little do they know that there's, there's a lot more work involved, but don't, don't scare them off yet because it will help them. But it is something that has to be, I think, and what I we've tried to do is try to make it so as easy as possible, less not as impacting as far as their day-to-day life, but try to slowly get them to start looking at things and so going over things and saying, and, and using the tooling that we have available saying, "Here we know you did it in this particular tool, this is how you should do it in this tool," and providing that data and then doing training on that too. So trying, so they're slowly, it's slowly becoming part, but I would not say it's 100, like it's far from 100% yet. It's still, there's a lot of work to still to be done there just so that they start to, and they all understand why it's, it's part of the value, right? Um, it's only like I think right now it's still a handful and I could be wrong. Maybe I might get in from snack later. I'll get, you know, told, "No, we all love it." Who knows?

**Speaker 1:** in terms of instrumentation, like what, uh, what languages, um, are part of like your, your application landscape?

**Dan:** um, mostly Python.

**Speaker 1:** oh, okay, okay.

**Dan:** so mostly, so we're, what's that?

**Speaker 1:** oh, I was gonna say, and you alluded to some auto instrumentation there.

### [00:40:00] Instrumentation culture at Top Hat

**Dan:** yeah, so we did auto instrumentation in our Python code. Um, and so we, and we use a couple different frameworks, but I think we're just trying to work towards getting to one framework. Um, and then we have, and then yes, it's fortunately there's not, so when it comes to, so we can, you can actually almost kind of get away with like doing a wrapper sort of library for people and have them just use that and then yeah, to get. And that's kind of what we did. We have this little wrapper library set up that uses otel with the, you know, all the different things that they need to pull in there and people just kind of just dumped it in and did auto instrumentation the way they went. So they kind of like, so it's kind of like all done for them and they just kind of include this in a way they go. Um, but I think as we progress, it's going to be like, "Okay, let's start target this is my next stage will be talking to first off the PMS talking to them, okay, this is what we have so far, these are the gaps we have in our telemetry data, these are the SLOs we have set so far. Let's decide on how that should look because your team needs to own this. Your team needs to act on this. So when your error budget gets exhausted, your team needs to take that on in the next sprint." And that's one of, and like so that, and again, the technical side is not that difficult. It's the culture change process change requires a lot of effort, right? Because now you have to get people to say, "Okay, well instead of when we burn through our error budget, we need to take deal with this the next sprint," well that's usually, "Well that are this future that we need to push out," which, which one do we do? And so, but they have to, that's what I said when I first started this is that you have to agree upon this process. You have to think about you are making a contract with the company to say, "Yeah, we will act on these SLOs because they will impact the user journey. They will impact our users and that's what's most important to us today."

**Speaker 1:** so, for those who are not aware, Top Hat is an educational platform. So what our users are teachers or professors and students. So imagine, you know, if they're taking an exam and then everything goes sideways, that's going to be really, I mean, anybody who has kids or has been to university knows that that could be a huge frustrating factor when you're not, you know, if that's not working properly.

**Dan:** so yeah, it's important that, um, oh I did get a message. Somebody did tell me we all love it. Thanks Mark. Yeah, so um, yeah, uh, now I just completely thrown off there. Uh, what was I talking about? ADHD. 

**Speaker 1:** you were talking about the SLOs and how frustrating it is, uh, if, if the consumers of your platform, um, if it's not working as they expected.

**Dan:** yeah, yeah. So, and that's this kind of fundamental shift is going to SLOs and helping them see that and get through that. Um, yeah, because I think now I just went off the rails. I so, but that's all good.

**Speaker 1:** cool. I have one more question that I wanted to ask and then we actually do have some questions from, uh, from folks. See that our on our board, which I'm very excited about. Um, final, final question. Um, I guess there, the two questions I guess, um, are you, um, if, if are you using Kubernetes and if so, are you using the otel operator, um, to manage your collectors and to manage your auto instrumentation?

**Dan:** no, no.

**Speaker 1:** yeah, so we, um, I'm glad we're not, um, just because, and this is not because I think that Kubernetes has its place in, like all tools has its place and I think for what we're doing today, it would not make sense. It would add a layer of complexity that would completely destroy our service. So glad we're not using Kubernetes. I don't think it, not saying that there's anything wrong with Kubernetes, I just think that you, if you're going to run a Kubernetes environment, you need to know it.

**Speaker 1:** yeah, yeah.

**Dan:** right? And so if you don't know what you're doing with Kubernetes, you can really, really mess up your service. And I've been at places where we only had one or two people who knew Kubernetes and we had a number of problems always with our clusters. So I'm, when we're ready though, I think we'll probably make, probably make that push, but right now we're just in containers. So it's all just through, yeah, just containers, but not a Kubernetes.

**Speaker 1:** gotcha, gotcha. So you're using like another container management service, like a cloud provided one kind of thing?

**Dan:** a cloud provider container service, yes.

**Speaker 1:** gotcha, gotcha. You're trying to avoid all names of vendors right now.

**Dan:** yeah, yeah, let's be agnostic.

**Speaker 1:** and, and final question before we get to the questions. Um, collectors, what's your collector landscape like?

**Dan:** so we're using a combination of otel collector, um, as a SAR and then to a primary one that aggregates it. And then for our traces, we use a with one company that we're work where we're moving towards. Uh, they have their own collector that we send our traces to that we can do additional sampling rules on and send off. So, um, yeah, yeah. So we, um, so right now, because, and I really, really love the fact that you could choose, you could do send from code itself, but I love the otel collector because simply because you have these processors, you have these exporters and you can send to different places as you see fit. I've had conversations with our data team saying, "Hey listen, if you want stuff, otel is collecting a lot of this right and we don't have to send all of it to our for production. Like as far as what we're looking at as far as like would give us an idea of the user experience, but there may be things you are collecting that from our otel that we can send to an S3 bucket that you can pull in later for whatever you need. However, um, you know, we have, we're still haven't gotten to that point, but that's the beauty of the otel collectors that you can kind of like just point to different things and I love that because then it there's no code change. So if we decide we no longer want to use this or we want to add logging or we want to add set get, we start using, we don't use Prometheus now, but let's say we start using Prometheus as an open source tool, um, as an example though, uh, you would, we can easily just start getting that information right off the mark. So it, I just, if anything try not to use the sampling and the old push from this code itself. Use a collector because I love the fact that because it makes life so much easier. Then you can just, if you have to do little tiny changes of tweaks to your sampling or whatever, you don't have to go push it out to like the production environment again. It's just a small change on your collectors themselves, which makes life so much easier. That's just how I think of it.

**Speaker 1:** absolutely. And, and you're using all three like major signals, traces, logs, metrics right now like for otel or mostly traces right now?

**Dan:** um, so there was a decision to not do so metrics. We still are capturing the infrastructure metrics from the cloud and the logs are primarily staying in our cloud provider. So all of our services are, are just logging by default from container to the logging, um, in our cloud provider and they are staying there for the time being. Um, because that was another thing to, to start tackling and I didn't, I just, I felt like that'd be too much for folks as they're transitioning from the one vendor to open Telemetry for them to like get their heads wrapped around would be like, "Oh, by the way you also need to have structured logs too and this is how you should do it." No, this is just gonna, it'll make more confusion. And so I figured just, just focusing on one and then adding things as they go along as they, as they prove useful. So yeah, that's how we kind of approach that whole process.

**Speaker 1:** makes sense, baby steps.

**Dan:** exactly. Cool. Um, okay, so we're going to turn to the board and if you want, you can take a peek at the questions. They're on the, on the chat window, there's a link to the board. Um, take a look. So, um, let's have a look at the questions. So the first one is what is your opinion on Telemetry quality as a way to reduce spend? Is more data always better observability? Great question.

### [00:45:30] Q&A session with audience questions

**Dan:** awesome question and that's a tough one. Um, I don't think I can, I hope I can answer that in my opinion. Um, it's about quality. You need to actually, because really with the end of, because you can have a bunch of things going through, but if you're sort of, so let's just take a, a standard trace, right? If you have, um, a bunch of spans in there that are doing sort of some things that you really know it's just back and forth and you really like there's, I've noticed some traces where there, it there's, um, I forget what it's called and I, I wish I had a sample the code before. It's been like two weeks ago one of our engineers cleaned this up and what he did though is he looked and realized we're making these calls in the code that we're, we're adding span events to and we don't need to because so you're looking at the quality there and then you're not, you're seeing what's invaluable and what's not. So more data doesn't always mean better telemetry. Quality data means better telemetry. So in my opinion, you've got to look at what you're getting, making sure does that reflect the user experience? If it doesn't reflect the user experience or is not exposing things where you would say, "Oh you know, like that's kind of like the whole the concept of the unknowns," right? Like you're exposing those unknowns. You're not able to see those things and then you're not, the quality, it doesn't matter how much you have if you're not seeing that stuff, then it doesn't mean you're, it's the quality of the data that's most important and that will help you reduce spend because then you know what's good. You can sample that partially out and you're going to make sure that those, that the error prone things bubble up and become more apparent when you're looking at your data. Hopefully that answered the question.

**Speaker 1:** yeah, that was a really great answer. Um, yeah and I, I agree with you. It's, it's really like what's the point of capturing in telemetry for your system that's useless to you? Like that's just, and I think that's, um, that's a pitfall that a lot of people run into also with auto instrumentation initially, right? Because there's a lot of stuff coming in and, and, and fortunately now I think there's a way to actually switch off auto instrumentation for certain libraries so that you can, you can really zero in on the stuff that's, uh, super meaningful to you.

**Dan:** so yay, yay, cool. Okay, next question. Um, how do you approach the what to instrument and how much to instrument questions with your teams?

**Dan:** so that kind of is it linked to the previous question too in my opinion? Um, because what to instrument is again goes back to, this is so I went through the whole process of working with the teams, the development teams and the product teams to understand what the user journey looks like. So I went through and said, "Okay, what is the, so we did the user journey, we looked at their servers and I asked this like some did more than others, uh, but more or less it was like I asked him what are the typical interactions a user would have your service and write it down and then write in in so in plain language and then in plain language describe the tech, what's happening in the background and then we also talk about the prerequisites of what people expect when they interact with that system. So they have to be logged in, they have to have this tech, maybe this cookie or this cache or whatever. Um, and then what are the dependencies? And then we just kind of work through and go, "Okay, what are the things that where a user will potentially see problems based on this information?" And we went through that. So that's how I kind of approach what to instrument because then once you kind of know, "Okay, so we know that user logs in, so they know they have to do that. This, they have to interact with maybe, uh, let's hypothetically say they interact with a database, they interact with an authorization service, maybe they interact with something in the backend for something else, uh, another database." And so all these queries are checked in there. Now that all those things, all those functions are like talking to these different things, if they don't work means they can't log in. Therefore we have key spots what they should be mostly looking at to make sure there are always going to be times where people miss things. This is why we have the SLOs. This is why we have to have this constant part of our cycle to go back and make sure that those things are identified. So, you know, we're having problems with our servers, but we don't know why. That means maybe you're not instrumenting. So what that means you have to go revisit some of the things you did because maybe you did some changes that no longer uses this kind of database, uses maybe a cache service or uses a Redis cluster instead of these things. And so does your code or your user journey reflect that change? And does your instrumentation reflect that change? And if not, let's make sure they do. So, but it always boils down to the user journey and what the user is experiencing. If as long as we're capturing that, you're like 80% of the way, in my opinion, 80% of the way there. Uh, because you at least have a general idea, but you have to know, you have to have that conversation.

**Speaker 1:** awesome, thank you. Okay, next question, the Q. Um, in an org that has adopted distributed tracing, how do you make individual teams leave behind their outdated practices based on logs? How do you show value?

**Dan:** that was a spicy question. That's a spicy one. Y slap them around and tell them what? No, you do not. That's not the way to do things. You have to be very, exactly. That's number one, do not call her baby ugly. Um, that's a tough one because you really, really have to be patient and there sometimes you do have to say, "Okay, listen, this has been dictated by the AL ops that we're doing this," and you try to offer them as much help as possible, but you have to, you got to find out too why they're reticent about switching over. Uh, having these conversations is important because then you understand what, how they're looking at things and then you can restructure your discussions around that. Uh, so, oh I like using logging because you know, I get to see immediately when I'm doing my development, I can see it point out on the screen, I know where things are broken so I can go back and fix and blum blum blum and that's so you know that that's why they love logging as an example. So how you have to then help them bridge that gap and say, "Okay, well this is how you would do it with tracing and this is why you would probably want to do it because then with distributed tracing you're not just getting the value out of your service but you're seeing how service A interacted with your service, service B and how service B is going to interact with service C and how all that kind of works together. So this is how, why you want to have these pieces in the puzzle so you can get that full picture." But it takes time, it takes patience, it takes, and again, going back to which one of the things we talked about before, restructuring how you approach the, their, their, the, uh, the challenge of switching over. It's not just repeating yourself the same words over and over again because eventually people are just going to tune you out. Like at my work, I think they tune me out half the time.

**Speaker 1:** okay, uh final question we can squeeze it in hopefully in two minutes. Um, really love what you said about the tooling, uh, is the last thing you should think about and that you should lead with the question what is most important to your organization? What are some of the questions you ask to understand what is most important to someone's organization?

**Dan:** it's always about the user in my opinion. It's always about the user. Um, because first because you're looking at if the user is not happy and you can't measure that or you don't know that, then you're in trouble, right? Uh, and I always use streaming services like video streaming services as an example because, and we, everybody has tuned into one of many of these streaming services we have now today. So if you pull up a movie, you push play, it gets all pixelated for the first three seconds and then it clears up after that. Do you care? No. So if a person comes by, but does the streaming service know that? They probably are measuring that because they want to make sure that that doesn't become a bigger issue because they know that runway of where you're okay to where you're not okay and you're going to get frustrated and go, "I'm gonna go switch to this now because they have better quality." And so it's not, it's always about what the user, how we are we measuring the user experience? Do we know what the user experience is like? Do we know what frustrates them? And if we don't, how do we get there? And that's how we get, so that is usually with the tooling that we have available today, today with the SDKs and other things and the philosophies we have. And eventually then you can go because you know what, the tools comes out all the time. There's so many tools out there that the landscape has gotten bigger and bigger as people become more and more involved. But doesn't message mean they're all good? And you know, but you have to find the one that kind of works for you at the time. And this is why we like open Telemetry is because you can switch from one to the other to the other without with very minimal change. And that's why I like about getting people on open Telemetry. But when it comes to the tooling, that should be your last thing to think about. Don't build your solution around this tool in your spot. Build your solution about what your users are thinking and that's how I think a lot of people that resonates when you start talking in that light, especially with the higher ups anyway.

**Speaker 1:** so I think we got them all in.

**Dan:** yeah, absolutely.

**Speaker 1:** well, we got through all the questions. Yay! We filled up this hour quite nicely. So thank you so much, Dan, for joining us for hotel Q&A. Stay tuned for our next hotel Q&A and or hotel in practice. And again, thank you, Dan, for joining today. I really appreciate it.

**Dan:** oh yeah, that you know, it's my pleasure. Thank you for having me. Um, you know how much I love talking about these things. So, you know, I, I think I actually did not go down a rabbit hole today. I think I was pretty good.

**Speaker 1:** it's perfect. It was great. Thank you again.

**Dan:** all right. Thank you everyone. Thank you for showing up. See you.

## Raw YouTube Transcript

welcome everyone to otel Q&A and thanks Dan for joining us for otel Q&A especially on like we got you on such short notice um I'm glad we're able to squeeze you in for October um especially me too cucon is coming up in November so for folks who are going um a few of us will be there uh re Dan and I will be there from theel and user Sig so um be busy yeah it's going to be busy when is where is that happening by the way it's in Salt Lake City Salt Lake City nice I've never been there myself personally yeah me neither me neither oh oh so this be an experience for you too yeah yeah looking exciting exciting um start asking you questions I realize I'm the one here is supposed to be with the breaks folks it's gonna be a fun one oh yeah not sure about informative but it will be fun well here we go okay let's start uh let's start with the question so Dan why don't you introduce yourself and tell us what you do hello I am Dan Ravenstone I am a staff engineer at Top Hat uh I've been there for guess two and a half years maybe no a little bit longer than that come may I think it'll be three um a little of my background um I have been doing monitoring observability for the bulk of my technical career uh I started back in um where I actually this and I usually refer to as like where the bug bit the monitoring bug bit me was when I was at a company called affiliat which was at Young in 401 area in Toronto uh affiliat managed the info.org regist domain registry um in the back end of it uh for back in the day and I started there in texport but then I one day opened up my big mouth and said Hey listen I'm tired of our customers calling us up and telling us our stuff is down can we fix this and so they said well then you ask for it you get it so I went out and so I that's how I started learning about moning tools and that's back in the day of Big Brother nagios cacti all those fun things and um and I just loved it this kind of like I think one one of the things I liked about monitoring in those days was that like I like looking for patterns I like detective novels I like the all you all the fun things and what this do is like kind of allow for somebody to kind of like be in the front lines in the operations and dive in and find problems and report them back and then like the better more the better your monitoring tools were the more likely you were able to follow and find the problems a lot sooner before they become actually customer impact and I was always been my kind of like push when I did this and of course as things progressed and modern and changed uh we got into the wonderful world of observability and a whole different way of looking at it and I fell in love with with that obviously as well and tried to grow with the times and see how I you know how being proactive is a lot better than being reactive which is kind of like you would think is an obvious thing but it takes a lot of people time to kind of get their heads wrapped around um so yeah that is kind of uh how I got to the how I got to this part of the world world and a little bit about me there's tons of other things but we don't want to go into those because then we'll get lost in the weeds and we'll never get out and we'll be lost forever sounds good well thanks um and you know like you you've been in in in this for a while like when it's started out with monitoring evolved into observability what what kinds of like big changes have you seen um in in the course of that time uh let me start off the things I haven't I haven't seen change which is still a problem alert okay alerting still has not changed surprisingly um but what has changed and I think this is one of the things that um we're trying to get to I I so I focus on alerting because I was by myself time to think what has changed so now that you know my secret I just thought myself a couple more minutes so but what has changed though in observability and in the open Telemetry especially too is it allowed for us to actually look at our our services a lot differently where moning was kind of a very largely reactive kind of thing and we only had the symptoms of what was going on so if everybody has ever used nagios is well aware of the sort of out of thebox HST monitoring sort of plugin you usually incl include which is like your CPU your memory your disc SP and these things have been carried throughout time in you know to con be indicators of a problem but that's usually not quite the case anymore is it um especially way when we have kubernetes and containers and other tools now that these things shouldn't really play a role in us so it's like more like so what else is impacting the service and we never really had those that ability to look at it until we started sort of aggregating our logs and even when we aggregated our logs it was still strangely problematic um because you know we had guidelines on how to do structure logging and that kind of thing but still people would do their own thing and things were never and so even with all the good documentation in sort of best practice around it we still logs were still problematic and again you still have to still collect a bunch of data before you can actually understand oh wait a minute this is a problem before actually kind of knowing beforehand what observ abilia has done has taught us is how to sort of like set things up in a more of a a way that we're looking at it as an overall user experience and we don't get sort of sucked into minutia sometimes and there's because there's a lot of red herrings in operations when you're especially when you're in the middle of incident you're looking at all this all this data and you're like what is the actual problem we have high CPU over here and we have slow load times over here we have latency here we have some errors here but which one of these is the the smoking I would not I don't want to use a gun but for like better word Smoking Gun right and I see that that with observability has changed how we look at that however that's for this probably this group a lot of people are familiar with it there's a whole world of of Engineers out there who are still having are still trying to get their heads wrapped around this concept and this is why I'm clown and glad we have these conversations and I try to get those people this is like this is targeted towards them of why this is so important why you need to start looking at this because there's a number of good things one you know what the user is experiencing and that's all that matters if you know what the user is experiencing that means you can translate that to how much you're making like very very like very it's it's a cost thing and it's a money thing if you know if your users are happy that means you're making money because they're be spending money because they're going to keep using your service if they're not happy they're not going to be using your service and they're going to walk away and use something else or just not use it that often and try to avoid as much as possible that means you're not gonna be making the money anyway um and I'm gonna get in the weed so I'll you're gonna hear me say that a lot because I just do so you Adrian it's all up to you to rope me back in like you gota don't worries don't worries but I I do wanna um you said something really interesting um in in terms of translating it um in terms of money because you know like observability adoption and maybe I'm getting ahead of myself here but I I like way you said um in terms of observability adoption like it's as much a a top down thing as a bottom up thing and one way to speak to the benefits of observability is in dollars and cense and Executives speak in dollars and cense so um proving the the worth of of having an observable system I think becomes really really important and and being able to use that language um with Executives to be able to convince them I think is very valuable so and and that's what I've been trying to focus more on and like there are like there's quantif quantifiable yeah quantifiable reasons why to look at this we can do that why obsil excuse me uh provides value toward us the end of the day but I think we we' we're still I think we're still getting our we're still trying to get that figured out I feel like that um anyway you know what before we jump in let's let's keep moving forward because I I I could go into a whole other area and I don't know I don't want to sort of suck a lot of time in there but it is one of those things I think that does have is worth having conversations not with just those in the community but also talk like I mean if you're you know out there find out what your what the OB ability sort of platform or the concepts are your where you work what what are the drivers why why aren't you doing it why aren't your why aren't you using these things and ask those questions and then like you know that maybe we could that could help and feed that back to community because that could help us build out the reasons why you should be doing this because it does make better sense eventually um but yeah that's a sort of a general call to anyone who's out there who needs who wants to sort of get going on this and need help yeah I completely agree and I think this uh this takes us to our next question um which is uh you know like you've you've done this for a while now like first the monitoring space now moving into the observability space what do you think are some of the main challenges that most organizations are facing these days when it comes to observability uh there's a lot of challenges and I've been trying to get my head wrapped around some of this stuff one is communication I think so some of it I think it's mostly cultural really if you think about like I mean there's not a technical reason why people shouldn't go to this other than resourcing in time uh so and and I mean this might be jumping ahead of a little bit but where I am right now we we are using a current vendor uh which will remain nameless but we use a current vendor right now for our monitor oability purposes but we're getting away from them because of the library we're using is is is a vendor specific and that means that we're basically when it comes to doing any kind of of telemetry it has to use that that particular library and their's only getting and that's that that's one challenge because if you think about like okay so you're using this one vendor it's cost you x amount per month or per year it's exuberant however to get translate or get off of that one and go into another one where you have more options like with open Telemetry takes time it takes it's not like an like anybody who's ever gone on any kind of migration from one tool to another knows how long it takes and if you're and you have to have a few things in place to actually say why you should do this so one is like of course obviously the cost right well but you know it's hard for some Executives to say well we're spending this but it'll cost us this much to get over here and then we'll see the cost value so that there's this huge challenge of just trying to shift things over spend the time to do it and then do it properly and that's a whole other challenge is say okay yeah doing open Telemetry is one thing but doing it prop L like every other software or any other tool that you have out there you have to still use it for the right reasons and do it properly if you don't you'll still run into issues and whatever you're trying to sort of get away from over here will still might come up in a different way over here so there's those that's that's one of the challenges is just getting that just doing the whole Shi over uh I think that's a where a lot of people and they don't know how to do it and so and what happens too is then they do a shift they try it out and then they get burned for because you know they don't know how to do sampling or something like that and then they feed it off to some tool like let's say a tool that you get charged by event volume you're going to even though they're using otel they still became like oh I I saw one company who did that they start shifting you using otel but they started setting all this volume all this data to it they didn't do it right they were having like a 100,000 spans per Trace which is a huge amount and they weren't really getting a lot of value and their their costs are soaring and so it's like well you know then they kind of just drop and they well we'll figure it out another way we'll just use some other tools to figure out when there's problems with our service which is sort of like a defe kind of attitude but it does happen because they don't you know because there's a lot of work involved doing this is more than just a simple lift and shift it's a lot more involved to it um trying to think of other CH like those that to me is like some of one of the big some of the bigger challenges and I think a lot of people could probably emphasize with that one as well um there are others obviously but I mean I don't we don't have a whole lot of time and we could probably go on just you and I alone I bet you we both have experiences on some of those other challenges we have to experience but I mean uh I like to see like like how do I get how do we get past that is kind of like where I want to Target how do we get past these challenges and move forward which I don't have a proper answer to but I will want to talk about it you know that you you touched upon something that um I think is worth mentioning as well which is um moving away from like vendor libraries to open Telemetry um because you know even even though I I think one of the one of the wonderful things about open Telemetry is that it's vendor neutral um and so it makes it once you've moved all of your instrumentation to that super easy to switch vendors I would say Rel relatively easy right because they're still like yeah like at least on from the instrumentation standpoint it makes it super easy um but but it the challenge though is convincing folks to move away from those vendor libraries and I think that becomes um that becomes a bit of a sticking point because it's like they you need to move away from them because maybe that vendor is too expensive and you don't want to go with them and you want to try something fresh but there's so much time and effort and learning curve all that stuff associated with reinstrumentation and that what are your thoughts around that well and that's a very very powerful Point you're making because first off with vendor vendor libraries you get in the habit of doing things a certain way right and so again I do not want to there's no like but the thing is so some V some vendors may do things one way but it's not part of the industry standard or the best practice we see across the industry so they may like name things as certain way or do things a certain way and people get used to that so not only is it just just shifting a library over if it was just a matter of shifting a library to one or the other that that's in itself is a challenge but there's also all the behavior that gets attributed to using that older library that some for some things you got to go well listen you don't need to look at it like that anymore you need to look at it like this now and so it's not just a a technical shift it's also a cultural shift and how they even like Envision how their service is working with the new library with using open Telemetry um because open Telemetry is current it's trying to it's keep it's adding new things as we speak as every day uh it's you know it's you know we're seeing a lot more love now for mobile which is wonderful I'm so excited to see some of that because I feel that's largely been ignored for a long time but we're seeing these things getting pulled together and we're having this and it's kind of like keep following these sort of patterns and following these this this this these um standards and so make forces the developers to think a certain way there a lot of them are not used to that yet a lot of them still want to develop and I realizeed one of the things I noticed too was for developers perspective I don't I Adrian you you do you're a coder by Nature right like that's kind of yeah yeah that's when you yeah that's your back so it's not mine but I know that when I did do some coding though I would always print a line at certain parts in my code this okay where am I at so you're even like logging was is kind of part of your local development experience totally right now we're telling them don't worry so much about logging use instrumentation well you still have to have a mechanism for them to actually print that to screen and so it's just changing how they even develop in the beginning um and that and that's I think where we're facing some challenges there is that it's not just a it's it's um it's technical it's cultural it's it's about even how what you've been taught in the universities and how you even taught how to develop is changing a little bit and not everybody's quick to embrace change not and you know especially when you've got your CEO or your or your or your um your features team or product team saying we need this out now well you know you're not going to worry about trying to learn a new way of doing things and it's you know oh well now how my service is working from open Telemetry perspective it's like well I just need to get this out so I just slap these log lines in boom gone it passed you know and we're going now it's in production and we move on right so there's that that it's that kind of like also encouraging and supporting our developers to have the time to learn this and work with it so they can actually use it to their benefit and perhaps be able to do better code before even hits staging or development and then and then once it's a production you know how it should be based on all your stuff but that's it's getting that linking those people together getting them to work together in that thinking that's that's a that's a huge challenge yeah yeah absolutely and and getting getting them used to as you said like the nuances of you know instrumenting with open Telemetry which open Telemetry wants to standardize how you're doing like your your main signals in a certain way right which as you said maybe different from how you've been used to doing it whether it's like through your print statements or or vendor libraries or whatever it it's kind of like so I back in the day um this might be pre a lot of folks I don't know but I used to work with a lot with SNMP and what I liked about SNMP was it was it was basically a protocol that was agreed upon by the community and it was very basic but even then I saw this with loogy too but even with LMP I would still come across mibs I would still acoss across libraries that were not properly done so it required me even like because I knew it I would have to go in and kind of redo maybe a MIB to so I can translate the oid properly so when it goes into our or create a MIB because it wasn't available it just it just had these voids are available from an interface or something and you just have to guess what they were um even with standards in place like that something has been in our sort of part of our our industry for so long no longer really being used anymore um well actually still is by network devices but that's not hopefully we don't have to worry about that right now um is is that you know even with those things in place it still takes time for people that even then there's still mistakes and things happen so with a new thing like open Telemetry and and being setting standards it's taking a long time for people to understand how to use it to do it to set it up properly and we get their most value out of that instrumentation the most value out of using that from the beginning of the of the development cycle yeah absolutely um I do want to switch gears a little bit and talk um a little bit about specifically um the challenges that you're facing currently in in your role um when it comes to observability oh I'm really not my own mental health problems okay that's different um yeah so my my challenges have pretty much always been I feel like it's always been the same it's communication and not because I'm my challenge for me so I've been in this business for a while so what happens sometimes is as I make the mistake or the or make the assumption that folks know what I'm talking about and I have to be careful sometimes like for like I can have these conversations with many people in the same community is like within the monitoring obility community and we could go on for hours about little nuances of something whatever may people logging or or tracing anything like that and we can go on and talk about that as ad nauseum however when you when you're talking to other people and you you talk about basic things like um like the red method so rate of Errors um rate of requests durational requests which is a very basic kind of concept with the monitoring World um and which is helped kind of like we see a lot in even with notability too to a certain degree uh people don't understand that and you make the assumption that they actually kind of look at things in that light but they don't they still look at it from the old school and then like honestly like i' at some places I've been it's like they still see CPU as a indicator of a performance problem but it shouldn't be in my opinion it should because that's not CPU can is is cheap you can get more so that should be a scaling problem make sure you just you're scaling properly you should be good however I make the Assumption people know what I'm talking about and that's one thing is I got to con so I have to constantly be patient and communicate the same message over and over again to make sure they understand because people even if I tell them two or three times they still don't clue in I had one conversation with one engineer for about a month um and it took a month for them to actually understand what I was driving at and this is like because they were in different time zones so that's why it took a little bit longer because and so but it took a while before they actually oh that's what you meant and as soon as they were able to make that change everything was working better and like oh now I see where you're driving at I now see the point you're trying to make and it's just you have to be diligent you have to be patient you have to communicate and you have to change how you communicate sometimes too like you just can't say the same words over you have to rethink okay you're not getting what I'm saying what will help you understand and and then try to reward it so people understand um ad you met my friend Damian who who I work with he's actually the same place as I am but Damian who also works with me on a lot of stuff and we he's more of a he has a different background but he sometimes steps in and sort of translate what I'm saying because sometimes it helps to have a different because he puts a different sped on and all oh now I get it so that's those are some of the biggest things I find um to today are still the biggest sort of people get it it's just a matter of once but just getting them just lead you leave the horse to water type thing right but I am it's just leading them there once they're there they eventually do start to drink because they they kind of get oh I see this is good for me I will do it now yeah for sure but getting there there is sometimes you gotta kind of be patient you gota pause for a while and say okay yeah you want to look at the the pretty flowers for a moment that's cool you watch TR some grass great okay finally I'm equating developers and Engineers to horses so hopefully nobody will take offense to that but in the nicest possible way honestly I mean know it's not like I'm trying Hur I think you like you such a good point though with with like experimenting with different ways of of doing the messaging because as you said like you can't just keep repeating the same thing and then hoping that people are going to get it um you have to you have to come up with with different ways of saying it and and another thing that you and I have discussed before also is you can't tell them that they have an ugly baby you can't say your thing sucks my way is the better deal because people will get very offended rightfully so so you have to be very tactful in your messaging exactly no you're and that's right because they'll just take offense and then they'll walk and then everything you say afterwards it falls on deaf ears so you don't want to attack them you've got to draw them out and we we wear a lot of different hats in this role when it comes to observability I feel it's it's not just about knowing how to do it technically it's about how to assisting folks to see how they can learn from it and grow with it too and that is a huge challenge uh I that's why I've done like a lot of my videos I've done too like uh not too many of them been exposed publicly I've done them internally with at work but I will put on different characters like I would honestly like do different characters to help people understand just so they have something so it's not just another boring oh here we go get told off about monitoring obsil yes we know Dan logs are not monitoring okay like I but I try to build it up so they they go they it kind resonates a little bit because they're more engaged because oh Dan's put on a funny wig and he's making a funny voice but that point made actually does make sense what so then you know it starts to click eventually I hope maybe I'm wrong maybe they just look at me and laugh and just walk away nothing nothing six I like that though because you're you're you're also like putting on different personas right and I think people also res like different personas resonate with with different people they have to at the end of the day it's got to be how how does this benefit me right because otherwise if they don't see where they fit into all this then it's falling on exactly how does it benefit me that's that that's a good valid point and and it's getting them to see that there is a benefit to them it's just like all right well you just have to sort of showcase this several different times um I know somewhere along the way I heard somebody say that how when you're me creating a message for your organization you got to do it like seven times in like different formats to get it so people all kind of get it and think about it and understand it and I think that's got some truth to it to be honest because even to this day I'm still telling people by the way I've you know this is the reason why we're doing this and you kind of go through that process so um yeah no it's totally totally true yeah and I I think there's also the seeing is believing sort of thing to it too which is like yeah okay understand how like open Telemetry Works in theory right and but it's not until you put it in practice and instrument your code for the first time and even if it's like some you know I added a trace I added a log whatever then you see it go through the collector and Inter your observability back in and you're like what my life is transformed just even and you go like now what I do yeah right it's like you crave more right like give them taste it and it proofers in the pudding as the saying goes is a very very true too because I have seen a lot of success once you get the ball rolling they start to implement something and I and I I've seen it over the years in different tooling um I think only once have I seen it not work but that's only because I that was more my arrogance than anything else so I've dropped arrogance at the door after that and never did it again because I want to use a specific tool for the job and realize that that's not the way to approach it um and that's the other thing too as Engineers As Leaders technical leaders we should never push a tool in my opinion I think we should always look at what the organization needs what's most important thing to most value to the organization and then making sure we have the right practices in place the right standards in place and that will event then we can figure out what tooling we'll use after that tooling should be always the last thing you should decide on how you want to visualize or work with your data that's my just my two cents I don't know how the people that might cause problems but who knows now here here's a question for you along the lines of tooling because we see this a lot in in in Tech where you've got like a team they start using a tool but then the the corporate standard for tooling comes out how do you um how do you reconcile you know the the teams that are off doing their their own thing with the the corporate standard because sometimes it's it's really hard to get folks unstuck from that you you can you know try to force them but some folks are very adamant and and passionate about about the tooling that they use right they are and you you know what you got to I honestly personally that when it comes to that thing is like okay try to get them to as long as they're following best practices following industry standards then you can only push so far you know like you could say Hey listen you know stop using this particular tool because it's archaic it doesn't give you the the same value for what you have but you know and then they you're G to have push back I mean it doesn't matter where you are even when everybody's on board and they're you're going to get push back because people want to do things their own way right there's always that kind of like I I know better I know what I'm doing and you have to be delicate and you have to be patient and you have to be tactful um so it's better to just I would say just try to encourage them to use best practice and standards but don't try to push them off the tool if that's I mean if somebody if somebody higher up wants to make that decision and call them out on it let them do it but you want to I think it's more important to have the standards in place because if you can kind of tie everything together and you have a proper process flow and everybody can still can't get to the root of the cause of the issue then that's better than just saying you need to use x for your stuff moving forward tough NIS because that that kind of thing doesn't work either in my opinion yeah that makes a lot of sense especially like you know I I'd almost prefer to like okay you use whatever observability back end you want as long as we're all instrumenting with open Telemetry I feel like I feel like that is that is main thing yeah that is the bar exactly L now when you um when you joined your organization um were like was Top Hat doing observability at the time what was what was the landscape like at the time so at that time so we were at so we're we were not but we were um so this is one of those things where I kind of like I had to be very patient but they were like so when I first joined we were using a vender um uh everything was there was instrumentation in place there was logging in place there were um well I should say there was traces in place and there was logging in place they were linked but they weren't um it was done the initially was rolled out about five year ago years ago but no other real work was done just kind of building on what they already knew moving forward I was asked to come on and actually bring to take the company off the vendor and migrate to open Telemetry so that was two and a half years ago and we are almost there like this is uh I think uh we have one more service to migrate over to open telemeter this month and then we're done so it's taken to an almost about two and a half years to get to kind of get to this point amazing is take yeah uh it was hard obviously but it was also it was challenging um had to be a lot of patience involved um and what I did though in the beginning was actually kind of go back to remember the uh when we were at monorama together I did a talk on alerting and I did that talk because I kind of was that was a precursor to where I really wanted to go which is actually getting into slos right but that was sort of like here here's how you can first sort of understand your landscape when it comes to alerts and why so you've got and if you can categorize them in a certain way and realize a bulk of them were a lot of noise and doesn't have any value help people start seeing those things in that kind of light and then start showing the value of using service level objectives and how that can actually help with end user Journeys help actually bring the user experience home to the developer so they understand okay I don't need to alert on you know when the low balancer is hitting 5xx errors when I should be alerting on whether or not the service is being impacted by that and how it's being impacted and then yeah I mean those 5xr are important but where are they important how is that impacting the user experience if it's for some feature that we don't really care about yet we don't care about the 5xx because we don't need or we don't need to be alerted about that in the middle of the night but there's so that this was how we were trying to get that process going and then they start doing the migration okay now that we know how our Serv should look now we can do our instrumentation on that uh moving forward so we've we've done started off a lot of instrumentation but now we have those those user Journeys to reflect upon and go okay we're missing this this function in our Auto instrumentation let's make sure that's part of that because that impacts the users's experience so yeah so it's been a while uh but we are getting closer and closer to getting over over top and Telemetry we've set a lot of service level objectives two of them have already helped out like alerted us like two days before an incident actually happened so we're already seeing some value in this so it does again proof this is in the pudding right and we're already seeing some of that and we're you know it's it's great a lot of churn but in a lot of like turmoil a lot of folks were like oh no we're losing this but they're seeing that they're getting more value out of how we're doing things moving forward and they're able to query things better because they're able to have more richer data available at their fingertips so now in terms of uh vendor did you end up switching vendors from the one that you were using when you first came in or so we had the one vendor we were using we're still on that same vendor uh but as of the end of this month we're no longer going to be using that vendor we'll be on another vendor moving forward because we're pulling all the their vendors specific sdks out and we're just putting an open Telemetry in and we're doing that replacement so it's kind of like now we're getting to the point where the last in that in those open Telemetry sdks are pointing to a different vendor for our visualization of data cool awesome um all right moving on um yeah so the next question I had is um you know now now that you've gone through the exercise of um you know in of having of doing the instrumentation so actually around instrumentation is there um what what's the instrumentation culture like at Top Hat um is it because you know one one thing that I've spoken about in in the past is like it really instrumentation has to be the responsibility of the developers do you think that there is that attitude of developers instrumenting their own code where you're at not as of yet um and there's a the reason why and this is nothing against anybody so I hope anybody who's at top at listening don't take offense I love you all um no but it it's because we were kind of like once we started getting going on this whole transition we had a very short period of time so there's been a few instances where staff engineers and principal Engineers have stepped in and done some of the work laid laid the groundwork for the other team and they kind of just went flipped over things and did things the most effort they put in was actually in the user Journeys themselves so the auto instrumentation has been more largely being kind of like we've we moved this put this in and they've G of look done it so they haven't really it hasn't PARTA of been ingrained in part of their cultural thinking yet it's still largely kind of like a checkbox for them to sort of okay we've gotten we've we've moved migrated to Hotel job done move on to something else and little do they know that there's there's a lot more work involved but don't don't scare them off yet because it will help them but it is something that has to be I think and what I we've tried to do is try to make it so as easy as possible less not as impacting as far as their day-to-day life but try to slowly get them to start looking at things and so going over things and saying and and using the tooling that we have available saying here we know you did it in this particular tool this is how you should do it in this tool and providing that data and then doing training on that too so trying so they're slowly it's slowly becoming part but I would not say it's 100 like it's far from 100% yet it's still there's a lot of work to still to be done there just so that they start to in they all understand why it's it's part of the value right um it's only like I think I right now it's still a handful and I could be wrong maybe I might get in from snack later I'll get you know told no we all love it who knows in terms of instrumentation like what uh what languages um are part of like your your application landscape um and mostly python oh okay okay so mostly so we're what's that oh I was gonna say and you alluded to some Auto instrumentation there yeah so we did Auto instrumentation in our python code um and so we and we use a couple different Frameworks but I think we're just trying to work towards getting to one framework um and then we have and then yes it's fortunately there's not so when it comes to so we can you can actually almost kind of get away with like doing a rapper sort of library for people and have them just use that and then yeah to get and that's kind of what we did we have this little wrapper Library set up that uses oel with the you know all the different things that they need to pull in there and people just kind of just dumped it in and did Auto instrumentation the way they went so they kind of like so it's kind of like all done for them and they just kind of include this in a way they go um but I think as we progress it's going to be like okay let's start Target this is my next stage will be talking to first off the PMS talking to them okay this is what we have so far these are the gaps we have in our our Telemetry data these are the es we have set so far let's decide on how that should look because your team needs to own this your team needs to act on this so when your air budget gets exhausted your team needs to take that on in next Sprint and that's one of and like so that and again the technical side is not that difficult it's the the cultural change process change requires a lot of effort right because now you have to get people to say okay well instead of when our we burn through our error budget we need to take deal with this the next Sprint well that's usually well that are this future that we need to push out which which one do we do and so but they have to that that's what I said when I first started this is that you have to agree upon this process you have to think about you are making a contract with the company to say yeah we will act on these slos because they will impact the user Journey they will impact our users and that's what's most important to us today so um for those who are not aware topad is an educational platform so what our our our our you our um our users are teachers or professors and students so imagine you know if they're taking an exam and then everything goes sideways that's going to be really I mean anybody who has kids or has been to University knows that that could be a huge frustrating Factor when you're not you know if that's not working properly so yeah so it's important that um oh I did get a message somebody did tell me we all love it thanks Mark yeah so um yeah uh now I just kind completely thrown off there uh what was I talking about ADHD you were talking about the slos and and how frustrating it is uh if if the consumers of your platform um if it's not working as they expected yeah yeah so and that's this kind of fundamental shift is going to slos and helping them see that and get through that um yeah because I think now I just went off the rails I so but that's all good cool I have one more question that I wanted to ask and then we actually do have some questions from uh from folks see that our on our board which I'm very excited about um final final question um I guess there the two questions I guess um are you um if if are you using kubernetes and if so are you using the otel operator um to manage your collectors and to manage your auto instrumentation no no yeah so we um I'm glad we're not um just because and this is not because I think that kubernetes has its place in like all tools has its place and I think for what we're doing today it would not make sense it would add a layer of complexity that would completely destroy our service so glad we're not using KU I don't think it not saying that there's anything wrong with kubernetes I just think that you if you're going to run a kubernetes environment you need to know it yeah yeah AB right and so if you don't know what you're doing with kubernetes you can really really mess up your service and and I've been at places where we only had one or two people who knew kubernetes and we had a number of problems always with our clusters so I'm when we're ready though I think we'll probably make probably make that push but right we're now we're just in containers so it's all just through yeah just containers but not a kubernetes gotcha gotcha so you're you're using like another container Management Service like a cloud provided one kind of thing a cloud provider container service yes gotcha gotcha you're trying to avoid all names of vendors right now yeah yeah yeah let's be agnostic and and H final question before we get to the questions um collectors what's your collector landscape like so we're using a combination of alel collector um as a sar and then to a primary one that Aggregates it and then for our traces we use a with one company that we're work where where we're moving towards uh they have their own collector that we send our traces to that we can do additional sampling rules on and send off so um yeah yeah so we um so right now because and I I really really love the fact that you could choose you could do send from code itself but I love the hotel collector because simply because you have these processors you have these exporters and you can send to different places as you see fit I've had conversations with our data team saying Hey listen if you want stuff otel is collecting a lot of this right and we don't have to send all of it to our for production like as far as what we're looking at as far as like would give us an idea of the user experience but there may be things you are collecting that from R tell that we can send to an S3 bucket that you can pull in later for whatever you need however um you know we have we're still haven't gotten to that point but that's the beauty of the hotel collectors that you can kind of like just point to different things and I love that because then it there's no code change so if we decide we no longer want to use this or we want to add loging or we want to add set get uh we start using we don't use Prometheus now but let's say we start using Prometheus as a open to open um Source tool um as an example though uh you would we can easily just start getting that information right Off the Mark so it I I just if anything try not to use the the sampling and the old push from this code itself use a collector because I love the fact that because it makes life so much easier then you can just if you have to do little tiny changes of tweaks to your your sampling or whatever you don't have to go push it out to like the production environment again it's just a small change on your collectors themselves which makes life so much easier that's just how I think of it absolutely and and you're using all three like major signals traces logs metrics right now like for otel or mostly traces right now um so there was a decision to not do so metrics we still are capturing the infrastructure metrics from the cloud and the logs are primarily staying in our cloud provider so all of our services are are just logging by default from container to the logging um in our cloud provider and they are staying there for the time being um because that was another thing to to start tackling and I didn't I just I felt like that'd be too much for folks as they're transitioning from the one vendor to open Telemetry for them to like get their heads wrapped around would be like oh well by way you also need to have structured logs too and this is how you should do it no this is just gonna it'll make more confusion and so I figured just just focusing on one and then adding things as they go along as they as they prove useful so yeah that's how we kind of approach that whole process makes sense baby steps exactly cool um okay so we're going to turn to the board and if you want you can take a peek at the questions they're on the on the chat window there's a link to the board um take a look so um let's have a look at the questions so the first one is what is your opinion on Telemetry as a uh on Telemetry quality as a way to reduce spend is more data always better observability great question awesome question and that's a tough one um I don't think I can I hope I can answer that in my opinion um it's Bell quality is you need to actually because really with the end of because you can have a bunch of things going through but if you're sort of so let's just take a a standard Trace right if you have um a bunch of spans in there that are doing sort of some things that you really know it's just back and forth and you really like there's I've noticed some Tes where there it there's um I forget what it's called and I I I I wish I had a sample the code before it's been like a two weeks ago one of the one of our Engineers cleaned this up and what he did though is he looked and realized we're making these calls in the code that we're we're adding span events to and we don't need to because so you're looking at the Quality there and then you're not you're seeing what's invaluable and what's not so more data doesn't always mean better Telemetry quality data means better Telemetry so in my opinion you've got to look at what you're getting making sure does that reflect the user experience if it doesn't reflect the user experience or is not exposing things where you would say oh you know like that's kind of like the whole the concept of the unknowns right like you're exposing those unknowns you're not able to see those things and then you're not the qual it doesn't matter how much you have if you're not seeing that stuff then it doesn't mean you're it's the quality of the data that's most important and that will help you reduce spend because then you know what's good you can sample that partially out and you're going to make sure that those that that the air prone things Bubble Up and become more apparent when you're looking at your data hopefully that answered the question yeah that was a really great answer um yeah and I I I agree with you it's it's really like what's the point of capturing in Telemetry for your system that's useless to you like that's just and I think that's um that's a pitfall that a lot of people run into also with auto instrumentation initially right because there's a lot of stuff coming in and and and fortunately now I think there's a way to actually switch off auto instrumentation for certain libraries so that you can you can really zero in on the stuff that's uh super meaningful to you so yay yay cool okay next question um how do you approach the what to instrument and how much to instrument questions with your teams so that kind of is it linked to the previous question to in my opinion um because what instrument is again goes back to this is so I went through the whole process of working with the the teams the development teams and the the product teams to understand what the user Journey looks like so I went through and said okay what is the so we did the user Journey we looked at their servers and I asked this like some did more than others uh but more or less it was like I asked him what are the typical interactions a user would have your service and write it down and then write in in so in plain language and then in plain language describe the tech what's happening in the background and then we also talk about the prerequisites of what people expect when they interact with that system so they have to be logged in they have to have this Tech maybe this cookie or this cash or whatever um and then what are the dependencies and then we just kind of work through and go okay what are the things that where a user will potentially see problems based on this information and we went through that so that's how I kind of approach what the instrument because then once you kind of know okay so we know that user logs in so they know they have to do that this they have to interact with maybe uh let's hypothetically say they interact with a database they interact with an authorization service maybe they interact with something in the back end for something else uh another database and so all these queries are checked in there now that all those things all those functions are like talking to these different things if they don't work means they can't log in therefore we have key spots what they should be mostly looking at to make sure there are always going to be times where people miss things this is why we have the the slos this is why we have to have this constant part of our cycle to go back and make Ure that those things are identified so you know we're having problems with our servers but we don't know why that means maybe you're not instrumenting so what that means you have to go revisit some of the things you did because maybe you did some changes that no longer uses this kind of database uses maybe a cash service or uses a redice cluster instead instead of these things and so does your code or your user Journey reflect that change and does your instrumentation reflect that change and if not let's make sure they do so but it always boils down to the user journey and what the user is experiencing if as long as we're capturing that you're like 80% of the way in my opinion 80% of the way there uh because you at least have a general idea but you have to know you have to have that conversation awesome thank you okay next question the Q um in an org that has adopted distributed tracing how do you make individ idual teams leave behind their outdated practices based on logs how do you show value that was a spicy question that's a spicy one y slap them around and tell them what no you do not that's not the way to do things you have to be very exactly that's number one do not call her baby ugly um that's a tough one because you you really really have to be patient and there sometimes you do have to say okay listen this has been dictated by the AL hups that were doing this and you try to offer them as much help as possible but you have to you got to find out too why they're reticent about switching over uh having these conversations is important because then you understand what how they're looking at things and then you can restructure your discussions around that uh so oh I like using logging because you know I get to see immediately when I'm doing my development I can see it point out on the screen I know where things are broken so I can go back and fix and Blum Blum Blum and that's so you know that that's why they love logging as an example so how you have to then help them bridge that Gap and say okay well this is how you would do it with tracing and this is why you would probably want to do it because then with distributed tracing you're not just getting the value out of your service but you're seeing how service a interacted with your service service B and how service B is going to interact with service C and how all that kind of works together so this is how why you want to have these pieces in the puzzle so you can get that full picture but it takes time it takes patience it takes and again going back to which one of the things we talked about before restructuring how you approach the their their the uh the challenge of switching over it's not just repeating yourself the same words over and over again because eventually people are just going to tune you out like at my work I think they T me out half the time okay uh final question we can squeeze it in hopefully in two minutes um really love what you said about the tooling uh is the last thing you should think about and that you should lead with the question what is most important to your organization what are some of the questions you ask to understand what is most important to someone's organization it's always about the user in my opinion it's always about the user um because first because you're looking at if the user is not happy and you can't measure that or you don't know that then you're in trouble right uh and I always use streaming services like video streaming services as an example because and we everybody has tuned into one of one of many of these streaming servers we have now today so if you pull up a movie you push play it gets all pixelated for the first three seconds and then it clears up after that do you care no so if a person comes by but does no the the streaming service know that they probably are measuring that because they want to make sure that that doesn't become a bigger issue because they know that Runway of where you're okay to where you're not okay and you're G to get frustrated and go I'm gonna go switch to this now because they have better quality and so it's not it's always about what the user how we are we measuring the user experience do we know what the user experience is like do we know what frustrates them and if we don't how do we get there and that's how we get so that is usually with the tooling that we have available today today with the sdks and other things and the and the philosophies we have and eventually then you can go because you know what the tools comes out all the time there's so many tools out there that the landscape has gotten bigger and bigger as people become more and more involved but doesn't message mean they're all good and you know but you have to find the one that kind of works for you at the time and this is why we like open Telemetry is because you can switch from one to the other to the other without with very minimal change and that's why I like about getting people on open Telemetry but when it comes to the tooling that should be your last thing to think about don't build your solution around this tool in your spot Build Your solution about what your users are thinking and that's how I think a lot of people that resonates when you start talking in that light especially with the higher ups anyway so I think we got them all in yeah absolutely well we got through all the questions yay we filled up this hour quite nicely so thank you so much Dan uh for joining us for hotel Q&A stay tuned for our next uh Hotel Q&A and or hotel in practice and again thank you Dan for joining today I really appreciate it oh yeah that you know it's my my pleasure thank you for having me um you know how much I love talking about these things so you know I I think I actually did not go down a rabbit hole today I think I was I pretty good it's perfect it was great thank you again all right thank you everyone thank you for showing up see you

