# OTel Q&amp;A Feat. Luiz Aoqui of HashiCorp

Published on 2023-12-18T23:04:41Z

## Description

Luiz Aoqui, developer on the HashiCorp Nomad team shares the ups and downs of attempting to instrument Nomad Core with ...

URL: https://www.youtube.com/watch?v=HRIx9gJtECU

## Summary

In this YouTube video, Luiz Aoqui, a developer at HashiCorp working on the Nomad project, discusses his experiences with implementing OpenTelemetry for distributed tracing in Nomad. The session is formatted as a Q&A, with Luiz sharing insights into the challenges and methodologies adopted during his attempts to enhance visibility into Nomad's operations. He explains Nomad's architecture, including its server-client structure and the scheduling process, before diving into his various approaches to integrating OpenTelemetry. Luiz elaborates on his journey from initially trying to instrument every aspect of Nomad (which proved overly complex) to focusing on core functionalities, allowing users to leverage built-in telemetry capabilities without extensive modifications. He highlights the importance of semantic conventions in creating meaningful traces and shares lessons learned about managing overhead and user experience challenges. The conversation emphasizes the value of real-world implementations of OpenTelemetry in legacy systems and provides practical insights for developers interested in telemetry.

# OpenTelemetry Q&A with Luiz Aoqui

Welcome everyone to the OpenTelemetry Q&A! I’m super excited to have Luiz Aoqui join me. He is a developer on HashiCorp Nomad. 

**Luiz:** Thank you! I’m really excited to be here. It’s always fun to talk OpenTelemetry. I haven't used it yet extensively, but I've dabbled a lot and will share more details about my adventures with OpenTelemetry.

### Introduction

Let me introduce myself first. As I mentioned, I'm Luiz, an engineer working at HashiCorp, specifically on the Nomad project. I've been working on Nomad for over four years now. At some point, I bumped into OpenTelemetry, particularly Distributed Tracing. It felt super interesting to me as a developer on Nomad because Nomad can be opaque to debug. Implementing and deploying it can be quite complex, so I thought it would be cool to incorporate some distributed tracing to better understand the internal workings.

After our releases, we usually have an internal hack week to cool down after the stressful moments. By the way, we just released Nomad 1.7, so if you want to try that out, it's available now! We’ll probably have a hack week soon, and maybe I’ll work on some extra OpenTelemetry integration then.

### Nomad Overview

Let’s start with a quick introduction to Nomad since understanding its internals is essential for discussing the tracing aspect. 

**What is Nomad?**
Nomad is a workload orchestrator, similar to Kubernetes. You provide it with a configuration file, and it orchestrates the deployment of containers and other workloads. 

Internally, Nomad has a simple architecture consisting of two main components: **servers** and **clients**.

- **Servers:** Typically, you have three to five servers that maintain the core state of the cluster. They communicate with each other to ensure state consistency and to maintain the jobs running in the cluster. 
- **Clients:** These are the machines that actually run your workloads. You can have anywhere from one to thousands of clients in a cluster.

### Scheduling and Operations

The servers are responsible for two main tasks:

1. **State Management:** They store the state of the cluster, similar to how etcd functions in the Kubernetes world.
2. **Scheduling:** The scheduling process occurs within the servers. When a job is submitted, the scheduler determines where to place the workload among the available clients.

Each client periodically queries the server for instructions on which allocations to run, similar to a pod in Kubernetes.

### OpenTelemetry Integration Attempts

Now, I want to dive into my attempts to integrate OpenTelemetry with Nomad. I'll go over three different approaches I took, each with its own challenges and lessons learned.

1. **First Attempt - Boil the Ocean Approach:** 
   My initial goal was to add distributed traces everywhere to provide comprehensive visibility. This approach resulted in excessive overhead due to too many spans and data being sent over the network. It became clear that monitoring at the functional level was not ideal for a monolithic application like Nomad.

2. **Second Attempt - User-Focused Instrumentation:** 
   In my second approach, I shifted my perspective to help users instrument their applications running on Nomad. I began by adding environment variables containing Nomad metadata that would automatically populate the spans generated by applications using OpenTelemetry. However, this approach also had limitations, primarily due to the static nature of environment variables.

3. **Final Attempt - Targeted Instrumentation:** 
   In my last attempt, I focused on instrumenting key aspects of Nomad without trying to capture every single function call. I created spans for significant events while utilizing semantic conventions to standardize the attributes across different traces. This approach allowed for better filtering and analysis of the traces related to specific jobs or allocations.

### Conclusion and Discussion

This journey through integrating OpenTelemetry with Nomad has been quite insightful. I learned that it’s essential not to overwhelm the system with too many spans and that sometimes less is more when it comes to tracing. 

**Q&A Session:**
Feel free to ask any questions as we go. 

**Audience Member:** What kind of information does each span provide?

**Luiz:** Each span can have different attributes depending on what’s being monitored. Some spans might have additional metadata like job IDs or eval IDs, while others might contain standard OpenTelemetry SDK data.

Thank you for joining this session! I hope you found it helpful and insightful. We’ll be posting a recording of this Q&A on the official OpenTelemetry YouTube channel. Keep an eye out for that!

Thank you, everyone, for attending!

## Raw YouTube Transcript

So welcome everyone to, Otel Q&A, and I am
super excited to have Luiz Aoqui join me. He is a developer on HashiCorp Nomad. Welcome. Thank you. Really excited to be here. always fun to talk OpenTelemetry. I mean, I haven't used it yet, but I
dabble a lot, and I'll share more details of my adventures with OpenTelemetry. I guess I'll introduce myself first. So as I mentioned, I'm Luiz, I'm
an engineer, working at HashiCorp specifically on the Nomad project. I've been working on Nomad
for like four plus years now. And at some point, I don't
remember exactly when, I bumped into Open Telemetry, Distributed
Tracing more specifically. And it felt super interesting to me
as, as a developer in Nomad because Nomad can be so opaque, to debug. But also to like implement and deploy
and use and multiple different ways. I was like, Oh, it might be cool to do
some distributed tracing there to get a sense of what's going on internally. and then like through the course of
several sort of like after our release, we usually do like an internal hack
week just to like, you know, cool down a little bit after that stressful moment. by the way, we just released Nomad 1. 7. So if you want to try that. It's out now, which means we'll
probably have a hack week soon and maybe I'll do some extra Open
Telemetry work, but that's a tangent. and yeah, like I've, I've tried
three different work streams with Open Telemetry Nomad and, you know,
I'll go into more details there. but each of them had sort of like
their own challenges and also nice things, but also bad things. And then sort of, I'll explain to those. but yeah, that's. That's it about me. I don't know if we should do like a quick
Nomad intro, because like a lot of the things, since like, since the idea of just
retracing is to do the internals, exposing the internals of Nomad, I think I kind of
need to explain a little bit about those. Okay, yeah, that's helpful. Yeah, why don't you go ahead. Cool. Let's start there. I was going to do like PowerPoint,
but like Adrienne told me that's more like a Q&A and I also
tend to overdo in PowerPoint. So if you have something you can
share it, if not, that's cool. We are, it's super chill. Yeah. I'll just draw a life, just because
feel free to like mute, ask questions. I don't have any specific agenda here,
but I'll, I'll start there and then maybe that will, but the general idea is like,
feel free to, to ask questions as I go. Don't need to wait on anything. cool. So first of all, what's Nomad? Nomad is a workload orchestrator. Think of like Kubernetes, you give
it a magic file, that file becomes containers or other things, right? But that's what the
orchestration aspect of this is. now internally, you know, it's important
to understand some details of Numid to understand what I'm going to show
a bit later, is that the general architecture is like, you have two
components, very simple, like server. And client, um, and these two
components have different roles. So the server, and you usually
have three or five servers, they maintain the core room, basically. So they sort of are always talking
to each other to maintain state. So all of your. All the jobs that you run, everything
that you're running on your cluster is stored in the server. So, like, they have, like, an internal
database, you know, in the Kubernetes world, think of this, like, at CD,
but Nomad has, like, his internal database, but they all have the
ideas that they, they start a state. Of the cluster, and then they talk to each
other to make sure that that state is, uh, replicated and sort of, available in
case of a failure or something like that. So, uh, maybe I'll write bullet points. So they store state
that's their main goal. The second goal is that
they also do the scheduling. So the scheduling process
happens inside server. So all servers have several,
what we call workers. which is quite confusing, because I know
that in the Kubernetes world, workers is something completely different. but the workers here are
the scheduler worker. And what the workers do, and your each
instance normally has like one per core on your machine by default, I believe. What does workers do? They're like go routines, you
know, to go into middle bit. So like, there's like a thread running
on your servers and their goal is to given a request to run something. So that would be like a job in
a middle angle, given a job. Where should I place things? So imagine that, I should move this. So imagine that I have
a bunch of clients here. Oh, I didn't talk about what clients
do, but clients, they run stuff. So clients are the machines that are
actually running your containers. They're actually running your workloads. That's, that's the point of the clients. and normally you can have like, from
like one to like 10, 000 clients. So like to infinity and beyond here. Some people really try to
push NUMA to its limit. but like you can have a lot
of clients on your cluster. and so the job of the scheduling is
given a job or like, you know, like a YAML spec in the Kubernetes world,
given this file, how do I translate this into specific work for each client? So let's say. I want to run like three
containers here on my file. The job of the scheduler is to say,
okay, like I'm going to go put two containers here on this client and
then one container on this client. So that's what scheduling means. It's like, given the spec,
how do I make it real? And this thing that sort
of makes things real. This is called plan. So the work, the goal of the
workers here, the scheduling workers is to create a plan given, you
know, what we call the job spec. What's the plan? So that's what the, the,
the scheduling process do. cool. So those are the two main
things that servers do. They start state, they do scheduling,
and then the clients run stuff. When the client receives the works,
like after the plan is generated, it gets validated, gets store in state. And then the server says,
okay, like this client here is going to run two containers. This client's going to run one container. So periodically this client's going to
go and ask the server, like, Hey, give me, which allocations I should run. And again, to translate a
little bit, an allocation is like a pod in Kubernetes word. So it's like the unit of. Of workload that's going to run. So periodically the client goes
like, Hey, what should I be running? since we just scheduled two containers,
the server is going to reply. Hey, these are the two
containers you should run. And then the client is going to start
a few sort of internal components here. so first it's going to create an
outlook runner and the outlook runner is responsible for setting
up the allocation environment. So it's going to create. Folders in your file system, like folders
inside that machine to store files is going to, uh, register service, you know,
either internally or register service in console is going to, uh, do things that
like prepared the way of the land, right. Call CNI plugins, all of
that happens in DialogRunner. And then just like a pod can
have multiple containers, an allocation can have multiple tasks. and so the alloc runner for each
task inside your location is going to create a thing called task runner. And this task runner, kind of like
the alloc runner, is going to set up the environment for the tasks,
like for the specific container. So it's going to create like environment
variables, it's going to download files, download the Docker image, whatever. Everything for that
specific workload to run. and then the task runner is going to
start, like, so like the end result of the task runner is like a container or, a
binary or whatever you want to run here. It creates like a process on the machine. So that's like all things running on it. So from that file there that is
specified, what you want to run, you give that to the server. The server is going to send it to
the, oh, I forgot to mention the process of how it gets to the worker. Okay. So normally you have three or five servers
and one of them becomes the leader. So the leader is the only one that can
actually write state to disk and write the final, like the global, the single
source of truth is storing the leader. and the leader also has a
thing called the, the queue. So that's called the eval queue. and the eval queue is
like work to be done. So an eval is like something
has to happen in my cluster. So that. That's something that's described
into an eval, goes into queue, and then the other servers sort of
dequeue from there to find work to do. So the workers are dequeuing work,
processing that eval, generating a plan. That plan becomes state, so like gets
written into the global cluster state, and then the clients are sort of like
asking for updates on that state to figure out what needs to run, what
needs to be stopped, and all of that. I think that's it. That was a lot. so let me know if there are
any, questions on this part. Hopefully it all made sense. So let's get to the actual stuff that I did. so this is the Nomad repo. There are a few, so if you search
for OTEL branches, you're going to find my previous adventures,
putting Open Telemetry enrollment. And I'm going to go like sort of, and you
can see, like, it's been a while since I've been trying to do this properly. so I'll think I'll go
probably chronologically here. so the first attempt, when I learned
about this, it's like, cool, let's put distribute traces everywhere. And like, my goal here was
for, given all of this. Process that happens is like, this
should all like, in my mind, it was like, Oh, this is like one process. Right. So like this, everything here
should be a single tray or like have a trace, a unique trace ID
that like, Oh, this could be a span. And then when it goes to the
queue, that's another span. And then it gets dequeued , that's like
create a bunch of spans in a huge trees. So I wanted to map this process of running
something end to end, you know, as a user, submit a job, how do I get visibility
of what's happening inside the cluster? so that's the approach that I like
to call boil the ocean, because I wanted to do everything at once. so first let's run this branch, I guess. To see what that looks
like, let's hope that the two year code, two year code
is two runs and I was testing this yesterday, but I think it will work. I'm also going to start the, and the. Open Telemetry demo repo that I
found was like, I need some sort of, um, set up to run, like I need
Jaeger and I need a collector and I need an application to test. So I'm just going to start the whole demo
here, um, just to have something to, to generate stuff, uh, to generate data. So, okay. Deprecations or warnings are not error. So let's assume that this worked. so if you look at the branch,
let's start looking at the diff. So one of the things that I did or I
had to do was to have a sim config, uh, additional configuration for Nomad. So like we have it on, like when
you start a Nomad agent, you need to pass a configuration file. so I added like configuration
for Open Telemetry endpoint and. Yeah, just to like, explain to like,
where to send all the Open Telemetry data and I think I have it here. Yeah. So like telemetry, Open Telemetry,
endpoint, localhost for 3. 7.
That's where I'm running the, the Open
Telemetry collector from the demo and insecure because I'm not using TLS. So I think that's all I need. if you haven't used Nomad before, Nomad
Agent Dev is a quick way to start. It gives you like a fully
functioning cluster locally. So it starts a client, starts
a server in the same process. And so like very handy way to try stuff. It's also ephemeral. So once it goes down, it destroys
everything created, which is nice. For cleanups, but sometimes bad because
sometimes you do want to keep stuff. So, but it's a good way to,
to quickstart with Noma. and I'm going to read this config with the
additional Open Telemetry configuration. So let's start that. and I'm actually also going to run a job. So if you haven't seen it before,
a Noma job looks like this. He uses the HCL language,
sort of like Terraform does. And there's not a whole lot
to go over this right now. I think the main thing is
like, and it's kind of hard to map to Kubernetes, concepts. so I'll try my best, but like a group
is kind of like a deployment tells you what to run, like how to run things. So like in the group, you can have
count, you know, like how many instances you want to run stuff like that. the task kind of like. Or like the groups kind
of like the pod spec. So it tells you, it's a
pod spec plus deployment. So like, it tells you like what
to run and how to configure stuff. and then the task is like a
container definition inside a pod. You tell, Neumann doesn't only run
containers, so you can do other stuff. And that's plugins, so
those are test drivers. and then the config is how
you configure the test driver. So this is just like saying, hey,
I want to run a Redis container. and it's going to expose this board
and give it these resources to run. So fairly, fairly simple job here. but when I run that, it
will, so like running great. So like, as I mentioned, like it
creates, evals, like the evaluation because something changed in my cluster. That's how I communicate changes in them. It is like through this evals,
uh, that eval created a location. The location is like the pod. So it's like. The thing that is actually running, you
know, what is that, uh, and it works. It's all running. if I go to the Nomad UI, Oh, this
is a node version of the previously. There's like a special flag that
I had to run, but let's, let's hope it's actually running. now if I go to Jaeger,
which is where here, I will see. A lot of services. So those are for all the demo stuff. But I also see two Nomad services here. so on this Boil the Ocean approach,
I, as I mentioned, like, I wanted to get the whole flow end to end,
from user to, like, the clients. So I started from the CLI. So you can see here, there's a
CLI, Nomad CLI, what's it called? Service with a job run action here. And then, it's kind of small to see. And so, like, I started putting
traces everywhere I found stuff, like, it's one huge trace with a bunch of
different, uh, spans for, like, each individual action that is happening. So, like, you can see
here, the Nomad CLI ran. The job run command. I think there's like, I don't
remember how much extra data I put. Like, I was experimenting
with all of this stuff. So I could put log entries here. so there's some Nimitz CLI job run
command ran that CLI made an HTTP request. So they put request. To this end point here, that put
request, became a job request in the FSM. the FSM for those who haven't
heard this term before stands for finite state machine. That's how data replication works. imagine, imagine you're trying to
like help your friend find your house. Well, normally you tell them to go to
Google, but like, imagine you live in the middle of nowhere, there's no GPS. So you need to give them instructions
on how to find your house. You know, drive 2 miles east,
turn right, drive another, you know, how many miles, turn left. Like you give specific instructions. now imagine you're not just telling
one friend, but like multiple friends. And then you, you kind of give
that instructions to all of them at the same time, so they
can reach the same destination. So that, that's what the FSM does. It's like give an instruction. It mutates the state in a
consistent way across all servers. hopefully that made sense. I think I got, I confused myself, but let
me know if, uh, if that wasn't very clear. so it creates a job request in, so,
so now we moved away from the CLI, so that's the HDP request that was made. Now the Nomad agent is who received
that HTTP request, uh, and turn it into like an internal request. For a register. So that's a job register, commands. I caused the job register,
method internally. And you can see here, like I'm
putting logs and stuff, trying to, you know, events and figuring out,
how the Open Telemetry library works. after we registered that we have these
things called emission controllers, which there are two kinds of them. They're like the mutators
and the validators. So the mutators are the, so there
are things that we need to mutate. When the job comes in, so for example,
we need to, uh, so for example, if you're using vault in your job, we mutate your
job to put a constraint to say, Hey, only run this on clients that have vault. Otherwise your job will never succeed. So that's what the mutator does. It mutates the input. and there are multiple of them. So one of them kind of, I hate
this word, canonicalize the input. So like sets. Default values that have not been
specified by the user just to make sure that the job is consistent. console connect, expose checks,
and like all of these are separate things that are mutating the job. And then we have validators. So they validate the job. and then it goes through So like, Raft is
the mechanism we use to propagate changes across our servers to make sure that those
changes are consistent in all the servers. so it goes to raft. There's like a bunch of
methods that are called there. then it goes to NQ. So that's the queue that I mentioned about
the eval broker queue is like here is some piece of work that needs to be done. Now it's in the queue. And then you can see here that it got
dequeued  later on by another server. and what's this? Oh yeah, I don't know what this is. yeah, and then like got dequeued  by
somebody to process and create that plan that I mentioned. So like. And then, yeah, like there's a lot
happening here, which is kind of neat, but also kind of overrun, but like after
the CLI made the foot request, it keeps monitoring the eval to get status updates. So it makes a bunch of HTTP GET requests,
uh, just to get the update for that eval. And then it starts monitoring
the deployment and again, start making a bunch of HTTP requests. So sort of the gist of it is like
that whole end to end approach. Like I wanted to create a span that
showed me, uh, what happens after I run Numad job run, because like from the user
perspective, all you see is just like this output, which is quite confusing. If you're not used to Numad
and quite tricky to debug. If you don't even know this, like
if you're an ops person and your job runs in like a pipeline somewhere, you
really don't have a lot of visibility of what happened or what's happening. So. That was my first attempt. It's like, how do I map this process
end to end and give, give as much detail as possible to the cluster operators. now why this was not a good
approach is one, because it creates a bunch of overheads. Like if you look at all the different
spans, the like zero microseconds long or like 50 micro, like that's
not very useful, I guess, like it's. It's nice to note about them,
but like in a practical sense. It creates a lot of overhead, like each
of these have a lot of data in them. that needs to go through the network,
needs to be stored somewhere. So like it creates a lot of overhead
for, every time a job runs, for example. What happens when you, just
drill down into one of the spans? What kind of information
does it, send you? I don't think I put a whole lot. it also depends on the span. Like sometimes they put data,
sometimes like, I think you're. I did and where was it to Q maybe
I put some Nomad specific data. Oh, here, like the eval ID, for example,
I was like trying to experiment with adding metadata to different spends. but it wasn't very consistent. So, like, some of them
have additional data. Others, there's just the
standard, Open Telemetry SDK data. yeah. So the first problem was the overhead. The second problem is that if you look at
the diff, there's a lot of code changes. For example, let's look at, it's
like the HTTP handlers, you know, OpenTelemetry does have a nice wrapper. It does have a nice wrapper for like Go,
HTTP, uh, but we don't quite use that. It's like I had to manually wrap
each endpoint in a function that had to use reflect to figure
out which RPC should be called. It's like, it's kind of
messy, but not too bad. The actual bad part is that
Normally distribute tracings. I think it was like created with
the idea of like microservice, so like there's like a metric boundary
more or less between your services. And since Numad is more like a
monolith, like each component of numad is a big monolith, a lot of
this, there's like, there's boundaries are defined by function calls. And so I had to modify all the, like
the functions that I needed to, to monitor like that I wanted to instrument. Had to be modified to like, take a
context, for example, as a new org, um, to keep propagating that span forward. And, and so like, using function as
the boundary for your telemetry is not great because it just becomes,
uh, like, it's not as transparent as monitoring the metric layer, right? So there's no way to do it automatically,
and it just requires so much code change. that, yeah, like after I got, and I
didn't even go through the whole process, if you notice, like I stopped at the
dequeued s, like after dequeued s, the span sort of ends because I just couldn't
keep going with all these code changes. So like most of this PR is just
adding context as the first argument to a bunch of functions, uh, which
is not bad by itself, like a lot of functions having a context there can
be helpful, especially for async work. But a lot of times it was not, it
was just to keep the span going. So there's a lot of code changes for not
a whole lot of, not necessarily not a lot of benefit, but like a lot of code changes
that were not the best changes to make. oh, hi Matthew. yes, I can send. I'll send you the list of
all the branches, I guess. Oh yeah, thanks. Also, hi, long time I'm
ahead of, long time no see. Hey, nice to see you. yeah, so these are all the branches. And then, like, once you click
on the branch, you can see GitHub will give you a link for, like, one
commit ahead, and that's the deal. So, yeah, so this approach, you know, I
think is very nice to have this view, and Jaeger has some nice things about giving
you, like, how the systems interact. So, like, I could make this bit
like each service could be like a different server and like I'll have,
you know, three servers, multiple clients and sort of have that view. but I don't think that
was the right approach. Like it is, it feels, it felt like
going against the principles of distributed tracing, which is like. You instrument your microservice and
then the network becomes sort of your, your boundary of when to move the span. And I was doing it at the functional
level, which fell very, very fine grain to what I was doing. just before you, you move on, you
mentioned that you'd done some, you had to like create some wrappers
yourself around some of the OTEL calls. Yeah, all of these. Yeah, so, how, how come
you had to do that? Sorry, I missed, your reasoning on that. Oh yeah, so, normally the SDK does it
automatically for you if you're using a, like, framework, like, NATHTP, or I think
it does support, like, the Gorilla MOOCs. So, normally you just import the library
and it sort of does the magic for you. But the way we do it in Nomad, it's
like another thing to keep in mind is like Nomad is a very old project. A lot of people don't know how old
Nomad is, but it's as old as Kubernetes. I think Nomad is like one
month younger than Kubernetes. Like the V1 came out sort of
like around the same time. So that was like 2017. I want to say. Yeah. So like Nomad is a very old code base. and so a lot of the things we
do like predates a lot of the new sort of code niceties. So like the whole HTTP layer, the
whole RPC layer is something that we had to create because It wasn't
a way like there wasn't a standard or like a more common way to do it. So you can you can leverage the Open
Telemetry conveniences as a result because you're working with some older. Yeah, because it has so much custom code. Gotcha. Gotcha. That's very interesting. I think especially hits on an interesting
point for for folks who are looking to incorporate an Open Telemetry
into their existing projects and. Oftentimes, like we are dealing with
very old code bases and that's not something that we discuss enough. So I'm, I'm definitely happy that
you're, you're bringing this to light because I'm sure you're not the first
and only person to encounter this. so yeah, thanks. Yeah, no worries. but like it did, as you can see, like
the rep is fairly simple and does use the SDK quite a lot, but it's just
like the way the project was was not. The way that usually go HTTP
handlers are written nowadays. So that was kind of why I had to do this. cool. So that was the first approach, uh,
kind of like gave up just getting too complex and the heck it was done. So I didn't have a lot
more time to work on. the second approach, I sort of
shift the perspective a little bit. So instead of like trying to
instrument Nomad itself, the idea was to have it easier. For people using Open Telemetry and using
them to instrument their own applications. so what this in this branch, what I
did was, so here's the test runner. So, you know, going back to the. The task runner is the thing
that runs your container that actually runs your workload. You set up the environment
to run your workload. and what I did here was I started adding. So if you use OpenTomato before,
you know, this environment variable will tell resource attributes,
which I think is part of the spec. And it defines, um, go here. So it defines. values that are automatically added to
your, like, span context or like whatever you create with Open Telemetry, this
environment variables sort of creates standardized values to, or like metadata
to add to those resources that you create. And so what I did here, oops, this one is
like, I say, okay, like if you're running, if you're already using Open Telemetry
and you're running, stopping now mode. Could be helpful to automatically get
NOMA data for your, Open Telemetry stuff. So having the alloc ID, the alloc
name, the evals, like all of this data sort of gets injected automatically
for you, um, by Nomad itself. So like this, but all of this
whole code, what it's doing is just setting the environment variable
with a bunch of Nomad data. So. Let's take a look at how
that works in practice. And I think, what you're doing
here is, is more or less like what Kubernetes does as well now when it,
when it emits like telemetry data that you have that kind of free with
purchase, um, environment variables. Right. Which is quite nice. Yeah. Like I think that like information
about the fault information about it, like that sort of just happens,
automatically for you, uh, when you're using the Open Telemetry. SDK and all the collector. so let's run this. I don't think I need the config
because I'm not sending any data. So I'm just starting Nomad
and oh yeah, I remember now. So I need, so like I'm running Nomad. Nomad is going to automatically
instrument my application. So I need to run something in
Nomad that uses OpenTelemetry. So I found this project. I just Google like
OpenTelemetry, simple job. Or docker image, so I found this
OTEL gen project here that it just generate traces for you. and so I'm running that image and
pointing to my docker container here that is running the collector
and generating multiple traces. and there's a back job, so
it's just going to run once. So when I run that. Sorry, what does, what
does that batch job do? It generates Open Telemetry data. Let me find that repo here. So, yeah, so it's this
project, which is very helpful. It just generates, you can tell that
you generate metrics, traces, and then it just creates sample data
for you in the OpenTelemetry format. And is it supposed to be like, it,
as you said, it's just sample data. It's just like, you're not
actually using it for anything other than for test purposes. Kind of thing. Yeah. Yeah. It's just like, imagine this is like your
application that is generating traces. Okay. Got it. Got it. Cool. And that application is running. so let's look at the data it generated. So he created three traces here, three
traces, say that fast, two times. so now, if you look at each trace,
he now has all the Nomad stuff. So he has like, he has ultimate,
like this was like a GitHub project. So I did not change
any source code at all. but just by the fact that is running
Nomad Nomad itself is instrument of like populating a lot of metadata
for each trace for each span. So like it gets, the alloc ID, the
alloc name, the eval, the group names, like If you are a user of Nomad and if
you're already using Open Telemetry, all the things that like with this
change, all the things that you're running Nomad with Open Telemetry, we
will automatically have all of this additional context and data for you. kind of like, I didn't
know how much data to go. So I put like a lot, uh, but this is
like to help the data is not instrument Nomad itself was to help people that
already instrumented their applications. When they're running them and
they get additional benefit. So that was different
approach to, Open Telemetry. so yeah, it generates this
additional context here. now the problem with this approach
was one, I didn't know what to put. So I just put a lot of things. A lot of this is probably not
useful, like job type and who cares. And then like, cause like. You know, metadata is good, but
too meta, too much metadata. It's also tricky because you increase
your, you know, your metric packet size. You need to store this somewhere. So it kind of balloons
your store and store. and so the challenge with this approach
was just to, how do we, it's more like a UX problems, like how do we allow
people to control what information they want, or like, do we allow them to
control or do we hard code some values? so it's like figuring that
balance was a little tricky. and looking at the code, I think
this is just like, yeah, I just hard coded a bunch of stuff because
again, this is a HackyVic project. but yeah, it's like, if you see,
there's no knob for users to control what kind of data they want. I think the next work is kind of similar. I was also doing this, this
environment variable here. I haven't actually
opened up here for this. It's like, it's just like
cleaning up code a little bit. this one is kind of, yeah, it's
like the same as this one, but like cleaning up and writing better
code with tests and everything. Oh yeah. One, one of the problems with the
other approach is that if you are already using Open Telemetry, you're
probably doing something like this. You're already, you may already be
adding your own, um, values here, right? I know you probably, you may already have. Your own environment variable
for your attributes out there. And so like, if I run this job
or this version of the job, now that code that I initially wrote
would kind of overwrite that. So you can see here, there's no key equals
value because the way I did it, I just like overwrite that environment variable
and you lose, stuff that you put yourself. So in this other approach, I. Instead of overriding, I will, I merge the
changes just like being nice for the user. what I think, oh yeah, I actually
found a bug, the SDK here, um, which wasn't decoding the values
as it was describing the spec. But then we actually figured
out that there are like two versions of the spec in. Place, then I had to go
and change this back. There's like a whole side quest that
I had to go through to, to fix this. and I think it hasn't
been, let me do a check. So I changed this back, which
means every SDK had to be updated. And there's the link. Yeah. And I think there's still some open ones. Like people are looking for, you
know, contributing job and telemetry. This might be a good first issues. They're like sort of simple. It's just changing all the. That variable gets decoded, and
there's still some languages that have not been fixed. So yeah, for contributions, this could
be a good first issue to look into. yeah, so that's the second approach. Good for users, not so much instrumenting
GNOME itself, but could be useful. Just the UX paper cuts were
the major drawbacks there. Nothing OpenTelemetry specific,
just how do you do surface text. Oh, one thing, another downside
of this approach is that since it's using environment variables,
those values are rather static. like you cannot modify an
environment variable to a container that is already running. So if you want to put data that is
not known ahead of time or some, some data that can change throughout the
life cycle of that container, this approach doesn't quite work because the
environment variable is already created. So it's not great for, dynamic
values or things like that. Those you usually have to
do, inside your application. Cool. So that was the second approach. And then each, each
approach, like a year apart. and then after doing all
of that, like that's. I think it's sort of when I met,
right after this or like I met Adrena and the other folks at
the OpenTelemetry community. And I started to get some guidance
of like, you know, Hey, I'm trying to do this, you know, going
back to the original approach. It's not quite working because
of all of these reasons. And speaking with Adrena and I
think Ted was also very helpful in giving feedback on this. It's like, don't try to boil the ocean. Like you don't need to
instrument everything at once. You can start small. And like, you know, find your core, I
think the exec, suggested, like find your core business instrument that,
and then you get a lot of value out of that already and translating that
into the nomen word to me, it felt like, oh, okay, like I don't need
to instrument everything end to end. We already have this eval thing
that is like the unit of work by instrument that flow of the eval. Maybe I can get, you know,
enough information out of this to be, to be useful. So let's in practice how that works. So let's go to this branch now, and I don't think I need a config for this one. So let's run this version. And what this version does is like, it's
going back to the first implementation, like I want to instrument Nomad itself. So this is adding traces and spans and
all of that into the Nomad code itself to instrument Nomad for operators. And so going back to my traces here, Nothing happened because
I didn't run the job. Let's run a job to do some working. I don't need to wait. yeah. So now I have a service called Nomad. And if you look at the traces now,
instead of, you know, let's compare it to the previous implementation. So before I had, uh, I think it. Oh, yeah, I think I might have deleted,
but before I had like one huge trace of everything now, instead of going and
like trying to keep the context around each function calls, what I did was more,
and you can see here, like, there's a bunch of traces instead of a single one. So what I did was. I didn't try to connect all those
function calls, but it was more about instrumenting, instrumenting
the specific aspects like here in DialogRunner when DialogRunner starts. You create a new span like I, I
wouldn't, I didn't worry about keeping the same span going. It's like, okay, this is a
new action that is happening. It's going to be new span. so I, I guess the, the approach
was, being more mindful of like, what you wanted as far as like
what you wanted your traces to be. Because you went from, like, a mega trace
to a bunch of smaller traces, where, like, these smaller traces then have, like, more
meaningful information to you, is that? Yeah, yeah, exactly. It's like changing, changing the
perspective a bit instead of. Keeping the gigantic trace be
more like serious about like, okay, this function is important. So this function is going to create
a trace, uh, and what this means is like, I don't need to change my,
you know, my function signatures. I don't need to keep
passing context around. It can just, whenever something important
happens, I just create a trace there. the downside, as you can see, is
that there are a lot of traces here. but. A good thing, like something that
I learned later on is that, well, I knew this already, but I didn't
know how to use it specifically, but traces have attributes. And I think that was the key to
this approach is that, uh, and there's this notion of semantic
conventions in the OpenTelemetry SDK. for those who don't know, it's like. It's that of predefined attributes
for your, just like Docker has, or I guess they're called containers, like
your container engine has predefined keys that it's supposed to use. Kubernetes has its own AWS have
their own, attributes or like standardized attributes that they use. And so the key to this approach was like,
okay, what does that look for Nomad? and so I think it was at the bottom. Yeah. So I created like semantic
convention for like. Oh, what does a Nomad
region key looks like? Oh, it's Nomad. region. What does a Nomad space key? No, Nomad. space. And then this allows to each
span, you know, these are all like different spans coming from all
different parts of the code, but they have standardized attributes. And what this allows me to do is
like, I could come here and say, Oh, I actually want job ID to be example. And so I can use these values to filter
all this mess that nobody's ever done. It's not like a nice single
huge trace, it's just a bunch of different disconnected traces. But by using the semantic conventions, I
can help find and filter things better. So these are all the traces
for, you know, this job here. But let's say I go to the UI now. so this job created is
a location, for example. So let me get the allocation ID here. And I said, I don't want the whole job. I just want this. I'll look. So now it's like all the traces for that. I'll look, because you know, even
though they're different traces coming from different parts of the
code, different parts of Numad, they all share the same conventions. And so that allows me as like cluster
operator to go and filter out all the quote unquote noise that gets
generated, to be more meaningful to me. And you can see here the same things
that we saw in the big trace, but now they're just breaking down. So like the alloc runner, the
state stores, like the database. and I also started experimenting
with like the log events as well. So instead of having, uh, let me find it. I think that might be more
interesting data there. So this is when I registered a job. where'd I get the ID? Okay. is this oops. Again, maybe this, I don't remember how I, how I did it. let's remove all traces and then I'll
find manually, uh, I was looking for, yeah. On, sorry, the zoom
window gonna go, uh, see it? Yeah. Oh, maybe I'll, I'll, I'll find it. Share later. But the idea is that I
started to experiment with. Logs a lot more. maybe this one took a while. Let me go back to. Are you referring to just, span
events or, or actual like OTEL logs? I'm just curious. yeah. Yeah. Events, uh, data. Let me run the job again. And let me take a look at this. Yeah. So like, so for example, this is the,
the trace, the sort of like the scheduler part, the worker scheduler, all of that. So like I started putting logs that
like, as it was scheduling, it's like all the scheduling decisions
are like part of the trace now. so you can see like why. Why, why did this change happen? And like you can see here as part of
events in your span, instead of having, you know, the multiple tiny traces,
those function calls sort of become more events instead of a new span. Yeah, that's a strange point. That's awesome. we are at time, just FYI,
an hour blew by really fast. Yeah, that, that, that's
what I had showed. That was the last attempt that I made. and if you have questions,
sorry, I missed chat. you have to add attributes each trace. Oh yeah. So the code to add the attributes,
you do have to call for each trace. but you can kind of make it like
helper functions to, to do like, let me find an example here. Yeah. So like you can pass in like helper
function, like, okay, given this allocation, you know, this pod,
give me all the attributes for that. you do have for each trace,
but you need to, to, you can create helper functions too. To ease the burden. That is so awesome. thanks for sharing your,
your OTEL journey with us. Louise, this has been, this
has been super, super helpful. I hope everyone on this call
got something out of it. I, I think it's so important to have
these kinds of conversations because I think we're moving past the, you know,
let's, you know, intro to OTEL, like. We're, we're now getting into like
more people doing really digging deep into their OTEL implementations
and, and hearing about, you know, the challenges that you've experienced
and, and the different things that you've tried, has been really awesome. So thank you so much for
sharing this with us. and, for those of you on the call,
tell your friends who couldn't make it that we will be posting a recording. Of this on the OTEL, YouTube channel, the
YouTube channel is OTEL dash official. so just keep an eye out. I'll post on socials and also on Slack
once we have the recording up and running. So thank you everyone for joining. Really appreciate it. Thank you everyone. for coming and thank you for having me. Bye. Thank you.

