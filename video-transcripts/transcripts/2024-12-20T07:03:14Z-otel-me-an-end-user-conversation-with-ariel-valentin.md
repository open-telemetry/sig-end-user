# OTel Me... An End User Conversation with Ariel Valentin

Published on 2024-12-20T07:03:14Z

## Description

Join the OpenTelemetry End User SIG for our last event of 2024! We'll be talking to Ariel Valentin of GitHub for OTel Me...An End ...

URL: https://www.youtube.com/watch?v=vB9_SiTV5CI

## Summary

In this episode of "Oh Tell Me," hosted by Reys, a Senior Developer Relations Engineer at New Relic, the focus is on open telemetry and observability, featuring guest Ariel Valentin, a Staff Software Engineer at GitHub. The discussion revolves around Ariel's experiences with observability, the adoption of open telemetry at GitHub, and the challenges faced in transitioning from proprietary SDKs to open standards. Key topics include the importance of distributed tracing, the need for a shared vocabulary in observability, and the architectural landscape at GitHub, which utilizes a customized OpenTelemetry collector. Ariel shares insights on the impact of tracing on service monitoring, the growth in usage post-adoption, and the collaboration within the open telemetry community. The episode also emphasizes the importance of community involvement, encouraging viewers to participate in SIG meetings and contribute feedback.

## Chapters

00:00:00 Welcome and intro
00:01:37 Guest introduction
00:03:00 Format overview
00:04:10 Warmup questions
00:05:40 Observability discussion
00:09:40 Adoption challenges
00:10:50 GitHub's OpenTelemetry adoption
00:12:10 Architecture landscape
00:15:00 Telemetry capture methods
00:19:01 Community contribution process

**Reys:** Hello everyone! Welcome to a brand new episode of Oh Tell Me, an end user Q&A. If you have been to one of these sessions in the past, you might notice that this looks a little bit different and it has a bit of a different name. That's right, we have done some growing up since the last few times and we're very excited to debut this new look. But don't worry, almost everything else is going to be the same. We have a great interview here for you today to learn from. 

But before I introduce myself and our guest, I would love to know where everyone is connecting from. I am online from Portland, Oregon today and would love to see where people are from. It looks like someone is here from Brooklyn, New York, so thank you so much for joining out in New York. 

[00:01:37] So my name is Reys. I am a senior developer relations engineer at New Relic. I also co-lead the Ner Sig, which is our cute little logo that you see up in the right corner. At the end of your Sig, we really focus on connecting directly with users and we are dedicated to helping the rest of the SIGs get feedback from end users so they can help improve the project. To that end, this is one of those events that we host to do that. 

We are going to have Ariel on. Ariel, if you would like to join us and introduce yourself.

**Ariel:** Hey, how's it going, everybody? I'm Ariel Valentin. I'm a Staff Software Engineer at GitHub, working on observability. Thanks so much, Rey, for inviting me here to chat with you today. It's an absolute honor to be the first in the new format. Also, a big shout out to everybody who's done work to try to get this new platform up and running, so thank you.

[00:03:00] **Reys:** Oh no, thank you for being here! I'm really excited. Just to get you and everyone up to speed on the format, it's similar to the ones if you've been on one of these before, but we're going to start with some warmup questions where we'll kind of massage Ariel into being comfortable, and then we'll hit him with some meaty questions. Then we will actually also get into questions more around the Open Telemetry community, where he'll have an opportunity to share feedback about his experiences with contributing, using the project, stuff like that. Then he'll get the chance to ask us questions as well in a little section we call "turn the tables." At the very end, if there are any audience questions—oh actually, scratch that—if you have questions that come up during our conversation, feel free to put them into the chat. If you're watching from YouTube, then just put them in the live chat, and then I think LinkedIn will also have its own chat, so put in your question there, and we will get to them as we can throughout the conversation. Then at the end, if you have questions at the end as well, then we will get to those. But yeah, if you have any questions that pop up as Ariel is telling us his story, we definitely want to get to those as they come in. 

All right, so I think we are good to get started. Ariel, tell us a little bit about your role at the company. How did you get started with observability? How did you get started with Open Telemetry?

**Ariel:** Oh, those are great questions. I mean, I should have mentioned that I'm here in Austin, Texas—Sunny Austin, Texas—so you know where I am in the world. I think a lot of us started originally, and I don't think my experience is unique, is working with sort of like APM tools, which are vendor proprietary tools, generally speaking, that didn't have distributed tracing in place at that time. As the community evolved and Open Tracing became a standard, that was my first experience with working with distributed tracing was the Open Tracing specification and working and trying out different vendors, different experiences with Open Tracing. 

[00:05:40] By the time that I had gotten to GitHub, I was a champion for distributed tracing because I saw the power that was in there. Around that same time, folks were already moving towards developing and transitioning away from Open Tracing and Open Census to Open Telemetry, so I got really involved very early on in trying to spread the word and learning more and working with my team, which was the observability team, to start to adopt tracing more, to embrace it more, and to make Open Telemetry sort of our North Star for all of our telemetry signals. 

I feel like I keep saying the word telemetry over and over again. I'm going to have to find a synonym for that. 

**Reys:** Yeah, me too! We're both going to stumble over Open Telemetry at some point or distributed tracing at some point. It's all right; this is a safe space. 

What do you think is a main challenge that most organizations face when it comes to observability? Is it not just understanding the value of distributed tracing?

**Ariel:** Yeah, because these are all sort of new concepts to folks, right? Folks have their different levels of understanding of the tools that are available to them, and there's all this vocabulary that you hear that's a little bit hard to parse through. Sometimes it's like, "Oh, when you say trace, do you mean like a trace log? Log level is at the trace level? When you say trace, do you mean the samples taken from a profiler?" There's a lot of this sort of language that, even though we're converging on a lot of this language and we have these dictionaries that are defined and published everywhere, there's still sort of like this hump that we have to get over or like a challenge that we have with trying to get everybody speaking the same language within the same context. Very similar to in domain-driven design, we have these bounded contexts where the same term means a different thing. That flows over into sort of our domain language when it comes to observability and SRE practices, and I'm sure many folks have faced those challenges as well. It's kind of like, "Let's all get on the same page about what we mean."

**Reys:** Oh, absolutely. What are currently some of the most interesting problems that you are facing in your role?

**Ariel:** Oh well, I mean one of those things is that transition, right? It's really hard for an organization like us, who's been around for a long time. I say a long time, but you know, whatever it is—10 years—where the system has grown and evolved. There have been acquisitions that have been brought together; there's disparate kind of backends where we're collecting this data. It's trying to transition from one way of doing things to a new way of doing things, right? So it's learning something new. That's always going to be a big challenge. Folks are trying to do their job every day; they're not trying to learn new SDKs or trying to learn new vocabulary or trying to learn how to be proficient in their backends. What they're trying to do is keep the system up and running and keep our customers happy, right? 

I think those are some of the challenges that we all face. I know I face it, and I'm sure others do, which is making these transitions with the fewest pain points as possible, trying to avoid these pain points. There's so many more we can go on and on, Reys, but I think right now that's kind of like one of the biggest challenges for adoption overall.

[00:09:40] **Reys:** That's actually a great segue into the meaty section. What was the process like for GitHub to adopt Open Telemetry?

**Ariel:** For us, it was the role of the advocate, right? I acted as an advocate and was a champion for Open Telemetry at the company. I was specifically brought in to help advance the mission of tracing, and it was something that I had pitched. There were other challenges that we faced too, which was, "Hey, let's get everybody building a data dictionary. Let's get everybody agreeing to the same language when it comes to what our attributes were going to be." As you can imagine, as a system of walls or acquisitions or other teams are rolled in, everybody has their own log attributes or their own metric attributes or whatever it is. 

[00:10:50] I said, "Look, here's a North Star. Here's semantic conventions from Open Telemetry. Let's anchor onto this and let's follow the rules around semantic convention so that we can all build up our own internal dictionary." That comes with its own challenges—schema migrations and trying to keep up to date with the spec and what the instrumentations are doing, right? As an advocate, I was also a lead on the rollout. One of the things I wanted to do was sunset all of our old SDKs and move over to our open-source SDKs. 

My team and I were all involved in Open Telemetry Ruby, for example, because we're a big Ruby shop. We got involved there to help the instrumentations get better and to help test different releases of the SDK and get that rolled down into our GitHub monolith. We were, like I said, very early adopters, ran into some challenges, and continued to give feedback to the community that way. I'm not only an end user, but like I said, I'm also a maintainer, so I'm playing multiple roles there, trying to help the community along.

**Reys:** That's a really interesting position to be in as an end user and contributor. I definitely want to circle back on that when we get to the OpenTelemetry community questions. Tell us about the architecture landscape at GitHub and the telemetry that you're capturing.

[00:12:10] **Ariel:** Sure! I've got this little slide that I put together in markdown, so it's not anything official. We can bring that up now so we can take a look at it. I imagine that for a lot of folks out there who are working with virtual machines where they're running systemd units or if they're running Kubernetes, we have different deployment styles here. 

Some of our main workloads are running in Kubernetes, and the way that we've got everything set up, you can see here as part of our Kubernetes cluster, we run our own custom messaging graph. On every worker node, we're running a deployment of the OpenTelemetry collector, which we build using OCB. We have our own version of the OpenTelemetry collector that is specific to us. We chose that route because we wanted to ensure that we had the most secure build possible with the minimum number of dependencies, and we also wanted the ability to build our own custom processors so that we can address any issues that we might have that are specific to our needs. 

Inside of each of these pods, we also have a mesh sidecar. Ingress traffic is going to come into the OpenTelemetry collector over HTTP. If you have another application that's running your service, it's shooting over OTel traces over to the mesh and sending it to our OpenTelemetry collectors. From there, we generate span metrics and sample traces and send those off to a SaaS provider where we aggregate all this data. 

On each individual worker, we're running in this hybrid world right now. We're only leveraging OTel for traces and for generating span metric data. We're still living in a world where we're using non-OTel formats for collecting things like custom metrics, system metrics, and for collecting logs. On each one of our worker nodes, we have a metrics agent that can speak various different protocols, but mostly it's either using Open Metrics or it's using StatsD to collect data and aggregate it and send it off to the SaaS provider. 

[00:15:00] In addition to that, on all of our worker nodes, we have Fluent Bit running, and that's what we're using to collect our logs. All of our logs end up getting streamed out through Azure Event Hubs, which are processed by a bunch of consumers and then sent off to our log store and search system. 

In addition to these Kubernetes workloads, we have our own virtual machines where we run systemd units. You have to think about the Git file system services. Those are all running on systemd, and we have the metrics agent and Fluent running there. We don't yet have the OpenTelemetry collector deployed there, but that's where we want to get to. We want to get to a world where essentially our entire platform is running open-source software, the OpenTelemetry collector, and we're converging on OTel for all these formats. So those services are also using StatsD, Open Metrics, and Fluent Bit's pulling things out of JournalD because that's where we're streaming all of our logs of the system journal. 

All of that stuff again goes right through to the same channels, and it all gets aggregated in these places where we do our work. A little bit about some facts about our trace data is that we've rolled out the OpenTelemetry SDKs for different programming languages. We have them for Ruby, Go, JavaScript, Node.js, .NET. We had some experimental Rust usage, but we've had to scale that back, and we had some Java experiments that didn't pan out; we didn't roll those out to production. 

A lot of the stuff that we do, we started before any of the automatic instrumentation was available for some of these languages, so we're still deploying them through wrapper libraries. We maintain a set of wrapper libraries. You install those; it gives you the sort of the what we consider to be the minimal defaults that we require for our needs, and we'll do periodic updates of those through Dependabot. As we roll out new versions of those things, I feel like I've said a lot so far, and I don't know, Rey, if you had any follow-up questions for me.

**Reys:** I actually have so many! 

**Ariel:** Oh, okay, great!

**Reys:** You mentioned you are leveraging OpenTelemetry for traces right now, and you talked a little bit about the plans and some of the stuff that you tried. I was curious to find out more about is that mainly because you're primarily a Ruby shop and the SDKs for metrics and logs aren't quite as mature yet? Is that the reason?

**Ariel:** That's one of the reasons. One of the other challenges that I didn't discuss when you asked about adoption challenges is even though OTel says, "Hey, this is what the signals look like; this is what the semantic attributes are like," there's a lag between the time that something is published or declared in the OTel spec to the time that vendors or open-source platforms are able to leverage those things. There's sort of like this feedback loop as we go through OTs and say, "Hey, we want to try this new thing out." 

[00:19:01] We do a couple things in experimental languages; we'll do this stuff, and it was a big enough challenge for us to start to roll tracing out that we didn't want to take yet another thing on, which was to say, "Okay, now we're going to switch over to native OTel metrics as well." Like you said, some of the SDKs are ahead of others. So with Ruby, we're just recently, with the help of our amazing maintainers, working diligently to get the metric SDK up to speed for Ruby. So that's something that wasn't available to us to use. 

One of the biggest advantages that I see in tracing is the ability to generate span metrics from traces. It's like, "Hey, the less things that we have to impose on our users for them to try to figure out how to do, the better for us." 

**Reys:** Are you using the span metrics connector? Is that what you're using?

**Ariel:** We're using a custom connector right now. I'd like to be able to get us to the point where we're using a span metrics connector, but we don't have egress in OTel right now. We're still doing sort of like vendor proprietary formats for export. Where I want to get to is, you know, that's one of the biggest strengths of OTel is that OTel is that standard format that makes things portable for us.

**Reys:** Absolutely. So that sounds like the plan is to migrate your other signals over once those reach GA in languages.

**Ariel:** Oh, certainly. And once I have more time on the calendar too, right? We have so many projects that we have to do as engineers and companies, and it's like, "Hey, where do we fit these in?" As you imagine, GitHub is constantly growing every day. We just hit 150 million users, so it's like just an amazing growth of the company. Shout out to all the people at GitHub who've been working so diligently to make this happen. We're here to support that volume growth and support our end users. I hope that answers your question.

**Reys:** Absolutely. I also want to mention again real quick for those who joined us a little bit later, feel free if you have questions that come up. You want to learn more about something that Ariel has gone over, feel free to pop it into the live chat of whatever platform you're watching from, whether it's YouTube or LinkedIn. Go ahead and pop the question in, and we will try to get to them throughout the show as you can. 

I noticed on your second slide you mentioned probabilistic sampling, so it sounds like you're not doing any kind of tail sampling.

**Ariel:** No, not at the moment. Would you mind bringing that up again—the second slide?

**Reys:** Yeah, that's awesome.

**Ariel:** Yeah, so we started with probabilistic sampling. One of the hard things about tail sampling, as you can imagine, is that the traces all have to go to the same collector in order to make that decision if you're using the collector for tail sampling. Right now, we wanted to reduce that burden in complexity immediately. When we started with probabilistic sampling, some of the things that we want to do in the future is more advanced remote sampling so that we can have more fine-grain control of what we're looking at. We want to be able to leverage tail sampling rules, but as you know, that's very difficult to implement and a little bit difficult to scale in our case because we have about 2,000 collectors that are supporting everything right now across all of our fleet of like 14,000 hosts or something like that. I'm making that number up off the top of my head; I think that's the last number we had. 

Right now, we're doing about 26 million spans per second. Just yesterday during our peak times, we hit our all-time high of 32 million spans per second as we continue to grow our volume. This is one of the challenges that we've had, and so on my list of all the things that I want to do, tail sampling is definitely on there.

**Reys:** At what point do you think you might get to the point where you could implement tail sampling? 

**Ariel:** That would be more like when the processor has been more developed. I don't know how to answer that question because it's on the pile of the list of things that I want to do towards the bottom. 

**Reys:** Got it.

**Ariel:** Although I am curious about the other.

**Reys:** But that's okay! That’s all right. 

**Ariel:** Questions that I want to get to as well.

**Reys:** Sure! Also, just going back, you mentioned trying some experiments in Java and Rust that didn't quite pan out, and I was just curious what it was that didn't work out or didn't meet your expectations.

**Ariel:** Well, we've reduced the number—I'm sorry, all the JVM people out there. We've reduced the number of Java workloads that we're running, so those have been migrated over to different programming languages. That's one of the reasons why we didn't go through and say, "Oh, let me continue to roll out JVM work." We just didn't have the return on investment that we wanted to, you know, try. 

Also, we don't have a lot of the same thing for Rust. We have a very limited set of applications that run Rust, and those are performing for performance reasons. They ran into some challenges with how it impacted their latency when they introduced the use of the SDK. I'm going to be honest with you: me not being a Rust expert and being able to get in there and get my hands dirty to try to help out with addressing some of those problems and reporting them upstream, that was something that we had to put on pause. Again, it's like one program versus all of these other services that are running in Go and Ruby and JavaScript that we need to pay attention to. 

I think that's really where it was, where sort of like a critical mass of application services that use these programming languages, and it was like, "Hey, we have to focus our attention on those that are going to get us a higher return on investment for this."

**Reys:** Absolutely. Okay, so what was GitHub's observability tool before migrating to OpenTelemetry?

**Ariel:** Yeah, I mean, we were using a proprietary SDK for OpenTracing. OpenTracing was how we collected traces before, but generally speaking, GitHub is a huge StatsD metrics user still to this day. Logs are a big part of what we utilize here. Folks are looking at exception stack traces a lot of the times to try to understand where errors are coming from. They're looking at access log streams, and they were trying to piece together, "Hey, where's the request going, and where is it slowing down?" I showed up with my magic tool—distributed tracing. I'm like, "Hey, look, here it is on the flame graph or an icicle graph, or here's a waterfall view of this thing." It's like, "Oh, that's pretty cool!" People started looking into that as an additional tool in their toolbox for them to help try to debug things during an incident. 

I hope that answers your question there.

**Reys:** Yes, and kind of along those lines, how have things changed since GitHub switched to OpenTelemetry?

**Ariel:** What I'll tell you is that just this year alone, we've seen a dramatic—I wish I had these statistics off hand—but we've seen a dramatic increase in the number of people using tracing and the number of services that have been instrumented. Part of that comes from the fact that I was like, "Hey, everybody, we're all moving to this new SDK, so everybody install it." Let Dependabot go ahead and do these installations on your apps, and people started seeing the value of this investment as well. 

I want to say that when we started this adventure, only about 80—and I'm going to use the word services in air quotes—only about 80 services were instrumented, and now we're closer to about 300 of those services, which are effectively like Kubernetes deployment types or systemd types. As you can imagine, our monolith has, you know, maybe a thousand services within it, so there are like sub-services in there. But the monolith itself has broken down into like eight services—like web UI, API, GraphQL, background workers, stream processors, and whatnot. That's why I put them in air quotes as these are services. 

We saw that that was quite an increase in the number of services that came in. We were doing something like 5 million spans per second to now we're up to 32 million spans per second. It's just a huge increase in volume and usage.

**Reys:** Along with that volume increase in usage and data volume, how would you say that's impacted how your services are running and how quickly your team is able to debug issues as they arise?

**Ariel:** Part of it is an education thing. A lot of things that—I mentioned that I'm on the observability team, but really, we're two groups. One of the groups is called The Experience Team that works directly with teams. They operate in a role sort of like developer relations and advocacy, but also working on cost control and improving your experience, helping you with identifying new workflows and introducing them into your incident command experience. 

Those teams set up sessions, education sessions, to bring folks on board. We try to do them not during an incident because that puts some stress. What we try to do is, if an incident is happening, it's like, "Hey, here's some insights that we're gaining, and we'll share with you that we're seeing from the traces that you might not be able to see somewhere else." That has helped in a lot of cases where we couldn't exactly pinpoint what was going on. 

Then there are also these spots where not every system has been instrumented, so we have to rely a lot on sort of like client metrics or client trace data and say, "Hey, look, this client is experiencing this problem. Can we take a look deeper at this other service that hasn't yet been instrumented?" It's been sort of, I'm going to say, mixed results. Some teams have identified issues even before they go out to production, and other teams have been able to leverage it when it's like, "Oh, this, we're having an incident right now. Oh, here's the reason why this is failing." 

We've been able to do things like identify bottlenecks and mistakes, like little coding mistakes—"Oh, I forgot to ack the message before I pulled it out of Kafka. I'm just retrying that same message millions of times," or "Oh, the client timeout doesn't match the server timeout, so the client times out, and the server is continuing to turn along to try to do this request." We're identifying things like that that we couldn't identify before easily.

**Reys:** Oh no, that's awesome! I think I will probably be asking you more about that because I think this is really interesting. One more question from our meaty section: what would prevent you from implementing better telemetry?

**Ariel:** What would prevent me from doing that? I think it's because there's so much of the stuff that's in early stages. We were early adopters on a lot of things, but then there's a lot of stuff that's a lot more risky for us to try to roll out. For example, one of the things I'm really excited about when I came back from KubeCon is the continuous profiler. 

That's one of the things on my wish list that we'd be able to use today if we could. That's the thing that I would want to do, but because we're still in the early stages of the profiler and the specification, and there's still some churn with the data model, I think that there are—when it comes to the resource-intensive or sort of like introspective tools like that, we don't want to take that risk going a little bit too early to adopt those tools. 

Also, there's not a lot of support from our current vendors that will be able to leverage that. It's kind of like we would be experimenting with that to go to nowhere. That's kind of, you know, once vendors start to support the OpenTelemetry profiling format, data model, I should say, and once the profiler is a little bit more stable, I'd love to jump in there and be able to roll that out a little more widely. 

I think that a little bit of a bumpy road for us is still migrating Semantic conventions from pre-1.0 because we were early adopters of Semantic conventions, so we're at this pre-1.0 stage. Getting ourselves to migrate towards a 1.x version of Semantic conventions is another big challenge because we've already sent all of this data out to our backends. We have to figure out a way to upgrade it or to say, "Oh, this is version X." I kind of feel like a lot of the progress was stalled there for the moment.

**Reys:** If that answers your question.

**Ariel:** Oh absolutely! I think that is another interesting topic—migrating some semantic conventions, so I might jump back if we have time later. But I do want to get to some of these community questions.

**Reys:** Yes, ma'am!

**Ariel:** Yeah, so one of the big things I think working in the community that you've seen, I've seen, is people want to contribute to the project, but they're not really sure where to get started. We have resources such as, you know, I think in the documentation we added getting started, and you have various SIG channels and CNCF Slack. I think we all have the problem of how do we reach these people because there are still so many. What was the contribution process for you and your team like to OpenTelemetry?

**Ariel:** For us, it was a short process with a lot of observation. The things that we looked for—myself and my teammates at the time—when we approached the community, we were looking at the code of conduct: what were the expectations of the code of conduct? We went through and actually looked at issues, previous PRs, the feedback that was in those PRs to see if the behaviors expected in the code of conduct were reflected in the PRs and in the issues. 

There's nothing worse than you wanting to be part of this community and finding that those two things don't match. To see that that happened, then we participated in the SIGs, which was very nice for us to be able and convenient for us in the U.S. because the Ruby SIG was in U.S. hours. We joined those meetings during the day and started to provide some feedback as end users and say, "Hey, look, we ran into this challenge." If we identified something that was a challenge for us, we would contribute a PR. 

The maintainers were very generous with their time, did their reviews, and we were able to get a couple of custom propagators merged. We got some instrumentations merged, and we identified other gaps for the instrumentations that we use because one of the biggest challenges, I think, as a maintainer is there's so much that you have to do. You've got the SDK, you've got the API, you've got documentation to do, and you have to have language-specific instrumentations. That's daunting, especially with all of the popular libraries that are out. 

For us, we contributed back to the libraries that we use heavily. We have a subset of libraries that are very popular at GitHub and sort of blessed by their popularity. That's where we focused our energy and our contributions. Then we reached out to people. Anytime somebody comes to me and says, "Hey, I have this challenge with this thing. It's missing this attribute, or I'd really like it to work like this, or I've identified this bug," the first thing I do is say, "Let's open up an issue and let's work on this together. I'd love to review your PR." 

Creating an environment that lowers the barrier to entry for folks to collaborate and submit contributions is important. I want them to feel like this is yours, this is ours—this is not mine, and I'm gatekeeping. We still have some standards for quality, obviously, and some expectations from you as a maintainer, but we try to make this a painless experience for you, at least in the Ruby community. That’s what the contribution process was like for me and getting involved in the community.

**Reys:** That's awesome! So you started basically joining the SIG meetings as an end user and sharing feedback and then finding information about how you can open an issue and then building custom stuff and helping the community or sorry, the Ruby SIG build out more of their components.

**Ariel:** Okay, yeah, certainly. And like, you know, with the home for open source, right? For us, we open-sourced, for example, new database drivers in Ruby for MySQL. We had that instrumentation in-house before all of that became public, and as soon as we released that new driver, I went right to the SIG and said, "Ta-da, I have a donation for you!" It's this thing where there's just this spirit of collaboration and supporting open source that’s important to our mission.

**Reys:** Okay, so it sounds like you had a pretty positive experience from contributing to Ruby. Do you have any feedback for the SIG on ways to improve how things work based on your experiences with Ruby and then other SIGs? I know you mentioned also some other components that you've used, but we'll come back to that.

**Ariel:** Yeah, for sure. Again, I want to thank everybody for their amazing work that they've done contributing to the collector in particular because we release our OCB build, or sorry, our custom build, every time a new version of the collector package rolls out. Every week, Dependabot is telling us, or every two weeks, it's telling us, "Hey, it's time to upgrade; a new release has rolled out." 

We ran into some challenges where we identified some performance issues in the collector, and we worked with the team, provided profiles, provided feedback, and showed them our configurations, and they were so responsive to our concerns. They were happy to see that we were able to contribute back just in providing feedback, just giving them actual data of production workloads, which is very difficult to do as a maintainer. 

It's really hard for me to know, "Hey, what's going on in your customer's deployment or this user's deployment?" We can't replicate it in our own environment. That spirit of collaboration was really great, and that's the thing that I'd like to see happen in all the other SIGs. I have limited experience with other SIGs. 

The other part was contributing to semantic conventions, and I contributed to semantic conventions, trying to introduce some new attributes, but there's just so much churn in that repository that it was hard for me to keep up with changes and deprecations. That was a little bit on me to be able to keep up and find out that the stuff I had submitted was deprecated in favor of something else. It's those kinds of challenges that I like—I have to find a better way for us to all be able to keep up with each other. 

One suggestion I might say is, "Hey, look, a change happened. Here's an OTOP—a change happened in the spec that should automatically open issues for every maintainer repo and say, 'Here's a new thing that has changed. This is a new feature we expect that the SDK should support.'" That way, we're able to track it because right now, the way it works is, "Hey, we have somebody who's a representative at the SIG spec; they collect some data, they come back to us, and they tell us, 'Hey, we need to implement this other thing.'" Sometimes we're not diligent about project management, right? We're ICs who are doing this in our spare time, so we need to improve overall at being able to keep each other informed and tracking work.

**Reys:** Got it. So, making sure everyone's on the same page at the same time, which I can see obviously is a bit daunting.

**Ariel:** Yeah.

**Reys:** I know you mentioned some of the SIGs that you have worked with in the past. What would you say are the things you interact with most right now?

**Ariel:** Right now, I almost exclusively participate in the Ruby SIG. I don't have a lot of interaction with other SIGs at the moment, other than through an occasional issue or a discussion that'll pop up, or in Slack, I'll jump into a channel and say, "Hey, I've run into this problem," or "I have this question for y'all. Can you help me?" 

I have limited interactions now these days with other SIGs. Just recently, I'm meeting the end user SIG at KubeCon, so I'm going to probably get more involved in the end user SIG now that I have this special invitation from you.

**Reys:** Oh, I'm so excited!

**Ariel:** This brings us to our "turn the table" section where you get to ask me or anyone on the call questions, and we'll see if I can answer them or if anyone else can answer them.

**Ariel:** Okay, so for me, Rey, it's like that's how I got involved, but maybe you know a special sort of formula or the best way to have someone who's new get involved, in particular in the end user SIG, to provide feedback, or how do they get involved in other SIGs that you've been involved in?

**Reys:** Oh, for sure! So there are a lot of different ways to contribute, right? I think a lot of people, when they think of contribute, they think it means, "Oh, I got to open PRs. I got to be writing code." That's not necessarily the case. There are so many different ways you can provide feedback. One of the ways you mentioned was you just started out by going to a SIG meeting and sharing some stuff that you're seeing. That is an absolutely great way to contribute to the project, and that's what the end user SIG is all about. 

One of the things we're all about is gathering feedback to give back to the appropriate SIG so we can help improve things. To that end, we do things like surveys, open solar Q&A interview sessions where we kind of dive deeper into the adoption implementation process and find out your pain points and specific feedback. You can contribute blogs. If you love writing, documentation is a great way. 

If you are not already a part of the CNCF Slack instance, I would join, and I just realized I did not include that link, but we will have it in the show notes. Once you are a member of the CNCF Slack instance, you can scan the QR code and join the OTel Sig end user channel, and we will be happy to direct you if you have questions about your implementation or you just kind of want to know, "Hey, where do I get started?" 

We are happy to help direct you. We currently have a survey going, which I also just realized I did not share the link. I'm so sorry; I'm going to see if I can get that right now, Henrik. 

So yes, join the SIG meetings where they can access the calendar. It’s through the community repository; it has links to the calendar.

**Ariel:** Perfect!

**Reys:** Yes, if there's a specific language that you are already working in, check out the calendar to find out when the SIG meets and just hop on. When I first started, I was like, "I'm just going to check out a few different SIGs." I went to the Python SIG, and everyone there was so welcoming and so nice. I was so intimidated at first, and then I was like, "Oh wow, these people are so lovely!" That's been my experience with almost every SIG meeting that I've jumped into. Everyone is so open to answering questions, and they want your help. They want to help you help them. 

One of the things is just, you know, letting people know that it doesn't have to be contributions. Of course, if that's what you want to do and are able to, we would love that, but there are so many other different ways—feedback, donation, blogs, doing interviews, and surveys. 

Every little bit helps, right? Every little bit counts. If you find yourself interested in trying to contribute back or you don't know where to get started or you feel just like intimidated, just know that all of us, we do our best to make the most welcoming communities possible in our SIGs. I love that the end user SIG creates these opportunities for folks to come and just provide feedback, a space for them to say, "Hey, I'm having trouble with this," or "I like this," or "This was a great experience for me." 

This part has been very enjoyable for me to be able to share my personal experience and my team's experience with you. I really hope that folks who are watching at home, hey, come on in; the water's fine! Don't worry! We have floaties, we have all kinds of gear to help you get started, right? 

**Ariel:** Exactly! It's just the little things. Every little contribution can help us. If your contribution is there to help you, it'll help out the community. So come if you're having trouble with a specific problem. I guarantee you at least a dozen other people are having the same problem. If you don't understand something, I guarantee you probably at least a hundred other people don't understand it either.

**Reys:** For sure!

**Ariel:** I really appreciate you answering my questions here.

**Reys:** Oh, you are so welcome! I guess I kind of put this at right about time in case anyone has any last-minute questions. We'll hang out here for another minute and just kind of chitchat and give people an opportunity to share if they would like. Otherwise, I will just ask about any fun Christmas or New Year plans.

**Ariel:** Yeah, we're going to be just spending it with family, so that's going to be really nice. We're going to be traveling, so that might be a little bit hectic. We have to travel; we're going to go visit my father, but we are traveling on Christmas Eve, so that should be fun, right? I'm going to show up there, and we'll be singing Christmas carols at the top of our lungs and waking up the neighbors as soon as we arrive.

**Reys:** Oh my gosh! How about yourself?

**Ariel:** Whoa, whoa, whoa! What are these Puerto Rican Christmas songs, and where can I listen to them?

**Reys:** Oh, okay! You can look at them on YouTube, on TikTok; they're all over the place. In Puerto Rico, folks and in other Caribbean islands will join and make something called the "parranda" and have a "paranda," which is a party basically. It's like a Katamari ball, so you go from house to house knocking on doors, and then you start singing these songs. 

The songs are like, "Hey, open up the door! I'm here to say hello!" "Oh, you turned the lights on; I can see you in there! Let me in!" You have to feed the guests and give them beverages and stuff. What you do is you take those people whose home you invaded, and we call it an "asalto," which is like—we're showing up uninvited. We break into your home and then we bring you and add you to the "parranda," and we move to the next house, and we come and, you know, we'll sing and stuff like that. 

It's really great; it's a lot of fun, and so we're looking forward to doing that with our family!

**Ariel:** That sounds so fun! What did you say that was called?

**Reys:** The "parranda" and the "asalto." 

**Ariel:** That sounds lovely! Honestly, that sounds fantastic!

**Reys:** It's a lot of fun! I'm so excited! You have to share pictures!

**Ariel:** I will, thank you! Maybe videos too—who knows what kind of trouble we can get ourselves into!

**Reys:** Yes! I would love to have a video of you singing one of these songs!

**Ariel:** There are videos of me on the internet singing songs, but not now! 

**Reys:** What are you going to be doing this holiday season?

**Ariel:** I am going to be at home busting out hopefully a bunch of house projects that have been on my to-do list for three years.

**Reys:** Nothing like it, right? You're going to get to the tail sampling project, right?

**Ariel:** Oh my gosh! I have! But I'm really excited about it because I moved into this house about three years ago, and it's like, you know, 85% there. There are still—I just—don't look in the cabinets. That's a project; I need to organize all the inside stuff.

**Ariel:** That's the thing I tell people all the time: "Hey, don't look on the inside of the repositories. You may not know; you may not like how the way things are arranged, but you know where things are, so that's totally fine." 

**Reys:** Well, all right! Thank you so much, Ariel! You were a fantastic guest! I'm really excited to learn more. As I said, I did have some follow-up questions, so I'm sure I'll be slacking you to learn more about some of your adoption processes as well as some of the other components you mentioned you had used, such as the OCB. 

But yeah, if anyone wants to reach out to either of us, you can find us in the CNCF Slack's OTel SIG end user channel. Again, this link is up here for you to scan, and I believe we will have the information in the show notes as well, which will be posted, I think, right after this. 

Thank you so much for joining us, and we look forward to seeing you all again next time!

**Ariel:** Yeah, thank you for having me again, Rey! This was really awesome. Thanks everybody for watching, wherever you are and whenever you are! I hope to see y’all in a SIG room sometime.

**Reys:** Oh yes! And happy everything to everyone! Adios!

## Raw YouTube Transcript

hello everyone welcome to a brand new episode of oh tell me and end user Q&A if you have been to one of these sessions in the past you might notice that this looks a little bit different and it has a bit of a different name that's right we have done some growing up since the last few times and we're very excited to debut this new look um but don't worry almost everything else is going to be the same we have a great interview here for you today to learn from um but before I introduce myself and our guest I would love to know where everyone is connecting from I am online from Portland Oregon today and would love to see where um people are from it looks like someone is here from Brooklyn New York so thank you so much for joining out in New York and so my name is reys I am a senior developer relations engineer at New Relic I also co-lead the ner Sig um which is our cute little logo that you see um up in the right corner and yeah um at the end of your Sig we really focus on connecting directly with users and we are dedicated to helping um the rest of the sigs um get feedback from end users so they can help improve the project um and to that end this is one of those uh events that we host to do that so we are going to have um Ariel on Ariel if you would like to join us and introduce yourself hey how's it going everybody um Ariel Valentin I'm a Staff software engineer at GitHub working on observability thanks so much ree for inviting me here to chat with you today as absolute honor to be the first in the new format also so um you know big shout out to you know everybody who's uh done work to try to get this um this uh new platform up and running so thank you oh no thank you for being here I'm really excited um so just to get you and everyone up to speed on the format um it's similar to the ones if you've been on one of these before but we're going to start with some warmup questions where we'll kind of massage Ariel into being comfortable and then we'll hit him with some meaty questions and then we will actually um also get into questions more around the open c community and where he'll have an opportunity to share um like feedback about um his experiences with contributing um using the project stuff like that and then he'll get the chance to ask us questions as well um in a little section we call turn the tables and the very end if there are any audience questions um oh actually scratch that if you have questions that come up during our conversation feel free to put them into the chat if you're watching from YouTube then just put them in the live chat and then I think LinkedIn will also have its own chat so put in your question there and we will get to them as we can throughout the conversation um and then at the end if you have questions at the end as well then we will get to those but yeah if you have any questions that pop up as Ariel as telling us his story we definitely want to get to those as they come in all right so I think we are good to get started so adiel ad told us a little bit about your role of the company um how did you get started with observability how did you get started with open Telemetry oh those are great questions I mean I I should have mentioned that I'm here in Austin Texas Sunny Austin Texas uh so you know where I am in the world um I think a lot of us started originally um uh and I don't think my experience is unique is working with sort of like APM tools uh which are vendor proprietary tools uh generally speaking that uh didn't have distributed tracing in place um at that time and as the community evolved and open tracing became a standard uh that was my first experience with working with distributed tracing was uh the open tracing uh um specification and uh working and trying out different vendors different experiences with uh open tracing and then um when by the time that I had gotten to GitHub um I was a champion for distributed tracing because I saw the power that was in there and around that same time folks were already moving towards developing and transitioning away from open tracing and open sensus to open Elementary so I got really uh involved very early on in trying to spread the word and learning more um and working with uh my obser my team which was the observability team to uh start to adopt um tracing more to embrace it more and to make otel sort of our North Star um for all of our Telemetry signals I feel like I keep saying the word Telemetry over and over again I'm going to have to find yeah me a lot of that we're both going to stumble over over open Telemetry at some point or distribute tracing at some point it's all right this is a safe space what um okay so you mentioned you became a champion for distribute tracing um and you know I think something that's interesting is you know we're so ins scon in the world of observability that we at least I you know tend to presume everyone understands what that is um and it's always surprising to me when someone's like what's distributed tracing and they working um what so kind of along with that what um do you think that's kind of a main Challenge and you think most organizations organizations face when it comes to observability is not just understanding the value of distribute tracing and yeah because these are all sort of new Concepts to folks right uh you know folks have their different levels of understanding of the tools that are available to them and so there's all this vocabul that you hear that's a little bit hard to to parse through so sometimes it's like oh when you say Trace do you mean like a trace log log level is at the trace level uh when you say Trace do you mean the samples taken from a profiler when you say so there's a lot of this sort of uh language that um even though we're we're uh converging on a lot of this language and we uh have these dictionaries that are defined and published everywhere everywhere there's still sort of like this uh there's a hump that we have to get over or like a challenge that we have with trying to get everybody speaking um um the same language within the same context right very similar to in domain driven design we have these bounded context where the same term means a different thing you know that that flows over into sort of our domain language when it comes to observability and Sr practices um and and I'm sure many folks have faced that the those challenges as well it's kind of like let's all get on the same page about what we mean right oh absolutely um what are currently some of the most interesting problems that you are facing in your role oh well I mean one of those things is uh is that transition right um it's really hard for an organization like us who's been around for a long time um and I say a long time but you know whatever it is 10 years um where the system has grown and involved there have been Acquisitions that have been brought together uh there's disparate kind of uh uh backends where we're collecting this data um it's it's uh trying to transition from one way of doing things to a new way of doing things right so it's learning something new that's always going to be a big Challenge and you know folks ever are are trying to do their job every day they're not trying to learn new sdks or trying to learn new vocabulary or trying to learn how to be um proficient in their back ends what they're trying to do is keep the system up and running and keep our customers happy right so I think you know those are some of the the the challenges that that we all you know I know I face it I'm sure others do which is um is is making these transitions with the fewest pain points as possible with like trying to avoid these paying points um and there's I mean there's so many more we can go on and on Reese but I I think right now that's kind of like one of the biggest challenges for adoption I think overall that's actually um that's a great segue into the mey section so what was the process like for GitHub to adopt open Telemetry so uh for us it was um the the role of the advocate right so I acted as an advocate and was a champion for otel at the company and I specifically was brought in to help Advance the mission of tracing and uh it was something that I had pitched and and also sort there there were uh other challenges that we faced too which was hey let's get everybody um uh building a data dictionary let's get everybody uh agreeing to the same language when it comes to what our attributes were going to be right because as you can imagine as a system of walls or Acquisitions or other teams are rolled in everybody has their own uh you know log attributes or their own metric attributes or whatever it is and and so we started to I said look here's a Northstar right here here's semantic conventions from otel let's anchor onto this and let's follow the rules around semantic convention so that we can all uh build up our own internal dictionary right and that that comes with its own challenges right scheme of migrations and trying to keep up to date with the spec and you know what the uh instrumentations are doing right and um and as an advocate you know was also uh a lead on the roll out so one of the things I wanted to do was sunset all of our old sdks and move over to our open source sdks um and you know myself and a few other members for my team we all were involved in um otel Ruby for example because you know U we're a big Ruby shop uh but uh we got involved in there to help the instrumentations get better and to help test different um releases of the SDK and get that rolled down into our GitHub monolith so um we you know were very like I said very early adopters ran into some challenges and um and and continue to give feedback um to the community that way as a and I am not only an end user but like I said I'm also a maintainer so um I'm playing multiple roles there you know trying to help the community along yeah that's a really interesting position to be in is as end user and contributor and I definitely want to Circle back on that when we get to the OA community questions um tell us about the architecture landscape at GitHub and the Telemetry that you're capturing sure so um I've got this little slide that I put together in mer marked down so it's not anything official we can bring that up now so we can take a look at it um and I imagine that for a lot of folks out there who are do working um with uh um virtual machines where they're running system D units or if they're running kubernetes we have this this uh uh this uh we have different deployment Styles here but some of our main workloads are uh running in kubernetes and the way uh that we've got everything set up you could see here is as part of our kubernetes cluster you know we run our own custom uh meshing grass um and on every worker node um or um um on the worker nodes we're running a deployment of the open telemetry collector which we uh build using OCB so we have our own version of The otel Collector that is specific to us uh we chose that route because we wanted to ensure that uh we had the most secure uh build possible with the minimum number of dependencies and we also wanted the ability to build our own custom processors so that um we can uh address any issues that we might have that are specific to our needs um in inside of each of these pods you know we also have a a mesh side car and so Ingress traffic is going to come into the hotel collector otel over H ltlp over HTP um and if you have another application that's running your service it's shooting over um OTP traces over to the over the the mesh and sending it to our oel collectors which we then from there generate span metrics and um sample traces and send those off to a SAS provider that has a um where we aggregate all this data um and then on each individual worker you know we're running in this hybrid world right now um we're only um leveraging oel for traces and for generating um a span metric data uh we're still living in a world where we're using nonel non-lp formats for collecting things like custom metrics system metrics and for collecting logs um we have uh on each one of our worker nodes a metrics agent um and that can speak various different um um protocols but um mostly it's either using open metrics or it's using statsd uh to collect uh data and aggregate it and send it off to the SAS provider um in addition to that on all of our worker nodes we have a fluent bit running and that's what we're using to collect our logs um and all of our logs are end up getting streamed out through Azure event hubs which are uh processed by a bunch of consumers and then sent off to our log Store and search system um in addition to that you know we in addition to these uh kubernetes workloads we have our own virtual machines where we run systemd units you have to think about you know like the git file system Services um those are all running on systemd and we have the metrics agent and fluent running there we don't yet have the otel collector deployed there but um that's where we want to get to we want to get to a world where essentially our entire platform is running um an open source software the open Telemetry collector and um we're converging on OTP for all these formats uh so those services are also using statsd open metrics uh flu bits pulling things out of Journal D because we you know um um that's where we're streaming all of our our logs of the system Journal so um and all that stuff again goes right through to the same channels and it all gets aggregated in these places where we do our work um a little bit about some facts about sort of our our uh Trace data is that um we've rolled out the otel sdks for different programming languages we have them for Ruby goang uh JavaScript nodejs net we had some experimental uh uh rust usage but um uh uh we've had to scale that back um and we had some Java experiments that didn't pan out we didn't roll those out uh to production um and a lot of the stuff that we do when we we started before any of the automatic instrumentation was available for some of these languages so we're still deploying them through rapper libraries so we maintain a set of rapper libraries you install those it gives you the sort of the what we consider to be the minimal defaults um that we require for for our needs and uh we'll do periodic updates of those through dependabot right so it's like as we roll out new vers versions of those things um I feel like I've said a lot so far and I don't know ree if you had any follow-up questions for me or I actually have so many but oh okay great um so you mentioned and um so you mentioned you're leveraging open for traces right now um and then you talked a little bit about you know um the plans and some of the stuff that you tried um and I was curious to find out more about um is that mainly because if you're primarily a ruby shop and you know the SK maybe the signals for metrics and logs aren't quite as mature yet is that the reason or that's one of the reasons so you know one of the other challenges that I didn't discuss when you asked about adoption challenges is even though oel says hey this is what the signals look like this is what the semantic attributes are like this is um there's a lag between the time that something is published or declared in the oel spec to the time that um vendors or open-source platforms are able to leverage those things and there's sort of like this feedback loop as we go through otps and say hey we want to try this new thing out we do a couple things in experimental languages we'll do this stuff and um you know it's it was a a big enough challenge for us to start to roll tracing out that we didn't you know and adopting semom for logs that we didn't want to take yet another thing on which was to say okay now we're going to switch over to Native OTP metrics as well um and like you said some of the sdks are ahead than others so with Ruby we're just recently you know with the help of our amazing maintainers um Schwan and um and AAA they've been working diligently to get the metric SDK uh up to speed for Ruby so that so that's something that wasn't available to us to use um and I think that one of the the biggest advantages that I that that I see in tracing is the ability to generate span metrics from traces it's like hey the the the less things that we have to impose on our users for them to try to figure out how to do the better for us right and are you using the spam metrics connector is that what you we're using a custom connector right now I'd like to be able to get us to the point where we're using a span metrics connector but we don't have egress in OTP right now we're we're still doing sort of like vendor proprietary formats for uh for export where I want to get to is you know that's the strength one of the biggest strengths of the of otel is OTP is that standard format that we can that makes things portable for us absolutely so that's definitely sort of high on my list of things that I want gotta okay so it sounds like the plan is to migrate your uh other signals over once those reach GA in languages oh certainly and you know once I have more time on the calendar too right we have so many projects that we have to do as engineers and companies and it's like hey where do we fit these one these in right um uh as you imagine you know GitHub is constantly growing every day um we just hit 150 million users uh you know so it's like just an amazing um amazing growth of the company so shout out to all the people at GitHub who've been working so diligently to make this happen you know um and so we're here to support we we're here to support that volume growth and support our end users so um um so I hope that answers your question absolutely um and also want to mention again real quick for those um who joined us a little bit later feel free if you have questions that come up um you want to learn more about something that Ariel has gone over feel free to pop it into the live chat of whatever platform you're watching from so whether it's YouTube LinkedIn um go ahead and pop the question in and we will try to get to them um throughout the show as you can um so I noticed on your second slide you mentioned probabilistic sampling so it sounds like you're not doing any kind of tail sampling no not at the moment so would you mind bringing that up again uh um L the second slide yeah that's awesome yeah so we started with probabilistic sampling one of the hard things about tail sampling um as you can imagine is that um the traces all have to go to the same collector in order to make that decision if you're using the collector for uh for for tail sampling so um right now we wanted to reduce that burden in complexity immediately um and we um when we started with probabilistic sampling some of the things that we want to do in the future is um more advanced remote sampling uh so that we can have um uh more Frank grain control of what we're looking at we want to be able to do leverage uh tail sampling rules but as you know that's very difficult to implement a little bit difficult to scale in our case because you know we have about 2,000 collectors which are supporting everything right now um across all of our our Fleet right of like 14,000 hosts or something like that and I'm making that number up off the top of my head I think that's the last number we had um and you know right now we're we're doing about 26 million spans per second and uh just yesterday during our peak times you know we hit our alltime high of 32 million spans per second as we continue to um uh to grow our volume so this is one of the the challenges that we've had and so on my list of all the things that I want to do tail sampling is definitely on there you know what I mean so okay at what point do you think you'll you might get to the point where you could Implement heill sampling would be more like side when the you know processor has been uh more develops or devs it's you know it's it's uh I don't know how to answer that question because it's on the pile of the list of things that I want to do towards the bottom so it's like when can we do it when I get to all my other wish list items got it got it um and although I am curious about the other but that's okay that's all right questions that I want to get to as well sure um also just going back uh you mentioned trying some experiments in like Java and Russ I didn't kind of pan out and I was just curious um what was it that didn't quite work out or that you know didn't meet your expectations well uh We've reduced the number I'm sorry um all the jvm people out there we've reduced the number of java workloads that we've uh that we're running um so those have been migrated over to different programming languages um so that's one of the reasons why we didn't go through and say oh let me continue to roll out uh jvm work like the we just didn't have the return on investment that that we wanted to you know uh try and um also like we don't have a lot of um uh the same thing was for rust is that we have like a very limited set of applications that run rust and um those are perform for performance reasons they ran into some challenges with um how it impacted their latency when they introduce the use of the SDK and I'm going to be honest with you me not being a rust expert and being able to get in there and get my hands dirty to try to help out with addressing some of those problems and Reporting them Upstream that was something that we had to put on pause cuz again it's like one program versus all of these other services that are running and go and Ruby and um and JavaScript that we need to pay attention to right and um so that that I think that's really where it was where sort of like a critical mass of application services that use these programming languages and was like hey we have to focus our attention ption on those that are going to get us a higher return on investment for this absolutely okay so what was oh yeah what was github's observability tool before migrating to I think you mentioned um you were using proprietary sdks uh yeah I mean we were use proprietary SDK for open tracing so you know open tracing was our uh how we collected traces before um but um generally speaking the I I I I think I covered this in the first slide GitHub is a huge uh sort of statsd metrics user still to this day um and um logs are a big part of uh of what we utilize here so it's like folks are looking at acception stack traces a lot of the times to try to understand where errors are coming from they're looking at um uh access log streams um and they're trying to and they were trying to piece together Hey where's the request going and where is it slowing down and this and that and I showed up with my you know magic Tool Distributor tracing I'm like hey look here's a here it is on the flame graph or an icycle graph or here's a waterfall view of this thing and it's like oh that's pretty cool and um people started looking into that as uh as a an additional tool in their tool box for them to help try to debug things during an incident um so I hope that answers your question there yes and kind of along those lines how have things changed since GitHub switch to open cemetry so uh We've what I'll tell you is that uh just this year alone we've seen a dramatic I wish I had these statistics off hand but we've seen a dramatic increase in the number of people using tracing and a number of services that have been instrumented part of that comes from the fact that I was like hey everybody we're all moving to this new SDK so everybody install it and let the pendot go ahead and do these installations on your on your apps and people started seeing the value of this uh of this investment as well so they started to use it more um but we um I want to say that like when we started this adventure uh only about uh 80 and I'm going to use the word services in air quotes only about 80 Services were uh instrumented and now uh we're closer to about 300 of those Services which are effectively like kubernetes deployment types or like system D types uh because as you imagine our monolith has th you know uh maybe a thousand services within it so there're like subservices in there um but uh the but the monolith itself has broken down into like eight Services as like you know web UI and API and graphql and background workers and stream processors and whatnot so um that's why I put them in air quotes as these are Services um but we saw that that that was quite an increase in the number of services that that came in we were doing something like uh uh like 5 million spans per second to now we're up to 32 million spans per second it's just a huge uh uh increase in volume in usage and along with that volume increase in usage and data volume um how would you say that's impacted how your services are running how um you know quickly your team is able to debug issues as they arise part of it is an education think so a lot of things that uh so I mentioned that I'm on the observability team but really we're two groups uh one of the groups is called The Experience team that works directly with teams and they're they operate in a role sort of like developer relations and advocacy but also working on um you know cost control and improving your experience uh helping you with identify new workflows and introduce them into uh your incident command experience um those those those teams set up sessions education sessions to bring folks on board uh but we try to do them not during an incident um because that that that puts some stress and so what we try to do is like you know we'll go into the if an incident is happening it's like hey here's some insights that we're gaining and we'll share with you that we're seeing uh from the traces that you might not be able to see you somewhere else and that has helped in a lot of cases uh uh where we couldn't exactly pinpoint what was going on uh but then um there's also these spots where not every system has been instrumented so we have to rely a lot on sort of like client metrics or client Trace data and say hey look this client is experiencing this problem can we take a look deeper at this other service that hasn't yet been instrumented um and so it's been sort of I'm going to say like this mixed results some teams have like identified issues even before they go out out to production and other teams have been able to leverage it when it's like oh this we're having an incident right now oh here's uh uh and and here's the here's the reason why this is failing um and and we've able to do things like identify bottlenecks and like mistakes that you know little little coding mistakes oh I forgot to act the message before I pulled it out of kavka oh I'm just retrying that same message millions of times or um or oh the client timeout doesn't match the server timeout so the client out and the server is continuing to turn along to try to do this request so we're identifying things like that that we couldn't identify before easily easily you know um and so those are some anecdotes oh no that's awesome I think I will probably be asking you more about that because I think this is really interesting um one more question from our mey section what would prevent you from implementing better Telemetry what would prevent me from doing that I think it's because there's so much of the stuff that's in early stages and you know we were early adopters on a lot of things but then there's a lot of stuff that's a lot more risky for us to try to roll out so for example one of the things I'm really excited about when I came back from cubec con is the continuous profiler I mean that is one one of the when you talk about one of the things on my wish list that we'd be able to use today if we could uh that's the thing that I would want to do but because we're still in the early stages of the of the profiler and the specification and there's still uh some turn with the data model um I think that there are when it comes to the resource intensive or sort of like introspective tools like that it's I we don't want to take that risk going a little bit too early to adopt those tools uh because also there's not a lot of support from our current U vends and vendors that will be able to leverage that it's kind of like we would be experimenting with that to go to nowhere and so that's kind of you know once vendors start to support the OTP profiling format um data model I should say and once the profiler is a little bit more stable I'd love to jump in there and and be able to roll that out a little more widely um and um I think that uh a little bit of a a a bumpy road for us to still is migrating um semom from pre 1.0 because we were early adopters of semom so we're at this pre 1.0 stage and sort and getting ourselves to migrate towards a a 1x version of of semom is uh another big challenge because we've already sent all of this data out it's our backends there we have to figure out a way to upgrade it or to say you know oh this is version X um and and I and I kind of it feels like a lot of the progress was stalled there for the moment um so that there I don't know that there's any real solutions in um available that would say okay we can start migrating these schemas over and keep our user experience very high quality um if that answers your question oh absolutely I think that is another interesting topic migrating some semantic conventions so I might I might jump back if we have time later but I do want to get to some of these Community questions yes ma'am um yeah so one of the big things I think um you know working worked in the community that uh probably you've seen you know I've seen sure um is people want to contribute to the project but they're not really sure where to get started and yeah we have like resources such as you know I think in the documentation we added you know getting started um and you have you know various uh Sig channels and cncm slack um and I think we all have the problem of how do we how are how do we reach these people because you know there's still some many I don't know so what was the contribution process for you and your team like to open cemetry for us it was um uh it was a a short process with a lot of observation so the things that we looked for uh myself and my teammates at the time when we approached the community we were looking at code of conduct what were the expectations of the code of conduct we went through and actually looked at issues previous PRS the feedback that was in those PRS to see if the behaviors expected in the code of conduct were reflected in the PRS and in the issues uh cuz there's nothing worse than you wanting to be part of this community and finding that those two things don't match and to see that that happened then we um participated in the sigs which was very nice for us to be able and convenient for us in the US because the Ruby Sig was in the US hours we joined those meetings during the day and started to provide some feedback as end users and say hey look we ran into this challenge if we identified something that was a challenge for us we would could contribute a PR um and the maintainers were very generous with their time um did their reviews and um and we were able to get a couple of Uh custom propagators uh merged we got some instrumentations merged we identified other uh gaps for the instrumentations that we use cuz one of the biggest challenges I think as a maintainer is there's so much that you have to do right you've got the SDK you've got the API you've got documentation to do and you have to have language specific instrumentations and uh that's daunting especially with all of the popular libraries that are out so for us it sort of we contributed back to the libraries that we use heavily right we have like a subset of libraries that are very popular at GitHub and um sort of blessed right so by by their popularity so that's where we focused our energy and our contributions um and then we reached out to people you know uh anytime somebody comes to me and says hey I have this challenge with this thing it's missing this attribute or I'd really like it to work like this or I've identified this bug the first thing I do I say is let's open up an instr and let's work on this together and I'd love to review your PR and creating in um an environment that makes it lowers a barrier to entry to have folks uh uh collaborate and submit contributions I want them to feel like this is yours this is ours this is not mine and I'm gatekeeping right we still have some standards for Quality obviously and some expectations from you as a maintainer but we try to make this an a painless experience for you um at least in the Ruby uh um um community so that's what the the contribution process was like for me and getting involved uh in the community that's awesome so you started basically joining the Sig meetings as an end user and sharing feedback and then finding information about like how to you can open uh you know an issue and then like from there building custom stuff and uh helping the community or sorry the Ruby s to build out more of their components okay yeah certainly and like you know with the home for open source right so uh for us um you know we open sourced uh for example new database drivers in Ruby for um MySQL right um and so we were the we had that instrumentation in house before all of that became public and as soon as we released that new driver I went right to the Sig and said Tada I have a donation for you and it's like oh yeah we able we were able to pull that in there so um it's this thing where um there's just this Spirit of collaboration right and this Spirit of supporting open source that um that's important to our mission right okay so it sounds like you had a pretty kind of positive experience it sounds like from for contributing to Ruby um I like to know do you have any feedback to the SS on ways to improve how things work based on your experiences with Ruby and then other sigs I know you mentioned also some other components that you've used but we'll come back to that yeah for sure and I again I I want to thank everybody for their amazing work that they've done um contributing to the collector in particular uh because you know we are uh we release our OCB build or sorry our our uh our custom build every time a new version of the cont package rolls out so um every week depend on bot is telling us or every two weeks it's telling us hey it's time to upgrade a new releases rolled out and uh we ran into uh some challenges where we identified some performance issues um in the in the collector and we uh worked with the team provided profiles provided feedback uh showed them our configurations and they were so responsive to um our concerns and uh they were happy to see that we were able to contribute back just in providing feedback just giving them actual data of production workloads which is very difficult to do right as a maintainer it's really hard for me to know hey what's going on in your um in this customer's deployment or this users uh a deployment we can't replicate it in our in our own environment so um uh that Spirit of collaboration was um was really great and that's the thing that I'd like to see uh happen in all the other SE I have limited experience uh with other sigs the other uh part was contributing to semom um and I contributed to simcom trying to uh introduce some new attributes uh but there's just so much turn in that repository that it was hard for me to keep up with uh changes and deprecations right um and so that was a little bit on me to be able to to keep up and find out that the stuff I had submitted was deprecated for example in favor of something else uh but it's it's those kinds of challenges that I like I I I have to find a better way for us to all be able to to to keep up with each other so one suggestion I might say is hey look a change happened here's an otop A Change happened in the spec that should automatically open issues for every maintainer repo and say Here's a new thing that has changed uh um this is a new uh feature we expect that the SDK should support and that way we're able to track it because right now the way it works is hey we have somebody who's a representative at the Sig spec they collect some data they come back to us and they tell us hey we need to implement this other thing and sometimes you know we're not diligent about project management right we're IC who are doing this in our spare time right um so it you know we don't um we need to improve I think as overall um at being able to keep each other informed and tracking work got so making sure everyone's on the same page at the same time which I can see obviously is a bit of a it's daunting yeah um so I know you mentioned some of the sigs that you have worked with in the past what would you say are the things you interact with most right now right now it's the I almost exclusively participate in the Ruby Sig um I don't have a lot of interaction with other sigs at the moment other than through an occasional issue or a discussion that'll pop up or in slack I'll jump into a channel and say hey I've run into this problem um or I have this question for y'all can you help me um and so um I've limited interactions now these days with with other s and uh just recently I'm meeting the end user Sig at uh cucon um so I'm going to probably get more involved in the end user Sig now that I have this special invitation from you so yes oh I'm so excited um and actually this brings us to our turn the table section where you get to ask me or anyone on the call questions and we'll see if I can answer them or if anyone else can answer them okay so uh for me re it's like that's how I got involved but maybe you know a special sort of formula or the best way to have someone who's new get involved in particular in the end user Sig to provide feedback or or how do they get involved in other s that that you've been involved in oh for sure so there's a lot of different ways to contribute right I think a lot of people for them when they think contribute they think it means oh I got to open PRS I got to be writing code um and that's not necessarily the case there's so many different ways it can provide feedback and you know like one of the ways you mentioned was um you just started out by going to a Sig meeting and sharing some um stuff that you're seeing that is an absolutely great way to contribute to the project and that's what the N Sig is all about well one of the things we're all about is gathering feedback to give back to the appropriate Sig so we can help improve things um so to that end we do things like surveys um open solary Q&A interview sessions um where we kind of dive deeper into the adoption implementation process and find out your pain points and specific feedback um you can contribute blogs maybe love writing documentation is a great way um and probably if you are not already a part of cncf slack instance I would join and I just realized that I did not include that link but um we will have it in the show notes and you can once you are a member of the CNC of slack instance um you can scan the secure code and join the otel Sig end user Channel and we will be happy to direct you um if you have questions about you know your implementation or you just kind of want to know hey where do I get started um I'm trying to do you know we are happy to help um direct you and we currently have a survey going which I also just realized I did not share the link I'm so sorry I'm gonna see if I can get that right now Henrik um and let's see so yes join the Sig meetings um where can they access the calendar it's through the the community Repository has links to the calendar perfect yes so if there's like a specific language that you are already working in um check out the calendar to find out where uh sorry when the Sig meets and just hop on and you know when I first started I was like I'm just GNA check out you know a few different sigs um I went to the python Sig and everyone there was so welcoming and so nice I just like I was so intimidated at first and then I was like oh wow these people are so lovely and that's been my experience with like almost yeah I mean not almost but every Sig meeting that I've jumped into is everyone is so open to answering questions and they want your help you know they they want to help you help them um so I think one of the things is just you know letting people know that it doesn't have to be contributions of course if that's what you want to do and are able to we would love that um but there's so many other different ways um feedback doation blogs doing interviews um surveys and yeah so you know similar related to that is like you know you're saying that hey you can contribute in all these other ways what kind of expectations of commitment do you have from folks who want to participate like um do they have to show up to every single meeting do they have to constantly be reviewing PRS do they have to review you know issues um if they miss a meeting are they like kicked out of the group never to be invited again you know got it I think so for the end s specifically we are a pretty small group right now um so we are always happy to you know if anyone wants to hop in and um if you're an end user and you're like yeah I I have some stories to share perfect please reach out to us in the Sig Channel um we would love to work with you to get something scheduled and talk to you a little bit further um and as for the expectation uh commitments that you asked about I I mean I would hope that you could you know join at least once one Sig meeting a month just so we can you know connect virtually um but even if you have a hard time making this thing meeting because of like other commitments or time zone slack is a great place to get in touch with us we are totally happy to um Converse async over slack um so don't worry about you know if you can't make the meetings um or every meeting and as far as what's needed I think it just depends on the priorities of each individual Sig um for us right now it's really just we're working out we're working on building out um our YouTube channel for instance um we are still working on building out some of the end user resources we have available which there is a link to here um you can scan to see to to learn more about some of the events that we do um but yeah time commitment wise you know we have some contributors that hop on as they can so we'll maybe see them like you know a few times a quarter and honestly that's fantastic you know we don't need someone to be of course it would be great but um if you have obviously your day job uh you know it's taking up most of your time like we completely understand uh you know we would just love to see you coming as you can so don't worry about having to yeah get a number certain number of PRS um each month or hit a certain number of meetings um yeah it's it's like every little bit counts right every little bit helps so if you find yourself interested in trying to contribute back or you don't know where to get started or you feel just like intimidated just know that all of us um we do our best to make the most welcoming communities possible in our sigs and I love that the end user Sig um creates these opportunities for folks to come and just provide feedback a space for them to say like hey I'm having trouble with this or I like this or this was a great experience for me um that's this this part has been very enjoyable for me to be able to share our my personal experience and our my team's experience with you so um uh I really hope that folks who are watching at home hey come on in the the water's fine don't worry and we have if you didn't host him we have booies we have floaties we got all kinds of gear yeah to to help you get started right so it's just a it's the little things every little contribution can help us um if it help if your contribution is there to help you it'll help out the community so um come if you're having trouble a specific problem I guarantee you at least the dozen other people are having the same problem if you don't understand something guarantee you probably at least a hundred other people don't understand it either so for sure for sure so I really appreciate you um answering my questions here oh you are so welcome um and I guess I kind of put this at right about time um in case anyone has any last minute questions we'll hang out here for another minute and just kind of chitchat um and give people an opportunity to share if they would like um otherwise I will just ask about any fun Christmas New Year plans yeah we're going to be just spending it with family so that's going to be really nice um and uh we're going to be traveling so that might be a little bit hectic uh we have to travel uh we're going to go visit my father but uh we are traveling on Christmas Eve so that should be fun right I'm going to show up there and we'll be seeing U poran Christmas carols um at the top of our lungs and waking up the neighbors as soon as we arrive so uh totally fine for me you know oh my gosh how about yourself whoa whoa whoa whoa whoa what are these Puerto Rican Christmas songs and where can I listen to them oh okay you can look at them on on YouTube on Tik Tok they're all over the place but um they're uh um in Puerto Rico you know folks and in other Caribbean island folks will join and make uh something called the trya and have a paranda which is a party basically and it's like a Katamari ball so you go from house to house knocking on doors and then you start singing these songs and the songs are like hey open up the door I'm here to say hello oh you turn the lights on I can see you in there let me in and then you know you have to feed the guests and give them um Beverages and stuff and then um what you do is you take those people whose home you invaded that we call it a we call an asto which is like we show up and um we uh I don't want to the direct translation is more like you've been Stu we're sticking you up we're holding you up but uh we we break into your home and then we bring you and add you to the Kari ball and we move to the next house and we come and you know we'll sing and stuff like that so uh it's really great um it's a lot of fun and so we're looking forward to doing that with our family that sounds so fun what is that um what did you say that was called uh the there's the it's a couple of words the trya the the aalos those are the songs that we will sing and the asalto like the we're we're showing up uninvited that's so that's lovely honestly that sounds fantastic it's a lot of fun it's a lot of fun I'm so excited you have to you have to share pictures I will thank you um and uh maybe videos too who knows what kind of trouble we can get ourselves into yes oh would love to have a video of you singing one of these songs uh there are videos of me on the internet singing songs but not not now uh ree what are you gonna be doing this uh this holiday season I am going to be at home busting out hopefully a bunch of house projects that have been on my to-do list for three years nothing like it right you're going to get to the tail sampling project right that want oh my gosh I have yeah but I'm I'm really excited about it because yeah I moved into this house uh about three years ago and it's like you know 85% there there's still are just don't look in the cabinets that's a project is I need to organize all the inside stuff yeah and that's the thing I tell people all the time it's like hey don't look on the inside of the of the repositories you know you may not know you may not like how the way things are arranged but you know I know where things are so that's totally fine well all right thank you so much ARA you were a fantastic guest um I'm really excited to learn more um as I said I did have some fla questions so I'm sure I'll be slacking you to learn more um even more about um some of your adoption processes as well as um some of the other components you mentioned you had used such as the OCB um but yeah if anyone wants to reach out to either of us you can find us on um in cncf slacks otel Sig and user Channel um again this link is up here um for you to scan and I believe we will have the information in the show notes as well which will be posted I think right after this so yeah thank you so much for joining us and we look forward to seeing you all again next time yeah thank you for having again Reese this was really awesome thanks everybody for watching uh wherever you are and whenever you are um and I hope to see yall in a Sig room sometime oh yes and happy everything to everyone adios

